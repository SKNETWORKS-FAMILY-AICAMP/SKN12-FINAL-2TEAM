# ì´ íŒŒì¼ì€ marketaux apië¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
import os
from dotenv import load_dotenv
import requests
import csv
from datetime import datetime, timedelta
import time
import re
import pandas as pd

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜(API í‚¤)ë¥¼ ë¶ˆëŸ¬ì˜´
load_dotenv()
MY_API_TOKEN = os.getenv('MARKETAUX_API_KEY')

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
    if not text or text.strip() == "":
        return ""
    
    # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    text = text.replace('\n', ' ').replace('\r', ' ')
    
    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
    patterns_to_remove = [
        r'\d+\s*min\s+read',  # "5 min read", "10min read" ë“±
        r'read\s+more',       # "read more"
        r'click\s+here',      # "click here"
        r'learn\s+more',      # "learn more"
        r'see\s+more',        # "see more"
        r'view\s+more',       # "view more"
        r'continue\s+reading', # "continue reading"
        r'full\s+story',      # "full story"
        r'reuters',           # "Reuters" ì¶œì²˜ í‘œì‹œ
        r'bloomberg',         # "Bloomberg" ì¶œì²˜ í‘œì‹œ
        r'source:.*$',        # "Source: ..." ë“±
        r'image:.*$',         # "Image: ..." ë“±
        r'photo:.*$',         # "Photo: ..." ë“±
        r'video:.*$',         # "Video: ..." ë“±
        r'\[.*?\]',           # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©
        r'\(.*?\)',           # ì†Œê´„í˜¸ ì•ˆì˜ ë‚´ìš© (ë„ˆë¬´ ê¸¸ì§€ ì•Šì€ ê²½ìš°)
        r'https?://\S+',      # URL ì œê±°
        r'www\.\S+',          # wwwë¡œ ì‹œì‘í•˜ëŠ” URL
    ]
    
    for pattern in patterns_to_remove:
        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE)
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì••ì¶•
    text = re.sub(r'\s+', ' ', text)
    
    # ì–‘ ë ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def clean_source_name(source):
    """ì–¸ë¡ ì‚¬ ì´ë¦„ ì •ì œ í•¨ìˆ˜"""
    if not source or source.strip() == "":
        return ""
    
    source = source.strip().lower()
    
    # ë„ë©”ì¸ ì œê±° íŒ¨í„´ë“¤
    domain_patterns = [
        r'\.com$', r'\.net$', r'\.org$', r'\.co\.uk$',
        r'\.asia$', r'\.io$', r'\.ca$', r'\.au$',
        r'\.de$', r'\.fr$', r'\.jp$', r'\.kr$',
        r'\.in$', r'\.br$', r'\.mx$', r'\.it$',
        r'\.es$', r'\.nl$', r'\.ch$', r'\.se$',
        r'\.no$', r'\.dk$', r'\.fi$'
    ]
    
    for pattern in domain_patterns:
        source = re.sub(pattern, '', source)
    
    # ì„œë¸Œë„ë©”ì¸ ì œê±°
    subdomain_patterns = [
        r'^forums\.', r'^www\.', r'^news\.', r'^blog\.',
        r'^media\.', r'^press\.', r'^finance\.', r'^business\.',
        r'^tech\.', r'^sports\.', r'^health\.', r'^science\.'
    ]
    
    for pattern in subdomain_patterns:
        source = re.sub(pattern, '', source)
    
    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
    unwanted_phrases = [
        'news', 'media', 'press', 'times', 'daily',
        'herald', 'tribune', 'gazette', 'journal',
        'magazine', 'today', 'now', 'live'
    ]
    
    # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì œê±° (ì „ì²´ ì´ë¦„ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
    source_words = source.split()
    if len(source_words) > 1:
        source_words = [word for word in source_words if word not in unwanted_phrases]
        source = ' '.join(source_words)
    
    # ì²« ê¸€ìë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜
    source = source.capitalize()
    
    return source

def preprocess_news_data(news_data):
    """ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    print("ğŸ”„ ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    original_count = len(news_data)
    
    # 1. í…ìŠ¤íŠ¸ ì •ì œ
    for article in news_data:
        article['title'] = clean_text(article.get('title', ''))
        article['snippet'] = clean_text(article.get('snippet', ''))
        article['source'] = clean_source_name(article.get('source', ''))
    
    # 2. ê²°ì¸¡ì¹˜ ì œê±° (ì œëª©ì´ë‚˜ ìš”ì•½ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°)
    news_data = [article for article in news_data 
                 if article.get('title', '').strip() and 
                    article.get('snippet', '').strip() and 
                    article.get('source', '').strip()]
    
    after_missing_count = len(news_data)
    print(f"  â†’ ê²°ì¸¡ì¹˜ ì œê±°: {original_count - after_missing_count}ê°œ ì œê±°")
    
    # 3. ì¤‘ë³µ ì œê±° (ì œëª©ê³¼ ì–¸ë¡ ì‚¬ê°€ ì™„ì „íˆ ë˜‘ê°™ì€ ê²½ìš°)
    seen_combinations = set()
    unique_news = []
    
    for article in news_data:
        combination = (article.get('title', ''), article.get('source', ''))
        if combination not in seen_combinations:
            seen_combinations.add(combination)
            unique_news.append(article)
    
    final_count = len(unique_news)
    print(f"  â†’ ì¤‘ë³µ ì œê±°: {after_missing_count - final_count}ê°œ ì œê±°")
    print(f"  â†’ ìµœì¢… ë°ì´í„° ê°œìˆ˜: {final_count}ê°œ")
    
    return unique_news

def get_weekly_news_and_save_to_csv():
    """Marketaux APIë¥¼ ì´ìš©í•´ ì¼ì£¼ì¼ì¹˜ ë‰´ìŠ¤ë¥¼ ìµœëŒ€í•œ ë§ì´ ìˆ˜ì§‘í•˜ì—¬ CSVë¡œ ì €ì¥"""
    if not MY_API_TOKEN:
        print("âŒ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    print("âœ… API í‚¤ ë¡œë“œ ì„±ê³µ! ì§€ë‚œ 7ì¼ê°„ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # 1. ë‚ ì§œ ì„¤ì •: ì •í™•íˆ 7ì¼ ì „ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€
    today = datetime.now()
    seven_days_ago = today - timedelta(days=7)
    
    # ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ëª…í™•íˆ ì„¤ì •
    start_date = seven_days_ago.strftime('%Y-%m-%d')
    end_date = today.strftime('%Y-%m-%d')
    
    print(f"ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
    print("-" * 50)

    all_news_data = []
    seen_urls = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ URL ì§‘í•©
    
    # 2. ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„° ì–‘ í™•ë³´
    print("ğŸ“° ë°©ë²• 1: ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (ëª¨ë“  êµ­ê°€, ëª¨ë“  ì¹´í…Œê³ ë¦¬)")
    collect_news_by_method(all_news_data, seen_urls, start_date, end_date, method=1)
    
    print(f"\nğŸ“° ë°©ë²• 2: ì£¼ìš” êµ­ê°€ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ (í˜„ì¬ ì´ {len(all_news_data)}ê°œ)")
    collect_news_by_method(all_news_data, seen_urls, start_date, end_date, method=2)
    
    print(f"\nğŸ“° ë°©ë²• 3: ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ (í˜„ì¬ ì´ {len(all_news_data)}ê°œ)")
    collect_news_by_method(all_news_data, seen_urls, start_date, end_date, method=3)
    
    # 3. ëª©í‘œ ê°œìˆ˜ í™•ì¸
    print(f"\nğŸ“Š ìµœì¢… ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê°œìˆ˜: {len(all_news_data)}ê°œ")
    if len(all_news_data) < 500:
        print("âš ï¸  ëª©í‘œ 500ê°œì— ë¯¸ë‹¬í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ìˆ˜ì§‘ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        collect_additional_news(all_news_data, seen_urls, start_date)
    
    # 4. ë°ì´í„° ì „ì²˜ë¦¬
    print(f"\nğŸ”„ ì „ì²˜ë¦¬ ì „ ë°ì´í„° ê°œìˆ˜: {len(all_news_data)}ê°œ")
    all_news_data = preprocess_news_data(all_news_data)
    
    # 5. CSV íŒŒì¼ë¡œ ì €ì¥ (ìš”êµ¬ì‚¬í•­ì— ë§ì¶° 4ê°œ ì»¬ëŸ¼ë§Œ)
    if not all_news_data:
        print("âŒ ì „ì²˜ë¦¬ í›„ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” í´ë”ì— ì €ì¥
    current_dir = os.path.dirname(os.path.abspath(__file__))
    filename = os.path.join(current_dir, f"ì£¼ê°„_í•´ì™¸ë‰´ìŠ¤_{today.strftime('%Y%m%d')}.csv")

    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            # ìš”êµ¬ì‚¬í•­ì— ë§ì¶° 4ê°œ ì»¬ëŸ¼ë§Œ ì €ì¥
            writer.writerow(['ë‚ ì§œ', 'ì œëª©', 'ìš”ì•½', 'ì–¸ë¡ ì‚¬'])
            
            for article in all_news_data:
                # ë‚ ì§œ/ì‹œê°„ í˜•ì‹ ì •ë¦¬
                try:
                    published_date = datetime.fromisoformat(article['published_at'].replace('Z', '+00:00')).strftime('%Y-%m-%d %H:%M')
                except:
                    published_date = article.get('published_at', '')
                
                # 4ê°œ ì»¬ëŸ¼ë§Œ ì €ì¥ (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„°)
                writer.writerow([
                    published_date,
                    article.get('title', ''),
                    article.get('snippet', ''),
                    article.get('source', '')
                ])
        
        print("-" * 50)
        print(f"âœ… ì´ {len(all_news_data)}ê°œì˜ ì „ì²˜ë¦¬ëœ ë‰´ìŠ¤ë¥¼ '{os.path.basename(filename)}' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {filename}")
        print("ğŸ”§ ì „ì²˜ë¦¬ ì™„ë£Œ: í…ìŠ¤íŠ¸ ì •ì œ, ì–¸ë¡ ì‚¬ ì´ë¦„ ì •ì œ, ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ì œê±° ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def collect_news_by_method(all_news_data, seen_urls, start_date, end_date, method):
    """ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    page = 1
    
    while page <= 10:  # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘
        print(f"  í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
        
        if method == 1:
            # ë°©ë²• 1: ì „ì²´ ë‰´ìŠ¤ (í•„í„° ìµœì†Œí™”)
            url = (f"https://api.marketaux.com/v1/news/all"
                   f"?language=en"
                   f"&published_after={start_date}"
                   f"&published_before={end_date}"
                   f"&limit=100"
                   f"&page={page}"
                   f"&sort=published_desc")
        elif method == 2:
            # ë°©ë²• 2: ì£¼ìš” êµ­ê°€ë³„
            countries = ["us", "gb", "ca", "au", "de", "fr", "jp", "in"]
            country = countries[(page - 1) % len(countries)]
            url = (f"https://api.marketaux.com/v1/news/all"
                   f"?countries={country}"
                   f"&language=en"
                   f"&published_after={start_date}"
                   f"&published_before={end_date}"
                   f"&limit=100"
                   f"&page={page}"
                   f"&sort=published_desc")
        else:
            # ë°©ë²• 3: ì¹´í…Œê³ ë¦¬ë³„
            categories = ["general", "business", "technology", "entertainment", "health", "science", "sports"]
            category = categories[(page - 1) % len(categories)]
            url = (f"https://api.marketaux.com/v1/news/all"
                   f"?categories={category}"
                   f"&language=en"
                   f"&published_after={start_date}"
                   f"&published_before={end_date}"
                   f"&limit=100"
                   f"&page={page}"
                   f"&sort=published_desc")
        
        headers = {'Authorization': f'Bearer {MY_API_TOKEN}'}
        
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            articles = data.get('data', [])
            
            if articles:
                new_articles = []
                for article in articles:
                    url_key = article.get('url', '')
                    if url_key and url_key not in seen_urls:
                        seen_urls.add(url_key)
                        new_articles.append(article)
                
                all_news_data.extend(new_articles)
                print(f"    -> {len(new_articles)}ê°œ ìƒˆë¡œìš´ ë‰´ìŠ¤ ë°œê²¬. ì´ {len(all_news_data)}ê°œ")
                
                # ì´ë¯¸ ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆë‹¤ë©´ ì¤‘ë‹¨
                if len(all_news_data) >= 1000:
                    print("    -> 1000ê°œ ì´ìƒ ìˆ˜ì§‘ ì™„ë£Œ. ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ ì´ë™.")
                    break
            else:
                print("    -> ë” ì´ìƒ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            page += 1
            time.sleep(0.3)  # API ìš”ì²­ ê°„ê²©
            
        except requests.exceptions.RequestException as e:
            print(f"    âŒ API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if "429" in str(e):
                print("    â³ Rate limit ì—ëŸ¬. 3ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                time.sleep(3)
                continue
            break

def collect_additional_news(all_news_data, seen_urls, start_date):
    """ì¶”ê°€ ë‰´ìŠ¤ ìˆ˜ì§‘ (ëª©í‘œ 500ê°œ ë‹¬ì„±ì„ ìœ„í•´)"""
    print("ğŸ”„ ì¶”ê°€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ë” ë„“ì€ ë‚ ì§œ ë²”ìœ„ë¡œ ìˆ˜ì§‘
    extended_start = (datetime.now() - timedelta(days=14)).strftime('%Y-%m-%d')
    
    page = 1
    while len(all_news_data) < 500 and page <= 20:
        print(f"  ì¶”ê°€ ìˆ˜ì§‘ í˜ì´ì§€ {page}...")
        
        url = (f"https://api.marketaux.com/v1/news/all"
               f"?language=en"
               f"&published_after={extended_start}"
               f"&limit=100"
               f"&page={page}"
               f"&sort=published_desc")
        
        headers = {'Authorization': f'Bearer {MY_API_TOKEN}'}
        
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            articles = data.get('data', [])
            
            if articles:
                new_articles = []
                for article in articles:
                    url_key = article.get('url', '')
                    if url_key and url_key not in seen_urls:
                        seen_urls.add(url_key)
                        new_articles.append(article)
                
                all_news_data.extend(new_articles)
                print(f"    -> {len(new_articles)}ê°œ ì¶”ê°€. ì´ {len(all_news_data)}ê°œ")
                
                if len(all_news_data) >= 500:
                    print("    âœ… ëª©í‘œ 500ê°œ ë‹¬ì„±!")
                    break
            else:
                break
            
            page += 1
            time.sleep(0.3)
            
        except requests.exceptions.RequestException as e:
            print(f"    âŒ ì¶”ê°€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            break


if __name__ == "__main__":
    get_weekly_news_and_save_to_csv()