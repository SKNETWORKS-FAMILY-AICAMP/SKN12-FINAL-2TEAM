# Router.py  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from __future__ import annotations

import json, os
from typing import List, Dict, Any

from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel

from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langgraph.graph import StateGraph, MessagesState, END
from langgraph.prebuilt import ToolNode

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. í™˜ê²½ ë³€ìˆ˜
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not (OPENAI_API_KEY and OPENAI_API_KEY.startswith("sk-")):
    raise ValueError("âŒ ìœ íš¨í•œ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ToolCall ëª¨ë¸
class ToolCall(BaseModel):
    name: str
    arguments: Dict[str, Any]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. Router (í•„ìš” ì‹œ ì‚¬ìš©)
class Router:
    """OpenAI í•¨ìˆ˜-ì½œë¡œ ì–´ë–¤ íˆ´ì„ ì“¸ì§€ ê²°ì •í•˜ëŠ” ë¼ìš°í„°"""
    def __init__(
        self,
        tool_specs: List[Dict[str, Any]],
        *,
        model: str = "gpt-4o-mini",
        api_key: str | None = None,
        system_prompt: str | None = None,
    ):
        self.tool_specs = tool_specs
        self.client = OpenAI(api_key=api_key or OPENAI_API_KEY)
        self.model = model
        self.system_prompt = system_prompt or (
            "You are a router that decides which tools the assistant should call."
        )

    def route(self, query: str) -> List[ToolCall]:
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": query},
        ]
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            tools=self.tool_specs,
            temperature=0,
        )
        calls = resp.choices[0].message.tool_calls or []
        out: List[ToolCall] = []
        for tc in calls:
            try:
                out.append(ToolCall(name=tc.function.name,
                                    arguments=json.loads(tc.function.arguments)))
            except Exception:
                pass
        return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. íˆ´ ì •ì˜
@tool
def web_search(query: str, k: int = 5) -> str:
    """ì¸í„°ë„· ë‰´ìŠ¤/ì •ë³´ ê²€ìƒ‰"""
    from tool.newsAPI import GNewsClient  # local import
    return GNewsClient().get(keyword=query, max_results=k)

@tool
def financial_statements(ticker: str, limit: int = 3) -> str:
    """ì¢…ëª© ìž¬ë¬´ì œí‘œ ì¡°íšŒ"""
    from tool.financial_statements import IncomeStatementClient
    data = IncomeStatementClient().get(ticker, limit)
    return f"{ticker}: {data}"

TOOLS = [web_search, financial_statements]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. LLM (íˆ´ ë°”ì¸ë”©)
llm = ChatOpenAI(
    model="gpt-4o-mini",
    api_key=OPENAI_API_KEY,
    temperature=0,
)
llm_with_tools = llm.bind_tools(TOOLS)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. LangGraph ì›Œí¬í”Œë¡œ
def should_continue(state: MessagesState):
    """ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ì— tool_calls ê°€ ìžˆìœ¼ë©´ íˆ´ ì‹¤í–‰ ë‹¨ê³„ë¡œ"""
    last = state["messages"][-1]
    return "tools" if getattr(last, "tool_calls", None) else END

def call_model(state: MessagesState):
    msgs = state["messages"]
    resp = llm_with_tools.invoke(msgs)
    return {"messages": [resp]}

def build_workflow():
    builder = StateGraph(MessagesState)
    tool_node = ToolNode(TOOLS)                      # â˜… agent ì¸ìž ì—†ìŒ :contentReference[oaicite:0]{index=0}
    builder.add_node("call_model", call_model)
    builder.add_node("tools", tool_node)

    from langgraph.graph import START
    builder.add_edge(START, "call_model")
    builder.add_conditional_edges("call_model", should_continue, ["tools", END])
    builder.add_edge("tools", "call_model")

    return builder.compile()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. CLI
if __name__ == "__main__":
    print("ðŸ”§ LangGraph Tool Router CLI (Ctrl-C to exit)")
    graph = build_workflow()

    try:
        while True:
            q = input("\nQuestion > ").strip()
            if not q:
                continue
            result = graph.invoke({"messages": [{"role": "user", "content": q}]})
            for m in result["messages"]:
                print("â†’", getattr(m, "content", m))
    except (EOFError, KeyboardInterrupt):
        print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
