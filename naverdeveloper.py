import os
import requests
from dotenv import load_dotenv
import json
import csv
from datetime import datetime
import time
import re
import html

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜(API í‚¤)ë¥¼ ë¶ˆëŸ¬ì˜´
load_dotenv()
NAVER_CLIENT_ID = os.getenv('NAVER_CLIENT_ID')
NAVER_CLIENT_SECRET = os.getenv('NAVER_CLIENT_SECRET')

def clean_naver_text(text):
    """ë„¤ì´ë²„ API í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
    if not text or text.strip() == "":
        return ""
    
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = html.unescape(text)
    
    # HTML íƒœê·¸ ì œê±° (ë„¤ì´ë²„ API íŠ¹ì„±)
    text = re.sub(r'<[^>]+>', '', text)
    
    # ë„¤ì´ë²„ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
    text = text.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    
    # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    text = text.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
    
    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
    patterns_to_remove = [
        r'ê¸°ì‚¬\s*ì›ë¬¸',           # "ê¸°ì‚¬ ì›ë¬¸"
        r'ë”\s*ë³´ê¸°',            # "ë” ë³´ê¸°"
        r'ìì„¸íˆ\s*ë³´ê¸°',         # "ìì„¸íˆ ë³´ê¸°"
        r'ê´€ë ¨\s*ê¸°ì‚¬',          # "ê´€ë ¨ ê¸°ì‚¬"
        r'ì´\s*ê¸°ì‚¬ë¥¼',          # "ì´ ê¸°ì‚¬ë¥¼"
        r'ë‰´ìŠ¤\s*ì œë³´',          # "ë‰´ìŠ¤ ì œë³´"
        r'ì‚¬ì§„\s*=',            # "ì‚¬ì§„ = "
        r'ê·¸ë˜í”½\s*=',          # "ê·¸ë˜í”½ = "
        r'ì˜ìƒ\s*=',            # "ì˜ìƒ = "
        r'\[.*?\]',             # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš©
        r'\(.*?\)',             # ì†Œê´„í˜¸ ì•ˆì˜ ë‚´ìš© (ì§§ì€ ê²ƒë§Œ)
        r'ã€.*?ã€‘',              # ì¼ë³¸ì‹ ê´„í˜¸
        r'â—†.*?â—†',              # íŠ¹ìˆ˜ ê¸°í˜¸
        r'â–¶.*?â—€',              # íŠ¹ìˆ˜ ê¸°í˜¸
        r'â€».*?â€»',              # íŠ¹ìˆ˜ ê¸°í˜¸
        r'â˜….*?â˜…',              # ë³„í‘œ
        r'â—.*?â—',              # ì›í˜• ê¸°í˜¸
        r'â– .*?â– ',              # ì‚¬ê°í˜• ê¸°í˜¸
        r'â–².*?â–²',              # ì‚¼ê°í˜• ê¸°í˜¸
        r'=\s*ì—°í•©ë‰´ìŠ¤',        # "= ì—°í•©ë‰´ìŠ¤"
        r'=\s*ë‰´ì‹œìŠ¤',          # "= ë‰´ì‹œìŠ¤"
        r'=\s*ë‰´ìŠ¤1',           # "= ë‰´ìŠ¤1"
        r'ê¸°ì\s*$',            # ëì— ë‚˜ì˜¤ëŠ” "ê¸°ì"
        r'íŠ¹íŒŒì›\s*$',          # ëì— ë‚˜ì˜¤ëŠ” "íŠ¹íŒŒì›"
        r'@.*?\..*?$',          # ì´ë©”ì¼ ì£¼ì†Œ
        r'\.{3,}$',             # ëì— ìˆëŠ” "..." (3ê°œ ì´ìƒ ì—°ì† ì )
        r'â€¦+$',                 # ëì— ìˆëŠ” "â€¦" (ì¤„ì„í‘œ)
    ]
    
    for pattern in patterns_to_remove:
        text = re.sub(pattern, ' ', text, flags=re.IGNORECASE)
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì••ì¶•
    text = re.sub(r'\s+', ' ', text)
    
    # CSV êµ¬ë¶„ì ì¶©ëŒ ë°©ì§€: ìŠ¬ë˜ì‹œ(/)ë¥¼ ë‹¤ë¥¸ ë¬¸ìë¡œ ì¹˜í™˜
    text = text.replace('/', 'ï½œ')  # ì „ê° íŒŒì´í”„ ë¬¸ìë¡œ ì¹˜í™˜
    
    # ì–‘ ë ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def extract_source_name(original_link):
    """ì›ë³¸ ë§í¬ì—ì„œ ì–¸ë¡ ì‚¬ ì´ë¦„ ì¶”ì¶œ"""
    if not original_link:
        return ""
    
    try:
        # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        domain_match = re.search(r'https?://(?:www\.)?([^/]+)', original_link)
        if not domain_match:
            return ""
        
        domain = domain_match.group(1)
        
        # ì–¸ë¡ ì‚¬ ë„ë©”ì¸ ë§¤í•‘ (êµ­ë‚´ + í•´ì™¸)
        source_mapping = {
            # í•œêµ­ ì–¸ë¡ ì‚¬
            'yna.co.kr': 'ì—°í•©ë‰´ìŠ¤',
            'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',
            'newsis.com': 'ë‰´ì‹œìŠ¤',
            'news1.kr': 'ë‰´ìŠ¤1',
            'mk.co.kr': 'ë§¤ì¼ê²½ì œ',
            'hankyung.com': 'í•œêµ­ê²½ì œ',
            'sedaily.com': 'ì„œìš¸ê²½ì œ',
            'edaily.co.kr': 'ì´ë°ì¼ë¦¬',
            'etnews.com': 'ETë‰´ìŠ¤',
            'dt.co.kr': 'ë””ì§€í„¸íƒ€ì„ìŠ¤',
            'zdnet.co.kr': 'ZDNet',
            'bloter.net': 'ë¸”ë¡œí„°',
            'chosun.com': 'ì¡°ì„ ì¼ë³´',
            'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',
            'donga.com': 'ë™ì•„ì¼ë³´',
            'hani.co.kr': 'í•œê²¨ë ˆ',
            'khan.co.kr': 'ê²½í–¥ì‹ ë¬¸',
            'kbs.co.kr': 'KBS',
            'sbs.co.kr': 'SBS',
            'mbc.co.kr': 'MBC',
            'jtbc.co.kr': 'JTBC',
            'tvchosun.com': 'TVì¡°ì„ ',
            'mtn.co.kr': 'MTN',
            
            # í•´ì™¸ ì£¼ìš” ì–¸ë¡ ì‚¬
            'reuters.com': 'Reuters',
            'cnn.com': 'CNN',
            'bbc.com': 'BBC',
            'bbc.co.uk': 'BBC',
            'bloomberg.com': 'Bloomberg',
            'wsj.com': 'Wall Street Journal',
            'nytimes.com': 'New York Times',
            'ft.com': 'Financial Times',
            'cnbc.com': 'CNBC',
            'marketwatch.com': 'MarketWatch',
            'forbes.com': 'Forbes',
            'ap.org': 'Associated Press',
            'apnews.com': 'Associated Press',
            'yahoo.com': 'Yahoo News',
            'finance.yahoo.com': 'Yahoo Finance',
            'techcrunch.com': 'TechCrunch',
            'theverge.com': 'The Verge',
            'engadget.com': 'Engadget',
            'arstechnica.com': 'Ars Technica',
            'wired.com': 'Wired',
            'guardian.co.uk': 'The Guardian',
            'theguardian.com': 'The Guardian',
            'independent.co.uk': 'The Independent',
            'telegraph.co.uk': 'The Telegraph',
            'economist.com': 'The Economist',
            'time.com': 'Time',
            'newsweek.com': 'Newsweek',
            'usatoday.com': 'USA Today',
            'washingtonpost.com': 'Washington Post',
            'latimes.com': 'Los Angeles Times',
            'abcnews.go.com': 'ABC News',
            'cbsnews.com': 'CBS News',
            'nbcnews.com': 'NBC News',
            'foxnews.com': 'Fox News',
            'business.com': 'Business.com',
            'businessinsider.com': 'Business Insider',
            'investopedia.com': 'Investopedia',
            'fool.com': 'The Motley Fool',
            'seekingalpha.com': 'Seeking Alpha',
            'benzinga.com': 'Benzinga',
            'zacks.com': 'Zacks',
            'morningstar.com': 'Morningstar',
            'barrons.com': 'Barrons',
            'investor.com': 'Investors Business Daily',
            'thestreet.com': 'TheStreet',
            'moneycontrol.com': 'MoneyControl',
            'livemint.com': 'LiveMint',
            'economictimes.indiatimes.com': 'Economic Times',
            'nikkei.com': 'Nikkei',
            'japantimes.co.jp': 'Japan Times',
            'scmp.com': 'South China Morning Post',
            'straitstimes.com': 'Straits Times',
            'channelnewsasia.com': 'Channel NewsAsia',
            'aljazeera.com': 'Al Jazeera',
            'dw.com': 'Deutsche Welle',
            'france24.com': 'France 24',
            'euronews.com': 'Euronews',
            'rt.com': 'RT',
            'sputniknews.com': 'Sputnik News',
        }
        
        # ë§¤í•‘ëœ ì–¸ë¡ ì‚¬ ì´ë¦„ ì°¾ê¸°
        for domain_key, source_name in source_mapping.items():
            if domain_key in domain:
                return source_name
        
        # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° ë„ë©”ì¸ì—ì„œ ì¶”ì¶œ
        domain_parts = domain.split('.')
        if len(domain_parts) >= 2:
            return domain_parts[0].capitalize()
        
        return domain
        
    except Exception:
        return ""

def preprocess_naver_news(news_items):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬"""
    print("ğŸ”„ ë„¤ì´ë²„ ë‰´ìŠ¤ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    original_count = len(news_items)
    
    # 1. í…ìŠ¤íŠ¸ ì •ì œ ë° ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ
    for item in news_items:
        item['title'] = clean_naver_text(item.get('title', ''))
        item['description'] = clean_naver_text(item.get('description', ''))
        item['source_name'] = extract_source_name(item.get('originallink', ''))
    
    # 2. ê²°ì¸¡ì¹˜ ì œê±° (ì œëª©ì´ë‚˜ ìš”ì•½ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°)
    news_items = [item for item in news_items 
                  if item.get('title', '').strip() and 
                     item.get('description', '').strip()]
    
    after_missing_count = len(news_items)
    print(f"  â†’ ê²°ì¸¡ì¹˜ ì œê±°: {original_count - after_missing_count}ê°œ ì œê±°")
    
    # 3. ë‹¤ë‹¨ê³„ ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ (ìµœê³  ì •í™•ë„)
    print("  ğŸ”„ ë‹¤ë‹¨ê³„ ì¤‘ë³µ ì œê±° ì‹œì‘...")
    
    # 3-1ë‹¨ê³„: ì™„ì „ ë™ì¼ ì œëª© ì œê±°
    stage1_news = exact_title_removal(news_items)
    print(f"    â†’ 1ë‹¨ê³„ (ì™„ì „ë™ì¼): {len(news_items) - len(stage1_news)}ê°œ ì œê±°")
    
    # 3-2ë‹¨ê³„: ì •ê·œí™”ëœ ì œëª©ìœ¼ë¡œ ì œê±°
    stage2_news = normalized_title_removal(stage1_news)
    print(f"    â†’ 2ë‹¨ê³„ (ì •ê·œí™”): {len(stage1_news) - len(stage2_news)}ê°œ ì œê±°")
    
    # 3-3ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ì œê±°
    stage3_news = similarity_based_removal(stage2_news)
    print(f"    â†’ 3ë‹¨ê³„ (ìœ ì‚¬ë„): {len(stage2_news) - len(stage3_news)}ê°œ ì œê±°")
    
    # 3-4ë‹¨ê³„: í‚¤ì›Œë“œ íŒ¨í„´ ê¸°ë°˜ ì œê±°
    stage4_news = keyword_pattern_removal(stage3_news)
    print(f"    â†’ 4ë‹¨ê³„ (í‚¤ì›Œë“œ): {len(stage3_news) - len(stage4_news)}ê°œ ì œê±°")
    
    # 3-5ë‹¨ê³„: ì–¸ë¡ ì‚¬ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìµœì¢… ì„ íƒ
    final_news = priority_source_selection(stage4_news)
    print(f"    â†’ 5ë‹¨ê³„ (ìš°ì„ ìˆœìœ„): {len(stage4_news) - len(final_news)}ê°œ ì œê±°")
    
    final_count = len(final_news)
    total_removed = after_missing_count - final_count
    removal_rate = (total_removed / after_missing_count * 100) if after_missing_count > 0 else 0
    
    print(f"  âœ… ì´ ì¤‘ë³µ ì œê±°: {total_removed}ê°œ ì œê±° (ì œê±°ìœ¨: {removal_rate:.1f}%)")
    print(f"  â†’ ìµœì¢… ë°ì´í„° ê°œìˆ˜: {final_count}ê°œ")
    
    return final_news

def exact_title_removal(news_items):
    """1ë‹¨ê³„: ì™„ì „íˆ ë™ì¼í•œ ì œëª© ì œê±°"""
    seen_exact = set()
    unique_news = []
    
    for item in news_items:
        title = item.get('title', '').strip()
        if title and title not in seen_exact:
            seen_exact.add(title)
            unique_news.append(item)
    
    return unique_news

def normalized_title_removal(news_items):
    """2ë‹¨ê³„: ì •ê·œí™”ëœ ì œëª©ìœ¼ë¡œ ì œê±° (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì, ìˆ«ì ë¬´ì‹œ)"""
    seen_normalized = set()
    unique_news = []
    
    for item in news_items:
        title = item.get('title', '').strip()
        
        # ê°•ë ¥í•œ ì •ê·œí™”
        normalized = title.lower()
        normalized = re.sub(r'[^\wê°€-í£]', '', normalized)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        normalized = re.sub(r'\d+', 'NUM', normalized)       # ìˆ«ì ì¼ë°˜í™”
        normalized = re.sub(r'(ì£¼ê°€|stock|price|ê¸‰ë“±|ê¸‰ë½|ìƒìŠ¹|í•˜ë½|ë°œí‘œ|ê³µê°œ)', 'KEY', normalized)  # ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ ì¼ë°˜í™”
        
        if normalized and len(normalized) > 10 and normalized not in seen_normalized:
            seen_normalized.add(normalized)
            unique_news.append(item)
    
    return unique_news

def similarity_based_removal(news_items):
    """3ë‹¨ê³„: ìœ ì‚¬ë„ ê¸°ë°˜ ì œê±° (85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ)"""
    from difflib import SequenceMatcher
    
    unique_news = []
    
    for current_item in news_items:
        current_title = current_item.get('title', '').strip()
        is_duplicate = False
        
        # ê¸°ì¡´ ì•„ì´í…œë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
        for existing_item in unique_news:
            existing_title = existing_item.get('title', '').strip()
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarity = SequenceMatcher(None, current_title.lower(), existing_title.lower()).ratio()
            
            if similarity >= 0.85:  # 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_news.append(current_item)
    
    return unique_news

def keyword_pattern_removal(news_items):
    """4ë‹¨ê³„: í•µì‹¬ í‚¤ì›Œë“œ íŒ¨í„´ ê¸°ë°˜ ì œê±°"""
    seen_patterns = set()
    unique_news = []
    
    for item in news_items:
        title = item.get('title', '').strip()
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£A-Za-z]{2,}', title)  # 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'ê´€ë ¨', 'ë°œí‘œ', 'ì†Œì‹', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ë³´ë„', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ìµœê·¼', 'ìƒˆë¡œìš´', 'ìµœì‹ '}
        keywords = [k for k in keywords if k.lower() not in stop_words]
        
        # ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë¡œ íŒ¨í„´ ìƒì„±
        if len(keywords) >= 3:
            pattern = '|'.join(sorted(keywords[:5]))
            
            if pattern not in seen_patterns:
                seen_patterns.add(pattern)
                unique_news.append(item)
        else:
            # í‚¤ì›Œë“œê°€ ì ìœ¼ë©´ ì›ë³¸ ì œëª©ìœ¼ë¡œ íŒë‹¨
            if title not in [item.get('title', '') for item in unique_news]:
                unique_news.append(item)
    
    return unique_news

def priority_source_selection(news_items):
    """5ë‹¨ê³„: ì–¸ë¡ ì‚¬ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìµœì¢… ì„ íƒ"""
    
    # ì–¸ë¡ ì‚¬ ì‹ ë¢°ë„/ìš°ì„ ìˆœìœ„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìš°ì„ )
    source_priority = {
        # í•´ì™¸ ì£¼ìš” ì–¸ë¡ ì‚¬ (ìµœê³  ìš°ì„ ìˆœìœ„)
        'Reuters': 20, 'Bloomberg': 19, 'Wall Street Journal': 18,
        'Financial Times': 17, 'CNN': 16, 'BBC': 15,
        'CNBC': 14, 'MarketWatch': 13, 'Forbes': 12,
        
        # êµ­ë‚´ ì£¼ìš” ê²½ì œì§€
        'ì—°í•©ë‰´ìŠ¤': 10, 'ë§¤ì¼ê²½ì œ': 9, 'í•œêµ­ê²½ì œ': 9,
        'ì„œìš¸ê²½ì œ': 8, 'ì´ë°ì¼ë¦¬': 8, 'ë‰´ì‹œìŠ¤': 7,
        'ë‰´ìŠ¤1': 6, 'ETë‰´ìŠ¤': 6,
        
        # ê¸°íƒ€ ì–¸ë¡ ì‚¬
        'KBS': 5, 'SBS': 5, 'MBC': 5, 'JTBC': 4
    }
    
    # ë¹„ìŠ·í•œ ë‰´ìŠ¤ë¼ë¦¬ ê·¸ë£¹í™”
    similar_groups = []
    
    for item in news_items:
        title = item.get('title', '').strip()
        
        # ê¸°ì¡´ ê·¸ë£¹ê³¼ ìœ ì‚¬í•œì§€ í™•ì¸
        added_to_group = False
        for group in similar_groups:
            if group and is_similar_enough(title, group[0].get('title', '')):
                group.append(item)
                added_to_group = True
                break
        
        # ìƒˆ ê·¸ë£¹ ìƒì„±
        if not added_to_group:
            similar_groups.append([item])
    
    # ê° ê·¸ë£¹ì—ì„œ ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ ì–¸ë¡ ì‚¬ ì„ íƒ
    final_news = []
    for group in similar_groups:
        if len(group) == 1:
            final_news.append(group[0])
        else:
            # ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ ì•„ì´í…œ ì„ íƒ
            best_item = max(group, key=lambda x: source_priority.get(x.get('source_name', ''), 0))
            final_news.append(best_item)
    
    return final_news

def is_similar_enough(title1, title2, threshold=0.75):
    """ë‘ ì œëª©ì´ ì¶©ë¶„íˆ ìœ ì‚¬í•œì§€ íŒë‹¨"""
    from difflib import SequenceMatcher
    return SequenceMatcher(None, title1.lower(), title2.lower()).ratio() >= threshold

def get_naver_news(keyword):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ ì´ìš©í•´ íŠ¹ì • í‚¤ì›Œë“œì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        print("âŒ API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return []

    print(f"âœ… '{keyword}' í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
    url = "https://openapi.naver.com/v1/search/news.json"
    params = {
        "query": keyword, 
        "display": 50, 
        "sort": "date",
        "start": 1  # ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜ ëª…ì‹œ
    }
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }

    try:
        response = requests.get(url, params=params, headers=headers)
        response.raise_for_status()
        
        # ê²€ìƒ‰ëœ ê° ë‰´ìŠ¤ì— 'ê²€ìƒ‰ì–´' í•„ë“œ ì¶”ê°€
        items = response.json()['items']
        for item in items:
            item['search_keyword'] = keyword
        return items
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ '{keyword}' ê²€ìƒ‰ ì¤‘ API ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def save_to_csv(all_items):
    """ìˆ˜ì§‘ëœ ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    if not all_items:
        print("âŒ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” í´ë”ì— ì €ì¥ (marketaux.pyì™€ ë™ì¼í•˜ê²Œ)
    current_dir = os.path.dirname(os.path.abspath(__file__))
    filename = os.path.join(current_dir, f"ë„¤ì´ë²„ë‰´ìŠ¤_ëª¨ìŒ_{datetime.now().strftime('%Y%m%d')}.csv")

    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f, delimiter='/')  # êµ¬ë¶„ìë¥¼ ìŠ¬ë˜ì‹œ(/)ë¡œ ë³€ê²½
            # CSV í—¤ë” ì‘ì„± (ì–¸ë¡ ì‚¬ ì •ë³´ í¬í•¨)
            writer.writerow(['ê²€ìƒ‰ì–´', 'ì œëª©', 'ìš”ì•½', 'ì–¸ë¡ ì‚¬', 'ë§í¬', 'ë‚ ì§œ'])
            
            for item in all_items:
                # ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©
                title = item.get('title', '')
                description = item.get('description', '')
                source_name = item.get('source_name', '')
                
                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                try:
                    pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900').strftime('%Y-%m-%d %H:%M')
                except:
                    pub_date = item.get('pubDate', '')

                writer.writerow([
                    item.get('search_keyword', ''),
                    title,
                    description,
                    source_name,  # ì¶”ì¶œëœ ì–¸ë¡ ì‚¬ ì´ë¦„
                    item.get('originallink', ''),  # ì›ë¬¸ ë§í¬
                    pub_date
                ])
        
        print("-" * 50)
        print(f"âœ… ì´ {len(all_items)}ê°œì˜ ì „ì²˜ë¦¬ëœ ë‰´ìŠ¤ë¥¼ '{os.path.basename(filename)}' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {filename}")
        print("ğŸ”§ ì „ì²˜ë¦¬ ì™„ë£Œ: í…ìŠ¤íŠ¸ ì •ì œ, ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ, ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ì œê±° ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    # 1. ë‰´ìŠ¤ë¡œ ê²€ìƒ‰í•˜ê³  ì‹¶ì€ í‚¤ì›Œë“œ ì˜ˆì‹œ
    search_keywords = ["semiconductor", "cloud computing", "global stock", "GAM", "ì„¸ê³„ ì£¼ì‹", "US stock market", 
    "Wall Street", "Federal Reserve", "oil price", "dollar index", "tech stocks", "inflation"]
    
    # ì‚¬ìš©ìê°€ í‚¤ì›Œë“œë¥¼ ì§ì ‘ ì…ë ¥í•˜ê³  ì‹¶ì€ ê²½ìš°
    user_input = input("í‚¤ì›Œë“œë¥¼ ì§ì ‘ ì…ë ¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
    if user_input == 'y' or user_input == 'yes':
        custom_keywords = input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë“¤ì„ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•˜ì„¸ìš”: ")
        search_keywords = [keyword.strip() for keyword in custom_keywords.split(',')]
    
    print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(search_keywords)}")
    print("-" * 50)
    
    all_news_items = []
    
    # 2. ëª©ë¡ì— ìˆëŠ” ê° ê¸°ì—…ì— ëŒ€í•´ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰
    for keyword in search_keywords:
        news_items = get_naver_news(keyword)
        all_news_items.extend(news_items) # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        time.sleep(3) # API ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šê¸° ìœ„í•´ ìš”ì²­ ì‚¬ì´ì— 1ì´ˆ ëŒ€ê¸°

    # 3. ë°ì´í„° ì „ì²˜ë¦¬
    print(f"\nğŸ”„ ì „ì²˜ë¦¬ ì „ ë°ì´í„° ê°œìˆ˜: {len(all_news_items)}ê°œ")
    all_news_items = preprocess_naver_news(all_news_items)

    # 4. ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ì €ì¥
    save_to_csv(all_news_items)