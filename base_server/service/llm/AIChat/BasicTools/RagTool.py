import os
import json
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
import uuid
import logging
import datetime
import contextvars
import re
from typing import Optional, List, Dict, Any
from service.llm.AIChat.BaseFinanceTool import BaseFinanceTool
from service.rag.rag_config import RagConfig
from service.rag.rag_service import RagService
from pydantic import BaseModel, Field

# =========================
# Logging helpers (RAG)
# =========================
_request_id_var: contextvars.ContextVar[str] = contextvars.ContextVar("request_id", default="")
_RAG_LOG_FORMAT = "plain"
_RAG_LOG_CONTENT_MAXLEN = 200

def init_rag_logging() -> None:
    """Initialize structured logging for RAG once (idempotent)."""
    global _RAG_LOG_FORMAT, _RAG_LOG_CONTENT_MAXLEN
    level_name = os.getenv("RAG_LOG_LEVEL", "INFO").upper()
    level = getattr(logging, level_name, logging.INFO)
    _RAG_LOG_FORMAT = os.getenv("RAG_LOG_FORMAT", "plain").lower()
    try:
        _RAG_LOG_CONTENT_MAXLEN = int(os.getenv("RAG_LOG_CONTENT_MAXLEN", "200"))
    except Exception:
        _RAG_LOG_CONTENT_MAXLEN = 200

    logger = logging.getLogger(__name__)
    if getattr(logger, "_rag_initialized", False):
        return
    logger.setLevel(level)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setLevel(level)
        handler.setFormatter(logging.Formatter("%(message)s"))
        
        class OnlyTestLogFilter(logging.Filter):
            def filter(self, record: logging.LogRecord) -> bool:
                try:
                    msg = record.getMessage()
                except Exception:
                    msg = str(record.msg)
                return "log.test" in msg

        handler.addFilter(OnlyTestLogFilter())
        logger.addHandler(handler)
    logger.propagate = False
    setattr(logger, "_rag_initialized", True)

def _now_ts() -> str:
    return datetime.datetime.utcnow().isoformat(timespec="seconds") + "Z"

def _set_request_id(request_id: str) -> None:
    _request_id_var.set(request_id)

def _reset_request_id() -> None:
    try:
        _request_id_var.set("")
    except Exception:
        pass

def _truncate_text(text: str) -> str:
    if text is None:
        return ""
    if len(text) <= _RAG_LOG_CONTENT_MAXLEN:
        return text
    return text[:_RAG_LOG_CONTENT_MAXLEN] + "..."

def _emit_log(logger: logging.Logger, level: int, event: str, fields: Optional[Dict[str, Any]] = None, exc_info: bool = False) -> None:
    payload: Dict[str, Any] = {
        "level": logging.getLevelName(level),
        "ts": _now_ts(),
        "request_id": _request_id_var.get() or None,
        "event": event,
    }
    if fields:
        payload.update(fields)

    if _RAG_LOG_FORMAT == "json":
        try:
            message = json.dumps({k: v for k, v in payload.items() if v is not None}, ensure_ascii=False)
        except Exception:
            message = json.dumps({"level": payload.get("level"), "ts": payload.get("ts"), "event": event}, ensure_ascii=False)
    else:
        # plain key=value with quoted strings
        def _fmt_val(v: Any) -> str:
            if isinstance(v, (int, float)):
                return str(v)
            if isinstance(v, bool):
                return "true" if v else "false"
            if v is None:
                return "null"
            s = str(v)
            s = s.replace('"', '\\"')
            return f'"{s}"'
        parts = [
            f"level={payload['level']}",
            f"ts={payload['ts']}",
        ]
        if payload.get("request_id"):
            parts.append(f"request_id={payload['request_id']}")
        parts.append(f"event={_fmt_val(event)}")
        for k, v in payload.items():
            if k in ("level", "ts", "request_id", "event"):
                continue
            parts.append(f"{k}={_fmt_val(v)}")
        message = " ".join(parts)
    logger.log(level, message, exc_info=exc_info)

def _run_async(coro):
    """Run coroutine safely from sync context, even if an event loop is already running."""
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        return asyncio.run(coro)
    # If we're already inside an event loop, run in a separate thread with its own loop
    def runner():
        return asyncio.run(coro)
    with ThreadPoolExecutor(max_workers=1) as ex:
        future = ex.submit(runner)
        return future.result()

class RagInput(BaseModel):
    query: str = Field(..., description="ê²€ìƒ‰í•  ì§ˆë¬¸ ë˜ëŠ” í‚¤ì›Œë“œ. ì˜ì–´ë¡œë§Œ. (ì˜ˆ: 'finance', 'TSLA')")
    k: int = Field(5, description="ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)")
    threshold: float = Field(0.1, description="ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.1)")

class RagOutput(BaseModel):
    agent: str
    summary: str
    documents: Optional[List[Dict[str, Any]]] = None
    data: Optional[Any] = None

    def __init__(
        self,
        agent: str,
        summary: str,
        documents: Optional[List[Dict[str, Any]]] = None,
        data: Optional[Any] = None,
    ):
        super().__init__(
            agent=agent,
            summary=summary,
            documents=documents,
            data=data,
        )

class RagTool(BaseFinanceTool):
    
    def __init__(self, ai_chat_service, rag_config: Optional[RagConfig] = None):
        init_rag_logging()
        logger = logging.getLogger(__name__)
        from service.llm.AIChat_service import AIChatService
        if not isinstance(ai_chat_service, AIChatService):
            raise TypeError("Expected AIChatService instance")
        self.ai_chat_service = ai_chat_service
        # ìš°ì„ ìˆœìœ„: ì „ë‹¬ëœ rag_config â†’ ì„œë¹„ìŠ¤ ë³´ê´€ rag_config â†’ (ë‚˜ì¤‘ì—) ì „ì—­ RAG ì„¤ì •
        self.rag_config: Optional[RagConfig] = rag_config or getattr(ai_chat_service, 'rag_config', None)
        
        # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (OpenSearch/VectorDbService ê¸°ë°˜)
        try:
            _emit_log(logging.getLogger(__name__), logging.INFO, "rag.service.init.start")
            if not RagService.is_initialized():
                # í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì„¤ì •ì„ êµ¬ì„± (ì§€ì—° ìƒì„±)
                config_candidate = self.rag_config or getattr(ai_chat_service, 'rag_config', None)
                if config_candidate is None:
                    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìê²© ì¦ëª… ìœ ë¬´ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ êµ¬ì„±
                    aws_key = os.getenv('AWS_ACCESS_KEY_ID', os.getenv('AWS_ACCESS_KEY', '')).strip()
                    aws_secret = os.getenv('AWS_SECRET_ACCESS_KEY', '').strip()
                    kb_id = os.getenv('RAG_KB_ID', '').strip()
                    enable_vector = bool(aws_key and aws_secret and kb_id)
                    try:
                        self.rag_config = RagConfig(
                            aws_access_key_id=aws_key,
                            aws_secret_access_key=aws_secret,
                            knowledge_base_id=kb_id,
                            enable_vector_db=enable_vector
                        )
                    except Exception:
                        # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ë¡œì»¬/BM25ë§Œ ì‚¬ìš©í•˜ë„ë¡ ë¹„í™œì„±í™”
                        self.rag_config = RagConfig(enable_vector_db=False)
                else:
                    # ì œê³µëœ ì„¤ì •ì—ì„œ ìê²©ì¦ëª… ë¯¸ë¹„ ì‹œ ë²¡í„° DB ë¹„í™œì„±í™”ë¡œ ê°•ë“±
                    try:
                        if getattr(config_candidate, 'enable_vector_db', True) and (
                            not getattr(config_candidate, 'aws_access_key_id', '').strip() or
                            not getattr(config_candidate, 'aws_secret_access_key', '').strip() or
                            not getattr(config_candidate, 'knowledge_base_id', '').strip()
                        ):
                            self.rag_config = config_candidate.model_copy(update={"enable_vector_db": False})
                        else:
                            self.rag_config = config_candidate
                    except Exception:
                        # ì–´ë–¤ ì´ìœ ë¡œë“  ë³µì‚¬ ì‹¤íŒ¨ ì‹œ ì•ˆì „ ê¸°ë³¸ê°’
                        self.rag_config = RagConfig(enable_vector_db=False)
                # ìµœì¢… êµ¬ì„± í™•ì¸ìš© ë””ë²„ê·¸
                try:
                    from service.rag.rag_service import RagService as _RS
                    _ = self.rag_config.dict() if hasattr(self.rag_config, 'dict') else self.rag_config.__dict__
                    _emit_log(logging.getLogger(__name__), logging.DEBUG, "rag.config.active", {
                        "enable_vector_db": str(self.rag_config.enable_vector_db),
                        "kb_id_set": str(bool(getattr(self.rag_config, 'knowledge_base_id', ''))),
                        "region": getattr(self.rag_config, 'region_name', ''),
                    })
                except Exception:
                    pass
                RagService.init(self.rag_config)
            _emit_log(logging.getLogger(__name__), logging.INFO, "rag.service.init.done")
        except Exception as e:
            _emit_log(logging.getLogger(__name__), logging.ERROR, "rag.service.init.error", {"message": str(e)}, exc_info=True)

        # ë³´ì¥: self.rag_configê°€ í•­ìƒ ì¡´ì¬í•˜ë„ë¡ ìµœì¢… í™•ì¸
        try:
            if self.rag_config is None:
                from service.rag.rag_service import RagService as _RS
                self.rag_config = _RS.get_config() or RagConfig(enable_vector_db=False)
        except Exception:
            self.rag_config = RagConfig(enable_vector_db=False)
        
        # ë” ì´ìƒ ë¡œì»¬ Chroma ì´ˆê¸°í™” ì‚¬ìš© ì•ˆ í•¨
        self.embedding_model = None
        self.collection = None
    
    def _initialize_vector_db(self):
        """ì´ì „ Chroma ì´ˆê¸°í™” ë©”ì„œë“œ(í˜¸í™˜ì„± ìœ ì§€). í˜„ì¬ëŠ” ë¹„í™œì„±."""
        self.embedding_model = None
        self.collection = None

    def _get_embedding(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        logger = logging.getLogger(__name__)
        if self.embedding_model is None:
            _emit_log(
                logger=logger,
                level=logging.WARNING,
                event="embedding.skipped",
                fields={"reason": "embedding model not initialized"},
            )
            return []
        
        try:
            start_t = time.perf_counter()
            preview = _truncate_text(text)
            _emit_log(
                logger=logger,
                level=logging.DEBUG,
                event="embedding.start",
                fields={
                    "input_length": len(text),
                    "preview": preview,
                },
            )
            embedding = self.embedding_model.encode(text).tolist()
            elapsed_ms = int((time.perf_counter() - start_t) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="embedding.done",
                fields={
                    "dim": len(embedding) if isinstance(embedding, list) else 0,
                    "elapsed_ms": elapsed_ms,
                },
            )
            return embedding
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="embedding.error",
                fields={"message": f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}"},
                exc_info=True,
            )
            return []

    def _search_vector_db(self, query: str, k: int, threshold: float) -> List[Dict[str, Any]]:
        """ë²¡í„° DBì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        logger = logging.getLogger(__name__)
        if self.collection is None:
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="vector.search.skipped",
                fields={"reason": "vector DB collection not initialized"},
            )
            return []
        
        try:
            # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="vector.search.start",
                fields={
                    "query": _truncate_text(query),
                    "k": k,
                    "threshold": threshold,
                },
            )
            query_embedding = self._get_embedding(query)
            if not query_embedding:
                return []
            
            # ë²¡í„° DBì—ì„œ ê²€ìƒ‰
            q_start_t = time.perf_counter()
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            q_elapsed_ms = int((time.perf_counter() - q_start_t) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="vector.search",
                fields={
                    "k": k,
                    "returned": len(results['documents'][0]) if results.get('documents') and results['documents'] and results['documents'][0] else 0,
                    "elapsed_ms": q_elapsed_ms,
                },
            )
            
            documents = []
            below_threshold = 0
            filter_start = time.perf_counter()
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    distance = results['distances'][0][i] if results['distances'] else 1.0
                    similarity = 1 - distance  # cosine distanceë¥¼ similarityë¡œ ë³€í™˜
                    
                    # ì„ê³„ê°’ ì´ìƒì¸ ë¬¸ì„œë§Œ í¬í•¨
                    if similarity >= threshold:
                        metadata = results['metadatas'][0][i] if results['metadatas'] else {}
                        documents.append({
                            "content": doc,
                            "similarity": round(similarity, 3),
                            "metadata": metadata,
                            "source": metadata.get("source", "unknown"),
                            "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
                            "date": metadata.get("date", "ë‚ ì§œ ì—†ìŒ")
                        })
                        _emit_log(
                            logger=logger,
                            level=logging.DEBUG,
                            event="vector.search.doc",
                            fields={
                                "rank": i + 1,
                                "title": documents[-1]["title"],
                                "source": documents[-1]["source"],
                                "similarity": round(similarity, 3),
                                "distance": round(distance, 3),
                            },
                        )
                    else:
                        below_threshold += 1
            filter_elapsed_ms = int((time.perf_counter() - filter_start) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="filter.apply",
                fields={
                    "threshold": threshold,
                    "kept": len(documents),
                    "dropped": below_threshold,
                    "elapsed_ms": filter_elapsed_ms,
                },
            )
            
            return documents
            
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="vector.search.error",
                fields={"message": f"ë²¡í„° DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}"},
                exc_info=True,
            )
            return []

    def _fallback_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """ë²¡í„° DBê°€ ì—†ì„ ë•Œ ëŒ€ì²´ ê²€ìƒ‰ (ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰)"""
        logger = logging.getLogger(__name__)
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ë°ì´í„°ë¥¼ ë°˜í™˜
        _emit_log(
            logger=logger,
            level=logging.WARNING,
            event="fallback.used",
            fields={"reason": "vector search empty â†’ fallback enabled"},
        )
        fallback_docs = [
            {
                "content": f"'{query}'ì™€ ê´€ë ¨ëœ ê¸ˆìœµ ë‰´ìŠ¤ ë‚´ìš©ì…ë‹ˆë‹¤. ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.",
                "similarity": 0.8,
                "metadata": {
                    "source": "fallback_search",
                    "title": f"{query} ê´€ë ¨ ë‰´ìŠ¤",
                    "date": "2024-01-01"
                },
                "source": "fallback_search",
                "title": f"{query} ê´€ë ¨ ë‰´ìŠ¤", 
                "date": "2024-01-01"
            }
        ]
        docs = fallback_docs[:k]
        _emit_log(
            logger=logger,
            level=logging.INFO,
            event="fallback.return",
            fields={"returned": len(docs)},
        )
        return docs

    def get_data(self, **kwargs) -> RagOutput:
        logger = logging.getLogger(__name__)
        request_id = str(uuid.uuid4())
        _set_request_id(request_id)
        total_start = time.perf_counter()
        try:
            input_data = RagInput(**kwargs)
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="rag.param.error",
                fields={"message": f"ë§¤ê°œë³€ìˆ˜ ì˜¤ë¥˜: {e}"},
                exc_info=True,
            )
            return RagOutput(agent="error", summary=f"âŒ ë§¤ê°œë³€ìˆ˜ ì˜¤ë¥˜: {e}")

        try:
            # ê°€ë…ì„± ë†’ì€ ì‹œì‘ ë””ë²„ê·¸
            try:
                logger.debug(
                    f"log.test ragtool.start query='{str(kwargs.get('query', ''))[:80]}' k={kwargs.get('k', 5)} threshold={kwargs.get('threshold', 0.7)} request_id={request_id}"
                )
            except Exception:
                pass
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="RAG start",
                fields={
                    "query": _truncate_text(input_data.query),
                    "k": input_data.k,
                    "threshold": input_data.threshold,
                    "request_id": request_id,
                },
            )
            # RAG ì„œë¹„ìŠ¤ ê²€ìƒ‰ (OpenSearch/VectorDbService ê¸°ë°˜)
            used_fallback = False
            search_start = time.perf_counter()
            try:
                results = _run_async(RagService.retrieve(
                    query=input_data.query,
                    top_k=input_data.k,
                    hybrid=True
                ))
            except Exception as e:
                _emit_log(
                    logger=logger,
                    level=logging.ERROR,
                    event="rag.service.search.error",
                    fields={"message": str(e)},
                    exc_info=True,
                )
                results = []
            search_elapsed = int((time.perf_counter() - search_start) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="rag.service.search",
                fields={"returned": len(results) if results else 0, "elapsed_ms": search_elapsed},
            )
            try:
                logger.debug(
                    f"log.test ragtool.search returned={len(results) if results else 0} elapsed_ms={search_elapsed}"
                )
            except Exception:
                pass

            # ê²°ê³¼ ì •ê·œí™” ë° ì„ê³„ê°’ í•„í„°ë§
            documents = []
            if results:
                for idx, r in enumerate(results):
                    md = r.get("metadata", {})
                    score = float(r.get("score", 0.0))
                    doc = {
                        "content": r.get("content", ""),
                        "similarity": round(score, 3),
                        "metadata": md,
                        "source": md.get("source", "unknown"),
                        "title": md.get("title", md.get("ticker", "ì œëª© ì—†ìŒ")),
                        "date": md.get("date", "ë‚ ì§œ ì—†ìŒ"),
                    }
                    documents.append(doc)
                    _emit_log(
                        logger=logger,
                        level=logging.DEBUG,
                        event="rag.service.doc",
                        fields={
                            "rank": idx + 1,
                            "title": doc["title"],
                            "source": doc["source"],
                            "score": doc["similarity"],
                        },
                    )
                # ìƒìœ„ ê²°ê³¼ ìš”ì•½ ë””ë²„ê·¸ ì¶œë ¥ (ê°€ë…ì„±)
                try:
                    preview_count = min(5, len(documents))
                    for i in range(preview_count):
                        d = documents[i]
                        title_preview = (d["title"] or "")[:80]
                        logger.debug(
                            f"log.test ragtool.result#{i+1} score={d['similarity']:.3f} source='{d['source']}' title='{title_preview}'"
                        )
                except Exception:
                    pass

            kept_docs = []
            dropped = 0
            for d in documents:
                if d["similarity"] >= input_data.threshold:
                    kept_docs.append(d)
                else:
                    dropped += 1
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="filter.apply",
                fields={"threshold": input_data.threshold, "kept": len(kept_docs), "dropped": dropped},
            )
            documents = kept_docs

            # ì„œë¹„ìŠ¤ ê²°ê³¼ ì—†ê³  í´ë°± í—ˆìš© ì‹œ
            if not documents and self.rag_config.enable_fallback_search:
                fb_start = time.perf_counter()
                documents = self._fallback_search(input_data.query, input_data.k)
                fb_elapsed_ms = int((time.perf_counter() - fb_start) * 1000)
                _emit_log(
                    logger=logger,
                    level=logging.INFO,
                    event="fallback.done",
                    fields={"returned": len(documents), "elapsed_ms": fb_elapsed_ms},
                )
                used_fallback = True
                try:
                    logger.debug(f"log.test ragtool.fallback used=True elapsed_ms={fb_elapsed_ms}")
                except Exception:
                    pass
            
            if not documents:
                total_elapsed_ms = int((time.perf_counter() - total_start) * 1000)
                _emit_log(
                    logger=logger,
                    level=logging.INFO,
                    event="RAG return",
                    fields={
                        "documents": 0,
                        "used_fallback": used_fallback,
                        "total_elapsed_ms": total_elapsed_ms,
                    },
                )
                return RagOutput(
                    agent="RagTool", 
                    summary=f"ğŸ“­ '{input_data.query}'ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            # ìš”ì•½ ìƒì„±: "**ì œëª©** - ì¶œì²˜, ë‚ ì§œ" í˜•ì‹ë§Œ ë°˜í™˜
            sum_start = time.perf_counter()
            formatted_lines: List[str] = []

            def _clean_text(value: str) -> str:
                if not value:
                    return ""
                text = str(value)
                # ì‹¤ê°œí–‰/ì´ìŠ¤ì¼€ì´í”„ ê°œí–‰ ëª¨ë‘ ê³µë°±ìœ¼ë¡œ ì •ë¦¬
                text = text.replace("\n", " ").replace("\\n", " ").replace("\r", " ")
                # í”í•œ ë©”íƒ€ë°ì´í„° ê¼¬ë¦¬ ì œê±°
                text = re.sub(r"(\\?\"metadata\\?\":.*)$", "", text)
                text = re.sub(r"(Published on:.*)$", "", text, flags=re.IGNORECASE)
                # ê³µë°± ì¶•ì†Œ ë° ì–‘ë ë”°ì˜´í‘œ ì œê±°
                text = re.sub(r"\s+", " ", text).strip().strip("'\"")
                return text

            for doc in documents:
                md = doc.get("metadata", {})
                raw_title = md.get("title_en") or (doc.get("title") or "")
                title = _clean_text(raw_title)
                source = (doc.get("source") or md.get("source") or "").strip() or "Unknown"
                date = (md.get("date") or md.get("published_at") or "").strip()

                # í•­ëª©ì€ ì œëª©/ì¶œì²˜/(ì˜µì…˜)ë‚ ì§œë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ì„±í•˜ê³ , í•­ëª© ê°„ì—ëŠ” ë¹ˆ ì¤„ ì¶”ê°€
                formatted_lines.append(title)
                formatted_lines.append(f"ì¶œì²˜ : {source}")
                if date:
                    formatted_lines.append(f"ë‚ ì§œ : {date}")
                formatted_lines.append("")

            # ë§¨ ë ê³µë°± ì¤„ ì œê±°
            while formatted_lines and formatted_lines[-1] == "":
                formatted_lines.pop()

            # í—¤ë”ëŠ” êµµì€ ê¸€ì”¨ë¡œ, ë¬¸ì¥í˜• ë§ˆì¹¨í‘œ í¬í•¨
            header = f"**{input_data.query} ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ {len(documents)}ê±´ì…ë‹ˆë‹¤.**"
            summary = header + "\n\n" + "\n".join(formatted_lines)
            sum_elapsed_ms = int((time.perf_counter() - sum_start) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="summary.done",
                fields={"lines": len(formatted_lines), "elapsed_ms": sum_elapsed_ms},
            )
            try:
                logger.debug(f"log.test ragtool.summary lines={len(formatted_lines)} elapsed_ms={sum_elapsed_ms}")
            except Exception:
                pass
            
            # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì •ë¦¬ (ì‘ë‹µì— í¬í•¨í•  í•µì‹¬ ì •ë³´ë§Œ)
            max_length = self.rag_config.max_content_length
            document_list = [
                {
                    "title": doc["title"],
                    "content": doc["content"][:max_length] + "..." if len(doc["content"]) > max_length else doc["content"],
                    "similarity": doc["similarity"],
                    "source": doc["source"]
                }
                for doc in documents
            ]
            
            total_elapsed_ms = int((time.perf_counter() - total_start) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="RAG return",
                fields={
                    "documents": len(document_list),
                    "used_fallback": used_fallback,
                    "total_elapsed_ms": total_elapsed_ms,
                },
            )
            try:
                logger.debug(
                    f"log.test ragtool.return documents={len(document_list)} used_fallback={used_fallback} total_elapsed_ms={total_elapsed_ms}"
                )
            except Exception:
                pass
            return RagOutput(
                agent="RagTool",
                summary=summary,
                documents=document_list,
                data=documents  # ì „ì²´ ìƒì„¸ ë°ì´í„°
            )
            
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="rag.error",
                fields={"message": f"RAG ê²€ìƒ‰ ì˜¤ë¥˜: {e}"},
                exc_info=True,
            )
            return RagOutput(
                agent="error", 
                summary=f"ğŸ”§ RAG ê²€ìƒ‰ ì˜¤ë¥˜: {e}"
            )
        finally:
            _reset_request_id()

    def add_document(self, content: str, metadata: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì¶”ê°€ (í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” RAG ì„œë¹„ìŠ¤ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½ ê¶Œì¥)."""
        logger = logging.getLogger(__name__)
        try:
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="document.add.start",
                fields={
                    "title": metadata.get("title"),
                    "source": metadata.get("source"),
                    "date": metadata.get("date"),
                },
            )
            # ê°„ë‹¨íˆ ì„±ê³µ ì²˜ë¦¬ (Chroma ì œê±° í›„, ì‹¤ì œ ì €ì¥ì€ ìƒìœ„ íŒŒì´í”„ë¼ì¸ì—ì„œ ìˆ˜í–‰)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="document.add.done",
                fields={"note": "Use RagService.add_documents in service layer"},
            )
            return True
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="document.add.error",
                fields={"message": f"ë¬¸ì„œ ì¶”ê°€ ì˜¤ë¥˜: {e}"},
                exc_info=True,
            )
            return False
