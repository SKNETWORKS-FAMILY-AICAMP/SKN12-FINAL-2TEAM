import os
import json
import time
import uuid
import logging
import datetime
import contextvars
from typing import Optional, List, Dict, Any
from service.llm.AIChat.BaseFinanceTool import BaseFinanceTool
from service.rag.rag_config import RagConfig
from pydantic import BaseModel, Field

# =========================
# Logging helpers (RAG)
# =========================
_request_id_var: contextvars.ContextVar[str] = contextvars.ContextVar("request_id", default="")
_RAG_LOG_FORMAT = "plain"
_RAG_LOG_CONTENT_MAXLEN = 200

def init_rag_logging() -> None:
    """Initialize structured logging for RAG once (idempotent)."""
    global _RAG_LOG_FORMAT, _RAG_LOG_CONTENT_MAXLEN
    level_name = os.getenv("RAG_LOG_LEVEL", "INFO").upper()
    level = getattr(logging, level_name, logging.INFO)
    _RAG_LOG_FORMAT = os.getenv("RAG_LOG_FORMAT", "plain").lower()
    try:
        _RAG_LOG_CONTENT_MAXLEN = int(os.getenv("RAG_LOG_CONTENT_MAXLEN", "200"))
    except Exception:
        _RAG_LOG_CONTENT_MAXLEN = 200

    logger = logging.getLogger(__name__)
    if getattr(logger, "_rag_initialized", False):
        return
    logger.setLevel(level)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setLevel(level)
        handler.setFormatter(logging.Formatter("%(message)s"))
        logger.addHandler(handler)
    logger.propagate = False
    setattr(logger, "_rag_initialized", True)

def _now_ts() -> str:
    return datetime.datetime.utcnow().isoformat(timespec="seconds") + "Z"

def _set_request_id(request_id: str) -> None:
    _request_id_var.set(request_id)

def _reset_request_id() -> None:
    try:
        _request_id_var.set("")
    except Exception:
        pass

def _truncate_text(text: str) -> str:
    if text is None:
        return ""
    if len(text) <= _RAG_LOG_CONTENT_MAXLEN:
        return text
    return text[:_RAG_LOG_CONTENT_MAXLEN] + "..."

def _emit_log(logger: logging.Logger, level: int, event: str, fields: Optional[Dict[str, Any]] = None, exc_info: bool = False) -> None:
    payload: Dict[str, Any] = {
        "level": logging.getLevelName(level),
        "ts": _now_ts(),
        "request_id": _request_id_var.get() or None,
        "event": event,
    }
    if fields:
        payload.update(fields)

    if _RAG_LOG_FORMAT == "json":
        try:
            message = json.dumps({k: v for k, v in payload.items() if v is not None}, ensure_ascii=False)
        except Exception:
            message = json.dumps({"level": payload.get("level"), "ts": payload.get("ts"), "event": event}, ensure_ascii=False)
    else:
        # plain key=value with quoted strings
        def _fmt_val(v: Any) -> str:
            if isinstance(v, (int, float)):
                return str(v)
            if isinstance(v, bool):
                return "true" if v else "false"
            if v is None:
                return "null"
            s = str(v)
            s = s.replace('"', '\\"')
            return f'"{s}"'
        parts = [
            f"level={payload['level']}",
            f"ts={payload['ts']}",
        ]
        if payload.get("request_id"):
            parts.append(f"request_id={payload['request_id']}")
        parts.append(f"event={_fmt_val(event)}")
        for k, v in payload.items():
            if k in ("level", "ts", "request_id", "event"):
                continue
            parts.append(f"{k}={_fmt_val(v)}")
        message = " ".join(parts)
    logger.log(level, message, exc_info=exc_info)

class RagInput(BaseModel):
    query: str = Field(..., description="Í≤ÄÏÉâÌï† ÏßàÎ¨∏ ÎòêÎäî ÌÇ§ÏõåÎìú (Ïòà: 'Í∏àÎ¶¨ Ï†ïÏ±Ö', 'ESG Ìà¨Ïûê')")
    k: int = Field(5, description="Í≤ÄÏÉâÌï† Î¨∏ÏÑú Í∞úÏàò (Í∏∞Î≥∏Í∞í: 5)")
    threshold: float = Field(0.7, description="Ïú†ÏÇ¨ÎèÑ ÏûÑÍ≥ÑÍ∞í (Í∏∞Î≥∏Í∞í: 0.7)")

class RagOutput(BaseModel):
    agent: str
    summary: str
    documents: Optional[List[Dict[str, Any]]] = None
    data: Optional[Any] = None

    def __init__(
        self,
        agent: str,
        summary: str,
        documents: Optional[List[Dict[str, Any]]] = None,
        data: Optional[Any] = None,
    ):
        super().__init__(
            agent=agent,
            summary=summary,
            documents=documents,
            data=data,
        )

class RagTool(BaseFinanceTool):
    
    def __init__(self, ai_chat_service, rag_config: Optional[RagConfig] = None):
        init_rag_logging()
        logger = logging.getLogger(__name__)
        from service.llm.AIChat_service import AIChatService
        if not isinstance(ai_chat_service, AIChatService):
            raise TypeError("Expected AIChatService instance")
        self.ai_chat_service = ai_chat_service
        
        # RAG ÏÑ§Ï†ï (Îß§Í∞úÎ≥ÄÏàò ‚Üí ÏÑúÎπÑÏä§ ‚Üí Í∏∞Î≥∏Í∞í ÏàúÏÑúÎ°ú Ïö∞ÏÑ†ÏàúÏúÑ)
        self.rag_config = rag_config or getattr(ai_chat_service, 'rag_config', None) or RagConfig()
        
        # Î≤°ÌÑ∞ DB Ï¥àÍ∏∞Ìôî
        self._initialize_vector_db()
    
    def _initialize_vector_db(self):
        """Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî"""
        logger = logging.getLogger(__name__)
        try:
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="vector.db.init.start",
                fields={
                    "enable_vector_db": getattr(self.rag_config, 'enable_vector_db', None),
                    "embedding_model": getattr(self.rag_config, 'embedding_model', None),
                    "vector_db_path": getattr(self.rag_config, 'vector_db_path', None),
                    "collection_name": getattr(self.rag_config, 'collection_name', None),
                },
            )
            if not self.rag_config.enable_vector_db:
                self.embedding_model = None
                self.collection = None
                _emit_log(
                    logger=logger,
                    level=logging.INFO,
                    event="vector.db.init.skipped",
                    fields={"reason": "disabled by config"},
                )
                return
                
            # ÏûÑÎ≤†Îî© Î™®Îç∏ ÏÑ§Ï†ï
            from sentence_transformers import SentenceTransformer
            self.embedding_model = SentenceTransformer(self.rag_config.embedding_model)
            
            # Î≤°ÌÑ∞ DB Ïó∞Í≤∞ ÏÑ§Ï†ï
            import chromadb
            self.chroma_client = chromadb.PersistentClient(path=self.rag_config.vector_db_path)
            self.collection = self.chroma_client.get_or_create_collection(
                name=self.rag_config.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="vector.db.init.done",
                fields={
                    "enable_vector_db": True,
                    "embedding_model": self.rag_config.embedding_model,
                    "vector_db_path": self.rag_config.vector_db_path,
                    "collection_name": self.rag_config.collection_name,
                },
            )
            
        except ImportError as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="vector.db.init.error",
                fields={"message": f"Î≤°ÌÑ∞ DB ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§: {e}"},
                exc_info=True,
            )
            self.embedding_model = None
            self.collection = None
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="vector.db.init.error",
                fields={"message": f"Î≤°ÌÑ∞ DB Ï¥àÍ∏∞Ìôî Ïò§Î•ò: {e}"},
                exc_info=True,
            )
            self.embedding_model = None
            self.collection = None

    def _get_embedding(self, text: str) -> List[float]:
        """ÌÖçÏä§Ìä∏Î•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôò"""
        logger = logging.getLogger(__name__)
        if self.embedding_model is None:
            _emit_log(
                logger=logger,
                level=logging.WARNING,
                event="embedding.skipped",
                fields={"reason": "embedding model not initialized"},
            )
            return []
        
        try:
            start_t = time.perf_counter()
            preview = _truncate_text(text)
            _emit_log(
                logger=logger,
                level=logging.DEBUG,
                event="embedding.start",
                fields={
                    "input_length": len(text),
                    "preview": preview,
                },
            )
            embedding = self.embedding_model.encode(text).tolist()
            elapsed_ms = int((time.perf_counter() - start_t) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="embedding.done",
                fields={
                    "dim": len(embedding) if isinstance(embedding, list) else 0,
                    "elapsed_ms": elapsed_ms,
                },
            )
            return embedding
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="embedding.error",
                fields={"message": f"ÏûÑÎ≤†Îî© ÏÉùÏÑ± Ïò§Î•ò: {e}"},
                exc_info=True,
            )
            return []

    def _search_vector_db(self, query: str, k: int, threshold: float) -> List[Dict[str, Any]]:
        """Î≤°ÌÑ∞ DBÏóêÏÑú Ïú†ÏÇ¨ Î¨∏ÏÑú Í≤ÄÏÉâ"""
        logger = logging.getLogger(__name__)
        if self.collection is None:
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="vector.search.skipped",
                fields={"reason": "vector DB collection not initialized"},
            )
            return []
        
        try:
            # ÏøºÎ¶¨Î•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôò
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="vector.search.start",
                fields={
                    "query": _truncate_text(query),
                    "k": k,
                    "threshold": threshold,
                },
            )
            query_embedding = self._get_embedding(query)
            if not query_embedding:
                return []
            
            # Î≤°ÌÑ∞ DBÏóêÏÑú Í≤ÄÏÉâ
            q_start_t = time.perf_counter()
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            q_elapsed_ms = int((time.perf_counter() - q_start_t) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="vector.search",
                fields={
                    "k": k,
                    "returned": len(results['documents'][0]) if results.get('documents') and results['documents'] and results['documents'][0] else 0,
                    "elapsed_ms": q_elapsed_ms,
                },
            )
            
            documents = []
            below_threshold = 0
            filter_start = time.perf_counter()
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    distance = results['distances'][0][i] if results['distances'] else 1.0
                    similarity = 1 - distance  # cosine distanceÎ•º similarityÎ°ú Î≥ÄÌôò
                    
                    # ÏûÑÍ≥ÑÍ∞í Ïù¥ÏÉÅÏù∏ Î¨∏ÏÑúÎßå Ìè¨Ìï®
                    if similarity >= threshold:
                        metadata = results['metadatas'][0][i] if results['metadatas'] else {}
                        documents.append({
                            "content": doc,
                            "similarity": round(similarity, 3),
                            "metadata": metadata,
                            "source": metadata.get("source", "unknown"),
                            "title": metadata.get("title", "Ï†úÎ™© ÏóÜÏùå"),
                            "date": metadata.get("date", "ÎÇ†Ïßú ÏóÜÏùå")
                        })
                        _emit_log(
                            logger=logger,
                            level=logging.DEBUG,
                            event="vector.search.doc",
                            fields={
                                "rank": i + 1,
                                "title": documents[-1]["title"],
                                "source": documents[-1]["source"],
                                "similarity": round(similarity, 3),
                                "distance": round(distance, 3),
                            },
                        )
                    else:
                        below_threshold += 1
            filter_elapsed_ms = int((time.perf_counter() - filter_start) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="filter.apply",
                fields={
                    "threshold": threshold,
                    "kept": len(documents),
                    "dropped": below_threshold,
                    "elapsed_ms": filter_elapsed_ms,
                },
            )
            
            return documents
            
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="vector.search.error",
                fields={"message": f"Î≤°ÌÑ∞ DB Í≤ÄÏÉâ Ïò§Î•ò: {e}"},
                exc_info=True,
            )
            return []

    def _fallback_search(self, query: str, k: int) -> List[Dict[str, Any]]:
        """Î≤°ÌÑ∞ DBÍ∞Ä ÏóÜÏùÑ Îïå ÎåÄÏ≤¥ Í≤ÄÏÉâ (Ïòà: Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÌÇ§ÏõåÎìú Í≤ÄÏÉâ)"""
        logger = logging.getLogger(__name__)
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞Î•º ÌÇ§ÏõåÎìú Í∏∞Î∞òÏúºÎ°ú Í≤ÄÏÉâ
        # Ïó¨Í∏∞ÏÑúÎäî ÏòàÏãú Îç∞Ïù¥ÌÑ∞Î•º Î∞òÌôò
        _emit_log(
            logger=logger,
            level=logging.WARNING,
            event="fallback.used",
            fields={"reason": "vector search empty ‚Üí fallback enabled"},
        )
        fallback_docs = [
            {
                "content": f"'{query}'ÏôÄ Í¥ÄÎ†®Îêú Í∏àÏúµ Îâ¥Ïä§ ÎÇ¥Ïö©ÏûÖÎãàÎã§. Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú Í≤ÄÏÉâÎêú Í≤∞Í≥ºÍ∞Ä ÌëúÏãúÎê©ÎãàÎã§.",
                "similarity": 0.8,
                "metadata": {
                    "source": "fallback_search",
                    "title": f"{query} Í¥ÄÎ†® Îâ¥Ïä§",
                    "date": "2024-01-01"
                },
                "source": "fallback_search",
                "title": f"{query} Í¥ÄÎ†® Îâ¥Ïä§", 
                "date": "2024-01-01"
            }
        ]
        docs = fallback_docs[:k]
        _emit_log(
            logger=logger,
            level=logging.INFO,
            event="fallback.return",
            fields={"returned": len(docs)},
        )
        return docs

    def get_data(self, **kwargs) -> RagOutput:
        logger = logging.getLogger(__name__)
        request_id = str(uuid.uuid4())
        _set_request_id(request_id)
        total_start = time.perf_counter()
        try:
            input_data = RagInput(**kwargs)
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="rag.param.error",
                fields={"message": f"Îß§Í∞úÎ≥ÄÏàò Ïò§Î•ò: {e}"},
                exc_info=True,
            )
            return RagOutput(agent="error", summary=f"‚ùå Îß§Í∞úÎ≥ÄÏàò Ïò§Î•ò: {e}")

        try:
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="RAG start",
                fields={
                    "query": _truncate_text(input_data.query),
                    "k": input_data.k,
                    "threshold": input_data.threshold,
                    "request_id": request_id,
                },
            )
            # Î≤°ÌÑ∞ DBÏóêÏÑú Í≤ÄÏÉâ
            used_fallback = False
            documents = self._search_vector_db(
                input_data.query, 
                input_data.k, 
                input_data.threshold
            )
            
            # Î≤°ÌÑ∞ DB Í≤ÄÏÉâ Í≤∞Í≥ºÍ∞Ä ÏóÜÏúºÎ©¥ ÎåÄÏ≤¥ Í≤ÄÏÉâ ÏàòÌñâ
            if not documents and self.rag_config.enable_fallback_search:
                fb_start = time.perf_counter()
                documents = self._fallback_search(input_data.query, input_data.k)
                fb_elapsed_ms = int((time.perf_counter() - fb_start) * 1000)
                _emit_log(
                    logger=logger,
                    level=logging.INFO,
                    event="fallback.done",
                    fields={"returned": len(documents), "elapsed_ms": fb_elapsed_ms},
                )
                used_fallback = True
            
            if not documents:
                total_elapsed_ms = int((time.perf_counter() - total_start) * 1000)
                _emit_log(
                    logger=logger,
                    level=logging.INFO,
                    event="RAG return",
                    fields={
                        "documents": 0,
                        "used_fallback": used_fallback,
                        "total_elapsed_ms": total_elapsed_ms,
                    },
                )
                return RagOutput(
                    agent="RagTool", 
                    summary=f"üì≠ '{input_data.query}'Ïóê ÎåÄÌïú Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
                )
            
            # ÏöîÏïΩ ÏÉùÏÑ±
            sum_start = time.perf_counter()
            summary_lines = [f"üîç '{input_data.query}' Í≤ÄÏÉâ Í≤∞Í≥º {len(documents)}Í±¥:"]
            for i, doc in enumerate(documents, 1):
                similarity_percent = int(doc['similarity'] * 100)
                summary_lines.append(
                    f"{i}. {doc['title']} (Ïú†ÏÇ¨ÎèÑ: {similarity_percent}%) - {doc['source']}"
                )
            
            summary = "\n".join(summary_lines)
            sum_elapsed_ms = int((time.perf_counter() - sum_start) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="summary.done",
                fields={"lines": len(summary_lines), "elapsed_ms": sum_elapsed_ms},
            )
            
            # Î¨∏ÏÑú Î¶¨Ïä§Ìä∏ Ï†ïÎ¶¨ (ÏùëÎãµÏóê Ìè¨Ìï®Ìï† ÌïµÏã¨ Ï†ïÎ≥¥Îßå)
            max_length = self.rag_config.max_content_length
            document_list = [
                {
                    "title": doc["title"],
                    "content": doc["content"][:max_length] + "..." if len(doc["content"]) > max_length else doc["content"],
                    "similarity": doc["similarity"],
                    "source": doc["source"]
                }
                for doc in documents
            ]
            
            total_elapsed_ms = int((time.perf_counter() - total_start) * 1000)
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="RAG return",
                fields={
                    "documents": len(document_list),
                    "used_fallback": used_fallback,
                    "total_elapsed_ms": total_elapsed_ms,
                },
            )
            return RagOutput(
                agent="RagTool",
                summary=summary,
                documents=document_list,
                data=documents  # Ï†ÑÏ≤¥ ÏÉÅÏÑ∏ Îç∞Ïù¥ÌÑ∞
            )
            
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="rag.error",
                fields={"message": f"RAG Í≤ÄÏÉâ Ïò§Î•ò: {e}"},
                exc_info=True,
            )
            return RagOutput(
                agent="error", 
                summary=f"üîß RAG Í≤ÄÏÉâ Ïò§Î•ò: {e}"
            )
        finally:
            _reset_request_id()

    def add_document(self, content: str, metadata: Dict[str, Any]) -> bool:
        """Î≤°ÌÑ∞ DBÏóê ÏÉà Î¨∏ÏÑú Ï∂îÍ∞Ä"""
        logger = logging.getLogger(__name__)
        if self.collection is None:
            _emit_log(
                logger=logger,
                level=logging.WARNING,
                event="document.add.skipped",
                fields={"reason": "vector DB not initialized"},
            )
            return False
        
        try:
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="document.add.start",
                fields={
                    "title": metadata.get("title"),
                    "source": metadata.get("source"),
                    "date": metadata.get("date"),
                },
            )
            # Î¨∏ÏÑúÎ•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôò
            embedding = self._get_embedding(content)
            if not embedding:
                return False
            
            # Í≥†Ïú† ID ÏÉùÏÑ±
            doc_id = f"doc_{hash(content)}"
            
            # Î≤°ÌÑ∞ DBÏóê Ï∂îÍ∞Ä
            self.collection.add(
                embeddings=[embedding],
                documents=[content],
                metadatas=[metadata],
                ids=[doc_id]
            )
            
            _emit_log(
                logger=logger,
                level=logging.DEBUG,
                event="document.add.embedded",
                fields={"dim": len(embedding), "doc_id": doc_id},
            )
            _emit_log(
                logger=logger,
                level=logging.INFO,
                event="document.add.done",
                fields={"doc_id": doc_id},
            )
            return True
            
        except Exception as e:
            _emit_log(
                logger=logger,
                level=logging.ERROR,
                event="document.add.error",
                fields={"message": f"Î¨∏ÏÑú Ï∂îÍ∞Ä Ïò§Î•ò: {e}"},
                exc_info=True,
            )
            return False
