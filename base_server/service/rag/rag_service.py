import asyncio
import hashlib
import time
from typing import Optional, Dict, Any, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from service.rag.rag_config import RagConfig
from service.rag.rag_vectordb_client import RagVectorDbClient, RagVectorDbConfig
from service.core.logger import Logger

class RagService:
    """
    RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤
    
    SearchService(í‚¤ì›Œë“œ ê²€ìƒ‰)ì™€ VectorDbService(ë²¡í„° ê²€ìƒ‰)ë¥¼ ì¡°í•©í•œ 
    í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ì •ì  í´ë˜ìŠ¤ (111 íŒ¨í„´)
    """
    
    # 111 íŒ¨í„´ ìƒíƒœ ê´€ë¦¬
    _initialized: bool = False
    _config: Optional[RagConfig] = None
    _search_available: bool = False
    _vector_available: bool = False
    
    # RAG ì „ìš© ë²¡í„° í´ë¼ì´ì–¸íŠ¸ (coroutine ì¬ì‚¬ìš© ë¬¸ì œ í•´ê²°)
    _rag_vector_client: Optional[RagVectorDbClient] = None
    
    # ì„±ëŠ¥ í†µê³„
    _stats = {
        "documents_indexed": 0,
        "search_requests": 0,
        "hybrid_searches": 0,
        "avg_search_time": 0.0
    }

    @classmethod
    def init(cls, rag_config: RagConfig) -> bool:
        """
        RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (111 íŒ¨í„´)
        
        Args:
            rag_config: RAG ì„œë¹„ìŠ¤ ì„¤ì •
            
        Returns:
            bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            Logger.debug("log.test rag.init.enter")
            Logger.info("ğŸš€ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")
            
            if cls._initialized:
                Logger.warn("âš ï¸ RAG ì„œë¹„ìŠ¤ê°€ ì´ë¯¸ ì´ˆê¸°í™”ë¨")
                return True
            
            # ì„¤ì • ê²€ì¦
            if not cls._validate_config(rag_config):
                Logger.error("âŒ RAG ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")
                return False
            
            cls._config = rag_config
            
            # ì˜ì¡´ ì„œë¹„ìŠ¤ ìƒíƒœ ê²€ì¦
            if not cls._validate_dependencies():
                Logger.error("âŒ ì˜ì¡´ ì„œë¹„ìŠ¤ ê²€ì¦ ì‹¤íŒ¨")
                return False
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ê²€ì¦
            if not cls._validate_hybrid_setup():
                Logger.error("âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")
                return False
            
            # í†µê³„ ì´ˆê¸°í™”
            cls._reset_stats()
            
            cls._initialized = True
            Logger.info(f"âœ… RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            Logger.info(f"   - ë²¡í„° ê²€ìƒ‰: {'í™œì„±í™”' if cls._vector_available else 'ë¹„í™œì„±í™”'}")
            Logger.info(f"   - í‚¤ì›Œë“œ ê²€ìƒ‰: {'í™œì„±í™”' if cls._search_available else 'ë¹„í™œì„±í™”'}")
            Logger.info(f"   - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: {'í™œì„±í™”' if (cls._vector_available and cls._search_available) else 'ë¹„í™œì„±í™”'}")
            Logger.debug("log.test rag.init.ok")
            
            return True
            
        except Exception as e:
            Logger.error(f"âŒ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.init.fail err={str(e)}")
            cls._initialized = False
            cls._config = None
            return False

    @classmethod
    def _validate_config(cls, config: RagConfig) -> bool:
        """RAG ì„¤ì • ê²€ì¦"""
        try:
            Logger.debug("log.test rag.validate_config.enter")
            if not config.collection_name:
                Logger.error("collection_nameì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                return False
            
            if not config.embedding_model:
                Logger.error("embedding_modelì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                return False
            
            if config.default_k <= 0:
                Logger.error(f"default_këŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•¨: {config.default_k}")
                return False
            
            if not (0.0 <= config.default_threshold <= 1.0):
                Logger.error(f"default_thresholdëŠ” 0.0~1.0 ì‚¬ì´ì—¬ì•¼ í•¨: {config.default_threshold}")
                return False
            
            Logger.debug("RAG ì„¤ì • ê²€ì¦ í†µê³¼")
            Logger.debug("log.test rag.validate_config.ok")
            return True
            
        except Exception as e:
            Logger.error(f"RAG ì„¤ì • ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            Logger.debug(f"log.test rag.validate_config.fail err={str(e)}")
            return False

    @classmethod
    def _validate_dependencies(cls) -> bool:
        """ì˜ì¡´ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ìƒíƒœ ê²€ì¦"""
        try:
            Logger.debug("log.test rag.validate_deps.enter")
            from service.search.search_service import SearchService
            from service.vectordb.vectordb_service import VectorDbService
            
            # SearchService ìƒíƒœ í™•ì¸
            cls._search_available = SearchService.is_initialized()
            Logger.info(f"SearchService ìƒíƒœ: {'ì‚¬ìš© ê°€ëŠ¥' if cls._search_available else 'ì‚¬ìš© ë¶ˆê°€'}")
            
            # VectorDbService ìƒíƒœ í™•ì¸
            cls._vector_available = VectorDbService.is_initialized()
            Logger.info(f"VectorDbService ìƒíƒœ: {'ì‚¬ìš© ê°€ëŠ¥' if cls._vector_available else 'ì‚¬ìš© ë¶ˆê°€'}")
            
            # RAG ì „ìš© ë²¡í„° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (coroutine ì¬ì‚¬ìš© ë¬¸ì œ í•´ê²°)
            if cls._vector_available and cls._config.enable_vector_db:
                try:
                    Logger.debug("log.test rag.validate_deps.vector_client.init")
                    # RAG ì„¤ì •ì—ì„œ ë²¡í„° DB ì„¤ì • ì¶”ì¶œ
                    vector_config = RagVectorDbConfig(
                        aws_access_key_id=cls._config.aws_access_key_id,
                        aws_secret_access_key=cls._config.aws_secret_access_key,
                        region_name=cls._config.region_name,
                        knowledge_base_id=cls._config.knowledge_base_id,
                        aws_session_token=getattr(cls._config, 'aws_session_token', None),
                        max_retries=3,
                        retry_delay_base=1.0,
                        timeout=30.0
                    )
                    
                    cls._rag_vector_client = RagVectorDbClient(vector_config)
                    Logger.info("âœ… RAG ì „ìš© ë²¡í„° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                    Logger.debug("log.test rag.validate_deps.vector_client.ok")
                    
                except Exception as e:
                    Logger.error(f"âŒ RAG ì „ìš© ë²¡í„° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    Logger.debug(f"log.test rag.validate_deps.vector_client.fail err={str(e)}")
                    cls._vector_available = False
            
            # ìµœì†Œ í•˜ë‚˜ì˜ ì„œë¹„ìŠ¤ëŠ” ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•¨
            if not (cls._search_available or cls._vector_available):
                Logger.error("Searchì™€ Vector ì„œë¹„ìŠ¤ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ")
                Logger.debug("log.test rag.validate_deps.none_available")
                return False
            
            Logger.debug("log.test rag.validate_deps.ok")
            return True
            
        except Exception as e:
            Logger.error(f"ì˜ì¡´ ì„œë¹„ìŠ¤ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            Logger.debug(f"log.test rag.validate_deps.fail err={str(e)}")
            return False

    @classmethod
    def _validate_hybrid_setup(cls) -> bool:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ê²€ì¦"""
        try:
            Logger.debug("log.test rag.validate_hybrid.enter")
            # ë²¡í„° DB ì„¤ì • í™•ì¸
            if cls._config.enable_vector_db and not cls._vector_available:
                Logger.warn("ë²¡í„° DB í™œì„±í™” ì„¤ì •ì´ì§€ë§Œ VectorDbServiceê°€ ì‚¬ìš© ë¶ˆê°€")
                
            # Fallback ê²€ìƒ‰ ì„¤ì • í™•ì¸
            if cls._config.enable_fallback_search and not cls._search_available:
                Logger.warn("Fallback ê²€ìƒ‰ í™œì„±í™” ì„¤ì •ì´ì§€ë§Œ SearchServiceê°€ ì‚¬ìš© ë¶ˆê°€")
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ëŠ¥ì„± í™•ì¸
            hybrid_possible = cls._search_available and cls._vector_available
            if hybrid_possible:
                Logger.info("ğŸ” ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ëŠ¥")
            else:
                Logger.warn("âš ï¸ ì œí•œëœ ê²€ìƒ‰ ëª¨ë“œë¡œ ë™ì‘")
            
            Logger.debug(f"log.test rag.validate_hybrid.ok hybrid={hybrid_possible}")
            return True
            
        except Exception as e:
            Logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            Logger.debug(f"log.test rag.validate_hybrid.fail err={str(e)}")
            return False

    @classmethod
    def _reset_stats(cls):
        """í†µê³„ ì´ˆê¸°í™”"""
        cls._stats = {
            "documents_indexed": 0,
            "search_requests": 0,
            "hybrid_searches": 0,
            "avg_search_time": 0.0
        }

    @classmethod
    async def add_documents(cls, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ì¸ì§€ì…˜ íŒŒì´í”„ë¼ì¸ - ë‘ ì„œë¹„ìŠ¤ì— ë³‘ë ¬ ì €ì¥
        
        Args:
            documents: ì €ì¥í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
                [{"id": str, "content": str, "metadata": dict}, ...]
                
        Returns:
            Dict: ì €ì¥ ê²°ê³¼ í†µê³„
        """
        if not cls._initialized:
            raise RuntimeError("RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        Logger.debug(f"log.test rag.add_docs.enter count={len(documents)}")
        Logger.info(f"ğŸ“„ ë¬¸ì„œ ì¸ì§€ì…˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
        
        try:
            # ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ê²€ì¦
            processed_docs = cls._preprocess_documents(documents)
            if not processed_docs:
                Logger.warn("ì²˜ë¦¬í•  ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŒ")
                return {"success": False, "message": "ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŒ"}
            
            # ë¬¸ì„œ ì²­í‚¹ (ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• )
            chunked_docs = cls._chunk_documents(processed_docs)
            Logger.info(f"ğŸ“ ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {len(chunked_docs)}ê°œ ì²­í¬")
            Logger.debug(f"log.test rag.add_docs.chunked total_chunks={len(chunked_docs)}")
            
            # ë³‘ë ¬ ì €ì¥ ì‹¤í–‰
            storage_results = await cls._parallel_storage(chunked_docs)
            Logger.debug(f"log.test rag.add_docs.storage done success={storage_results.get('success_count',0)} error={storage_results.get('error_count',0)}")
            
            # ê²°ê³¼ ì§‘ê³„
            total_time = time.time() - start_time
            success_count = storage_results.get("success_count", 0)
            error_count = storage_results.get("error_count", 0)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            cls._stats["documents_indexed"] += success_count
            
            Logger.info(f"âœ… ë¬¸ì„œ ì¸ì§€ì…˜ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨ ({total_time:.2f}ì´ˆ)")
            Logger.debug("log.test rag.add_docs.ok")
            
            return {
                "success": error_count == 0,
                "total_documents": len(documents),
                "total_chunks": len(chunked_docs),
                "success_count": success_count,
                "error_count": error_count,
                "processing_time": total_time,
                "storage_details": storage_results
            }
            
        except Exception as e:
            Logger.error(f"âŒ ë¬¸ì„œ ì¸ì§€ì…˜ ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.add_docs.fail err={str(e)}")
            return {
                "success": False,
                "error": str(e),
                "processing_time": time.time() - start_time
            }

    @classmethod
    def _preprocess_documents(cls, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì „ì²˜ë¦¬ ë° ê²€ì¦"""
        processed = []
        Logger.debug(f"log.test rag.preprocess.enter count={len(documents)}")
        
        for i, doc in enumerate(documents):
            try:
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if not isinstance(doc, dict):
                    Logger.warn(f"ë¬¸ì„œ {i}: dict íƒ€ì…ì´ ì•„ë‹˜")
                    continue
                    
                if "content" not in doc or not doc["content"]:
                    Logger.warn(f"ë¬¸ì„œ {i}: content í•„ë“œ ì—†ìŒ")
                    continue
                
                # ID ìƒì„± (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
                if "id" not in doc or not doc["id"]:
                    doc["id"] = cls._generate_document_id(doc["content"])
                
                # ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’ ì„¤ì •
                if "metadata" not in doc:
                    doc["metadata"] = {}
                
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                doc["metadata"]["indexed_at"] = time.time()
                
                processed.append(doc)
                
            except Exception as e:
                Logger.warn(f"ë¬¸ì„œ {i} ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        Logger.debug(f"log.test rag.preprocess.ok kept={len(processed)}")
        return processed

    @classmethod
    def _generate_document_id(cls, content: str) -> str:
        """ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ID ìƒì„±"""
        return f"doc_{hashlib.md5(content.encode('utf-8')).hexdigest()[:16]}"

    @classmethod
    def _chunk_documents(cls, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ì²­í‚¹ - ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• """
        chunks = []
        Logger.debug(f"log.test rag.chunk.enter docs={len(documents)}")
        max_chunk_size = cls._config.max_content_length
        
        for doc in documents:
            content = doc["content"]
            
            # ë¬¸ì„œê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
            if len(content) <= max_chunk_size:
                chunks.append(doc)
                continue
            
            # ê¸´ ë¬¸ì„œëŠ” ì²­í‚¹
            chunk_texts = cls._split_text(content, max_chunk_size)
            
            for i, chunk_text in enumerate(chunk_texts):
                chunk_doc = {
                    "id": f"{doc['id']}_chunk_{i}",
                    "content": chunk_text,
                    "metadata": {
                        **doc["metadata"],
                        "parent_id": doc["id"],
                        "chunk_index": i,
                        "total_chunks": len(chunk_texts)
                    }
                }
                chunks.append(chunk_doc)
        
        Logger.debug(f"log.test rag.chunk.ok chunks={len(chunks)}")
        return chunks

    @classmethod
    def _split_text(cls, text: str, max_size: int) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í¬ê¸°ë¡œ ë¶„í• """
        if len(text) <= max_size:
            Logger.debug("log.test rag.split.noop")
            return [text]
        
        chunks = []
        sentences = text.split('. ')
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) <= max_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        Logger.debug(f"log.test rag.split.ok parts={len(chunks)}")
        return chunks

    @classmethod
    async def _parallel_storage(cls, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ë‘ ì„œë¹„ìŠ¤ì— ë³‘ë ¬ë¡œ ë¬¸ì„œ ì €ì¥"""
        Logger.debug(f"log.test rag.parallel_storage.enter docs={len(documents)}")
        search_results = []
        vector_results = []
        
        # ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        
        if cls._search_available:
            tasks.append(cls._store_to_search_service(documents))
        
        if cls._vector_available:
            tasks.append(cls._store_to_vector_service(documents))
        
        # ë³‘ë ¬ ì‹¤í–‰
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ë¶„ì„
            success_count = 0
            error_count = 0
            
            for result in results:
                if isinstance(result, Exception):
                    Logger.error(f"ì €ì¥ íƒœìŠ¤í¬ ì‹¤íŒ¨: {result}")
                    error_count += len(documents)
                else:
                    success_count += result.get("success_count", 0)
                    error_count += result.get("error_count", 0)
            Logger.debug(f"log.test rag.parallel_storage.done success={success_count} error={error_count}")
        
        return {
            "success_count": success_count,
            "error_count": error_count,
            "search_results": search_results,
            "vector_results": vector_results
        }

    @classmethod
    async def _store_to_search_service(cls, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """OpenSearchì— ë¬¸ì„œ ì €ì¥"""
        try:
            from service.search.search_service import SearchService
            
            Logger.debug(f"OpenSearchì— {len(documents)}ê°œ ë¬¸ì„œ ì €ì¥ ì‹œì‘")
            Logger.debug(f"log.test rag.store_search.enter count={len(documents)}")
            
            success_count = 0
            error_count = 0
            
            # OpenSearch í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì‚¬ìš©
            client = SearchService.get_client()
            if not client:
                raise Exception("OpenSearch í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ")
            
            index_name = cls._config.collection_name
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
            for doc in documents:
                try:
                    # ë¬¸ì„œ ì¸ë±ì‹±
                    response = await SearchService.index_document(
                        index_name, 
                        {
                            "content": doc["content"],
                            "metadata": doc["metadata"]
                        },
                        doc["id"]
                    )
                    
                    if response:
                        success_count += 1
                    else:
                        error_count += 1
                        
                except Exception as e:
                    Logger.warn(f"ë¬¸ì„œ {doc['id']} OpenSearch ì €ì¥ ì‹¤íŒ¨: {e}")
                    error_count += 1
            
            Logger.info(f"OpenSearch ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
            Logger.debug("log.test rag.store_search.ok")
            
            return {
                "service": "search",
                "success_count": success_count,
                "error_count": error_count
            }
            
        except Exception as e:
            Logger.error(f"OpenSearch ì €ì¥ ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.store_search.fail err={str(e)}")
            return {
                "service": "search",
                "success_count": 0,
                "error_count": len(documents),
                "error": str(e)
            }

    @classmethod
    async def _store_to_vector_service(cls, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """VectorDBì— ë¬¸ì„œ ì €ì¥"""
        try:
            from service.vectordb.vectordb_service import VectorDbService
            
            Logger.debug(f"VectorDBì— {len(documents)}ê°œ ë¬¸ì„œ ì €ì¥ ì‹œì‘")
            Logger.debug(f"log.test rag.store_vector.enter count={len(documents)}")
            
            success_count = 0
            error_count = 0
            
            for doc in documents:
                try:
                    # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
                    embed_result = await VectorDbService.embed_text(doc["content"])
                    
                    if not embed_result.get("success"):
                        Logger.warn(f"ë¬¸ì„œ {doc['id']} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                        error_count += 1
                        continue
                    
                    # ë²¡í„° ì €ì¥ (ì‹¤ì œ êµ¬í˜„ì€ VectorDbServiceì˜ ë©”ì„œë“œ í™•ì¸ í•„ìš”)
                    # ì—¬ê¸°ì„œëŠ” ì„ë² ë”©ì´ ì„±ê³µí–ˆë‹¤ê³  ê°€ì •
                    success_count += 1
                    
                except Exception as e:
                    Logger.warn(f"ë¬¸ì„œ {doc['id']} VectorDB ì €ì¥ ì‹¤íŒ¨: {e}")
                    error_count += 1
            
            Logger.info(f"VectorDB ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
            Logger.debug("log.test rag.store_vector.ok")
            
            return {
                "service": "vector",
                "success_count": success_count,
                "error_count": error_count
            }
            
        except Exception as e:
            Logger.error(f"VectorDB ì €ì¥ ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.store_vector.fail err={str(e)}")
            return {
                "service": "vector",
                "success_count": 0,
                "error_count": len(documents),
                "error": str(e)
            }

    @classmethod
    def _preprocess_query(cls, query: str) -> str:
        """ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ - ì£¼ì‹ ì‹¬ë³¼ í™•ì¥, ë™ì˜ì–´ ì²˜ë¦¬"""
        Logger.debug(f"log.test rag.preprocess_query.enter q_len={len(query) if query else 0}")
        if not query:
            return query
        
        # ì£¼ì‹ ì‹¬ë³¼ ë° íšŒì‚¬ëª… ë§¤í•‘ (í¬ë¡¤ëŸ¬ 150ê°œ ì‹¬ë³¼ ê¸°ë°˜)
        stock_mappings = {
            # ëŒ€í˜• í…Œí¬ì£¼ (FAANG+)
            'ì• í”Œ': 'AAPL', 'apple': 'AAPL', 'Apple': 'AAPL',
            'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸': 'MSFT', 'microsoft': 'MSFT', 'Microsoft': 'MSFT',
            'êµ¬ê¸€': 'GOOGL', 'google': 'GOOGL', 'Google': 'GOOGL',
            'êµ¬ê¸€ì•ŒíŒŒë²³': 'GOOG', 'alphabet': 'GOOG', 'Alphabet': 'GOOG',
            'ì•„ë§ˆì¡´': 'AMZN', 'amazon': 'AMZN', 'Amazon': 'AMZN',
            'ë©”íƒ€': 'META', 'meta': 'META', 'Meta': 'META',
            'í˜ì´ìŠ¤ë¶': 'META', 'facebook': 'META', 'Facebook': 'META',
            'í…ŒìŠ¬ë¼': 'TSLA', 'tesla': 'TSLA', 'Tesla': 'TSLA',
            'ì—”ë¹„ë””ì•„': 'NVDA', 'nvidia': 'NVDA', 'Nvidia': 'NVDA',
            'ë„·í”Œë¦­ìŠ¤': 'NFLX', 'netflix': 'NFLX', 'Netflix': 'NFLX',
            'ì˜¤ë¼í´': 'ORCL', 'oracle': 'ORCL', 'Oracle': 'ORCL',
            'ì„¸ì¼ì¦ˆí¬ìŠ¤': 'CRM', 'salesforce': 'CRM', 'Salesforce': 'CRM',
            'ì–´ë„ë¹„': 'ADBE', 'adobe': 'ADBE', 'Adobe': 'ADBE',
            'ì¸í…”': 'INTC', 'intel': 'INTC', 'Intel': 'INTC',
            'AMD': 'AMD', 'amd': 'AMD',
            'ì‹œìŠ¤ì½”': 'CSCO', 'cisco': 'CSCO', 'Cisco': 'CSCO',
            'IBM': 'IBM', 'ibm': 'IBM',
            
            # ë°˜ë„ì²´
            'í€„ì»´': 'QCOM', 'qualcomm': 'QCOM', 'Qualcomm': 'QCOM',
            'ë¸Œë¡œë“œì»´': 'AVGO', 'broadcom': 'AVGO', 'Broadcom': 'AVGO',
            'í…ì‚¬ìŠ¤ì¸ìŠ¤íŠ¸ë£¨ë¨¼íŠ¸': 'TXN', 'texas instruments': 'TXN', 'Texas Instruments': 'TXN',
            'ë§ˆì´í¬ë¡ ': 'MU', 'micron': 'MU', 'Micron': 'MU',
            'ì–´í”Œë¼ì´ë“œë¨¸í‹°ë¦¬ì–¼ì¦ˆ': 'AMAT', 'applied materials': 'AMAT', 'Applied Materials': 'AMAT',
            'ë¼ë©”ë¦¬ì„œì¹˜': 'LRCX', 'lam research': 'LRCX', 'Lam Research': 'LRCX',
            'ì•„ë‚ ë¡œê·¸ë””ë°”ì´ìŠ¤': 'ADI', 'analog devices': 'ADI', 'Analog Devices': 'ADI',
            'ë§ˆì´í¬ë¡œì¹©': 'MCHP', 'microchip': 'MCHP', 'Microchip': 'MCHP',
            'ì¼€ì´ì—˜ì—ì´': 'KLAC', 'kla': 'KLAC', 'KLA': 'KLAC',
            'ë§ˆë²¨': 'MRVL', 'marvell': 'MRVL', 'Marvell': 'MRVL',
            
            # ì£¼ìš” ETFë“¤
            'ìŠ¤íŒŒì´ë”': 'SPY', 'spy': 'SPY', 'SPY': 'SPY',
            'ííí': 'QQQ', 'qqq': 'QQQ', 'QQQ': 'QQQ',
            'ë²¡í„°í† íƒˆ': 'VTI', 'vti': 'VTI', 'VTI': 'VTI',
            'ëŸ¬ì…€': 'IWM', 'iwm': 'IWM', 'IWM': 'IWM',
            'ë²¡í„°ì„ ì§„êµ­': 'VEA', 'vea': 'VEA', 'VEA': 'VEA',
            'ë²¡í„°ì‹ í¥êµ­': 'VWO', 'vwo': 'VWO', 'VWO': 'VWO',
            'ì´ìƒ¤ì–´': 'EFA', 'efa': 'EFA', 'EFA': 'EFA',
            'ê³¨ë“œ': 'GLD', 'gld': 'GLD', 'GLD': 'GLD',
            'ì‹¤ë²„': 'SLV', 'slv': 'SLV', 'SLV': 'SLV',
            'êµ­ì±„': 'TLT', 'tlt': 'TLT', 'TLT': 'TLT',
            'í•˜ì´ì¼ë“œ': 'HYG', 'hyg': 'HYG', 'HYG': 'HYG',
            'íšŒì‚¬ì±„': 'LQD', 'lqd': 'LQD', 'LQD': 'LQD',
            'ê¸ˆìœµ': 'XLF', 'xlf': 'XLF', 'XLF': 'XLF',
            'ê¸°ìˆ ': 'XLK', 'xlk': 'XLK', 'XLK': 'XLK',
            'ì—ë„ˆì§€': 'XLE', 'xle': 'XLE', 'XLE': 'XLE',
            'í—¬ìŠ¤ì¼€ì–´': 'XLV', 'xlv': 'XLV', 'XLV': 'XLV',
            'ì‚°ì—…ì¬': 'XLI', 'xli': 'XLI', 'XLI': 'XLI',
            'ì†Œë¹„ì¬': 'XLP', 'xlp': 'XLP', 'XLP': 'XLP',
            'ìœ í‹¸ë¦¬í‹°': 'XLU', 'xlu': 'XLU', 'XLU': 'XLU',
            'ë¶€ë™ì‚°': 'XLRE', 'xlre': 'XLRE', 'XLRE': 'XLRE',
            
            # ê¸ˆìœµ (ì€í–‰, ë³´í—˜, í•€í…Œí¬)
            'JPëª¨ê±´': 'JPM', 'jpmorgan': 'JPM', 'JPMorgan': 'JPM',
            'ë±…í¬ì˜¤ë¸Œì•„ë©”ë¦¬ì¹´': 'BAC', 'bank of america': 'BAC', 'Bank of America': 'BAC',
            'ì›°ìŠ¤íŒŒê³ ': 'WFC', 'wells fargo': 'WFC', 'Wells Fargo': 'WFC',
            'ì‹œí‹°ê·¸ë£¹': 'C', 'citigroup': 'C', 'Citigroup': 'C',
            'ê³¨ë“œë§Œì‚­ìŠ¤': 'GS', 'goldman sachs': 'GS', 'Goldman Sachs': 'GS',
            'ëª¨ê±´ìŠ¤íƒ ë¦¬': 'MS', 'morgan stanley': 'MS', 'Morgan Stanley': 'MS',
            'ë²„í¬ì…”í•´ì„œì›¨ì´': 'BRK-B', 'berkshire hathaway': 'BRK-B', 'Berkshire Hathaway': 'BRK-B',
            'ë¹„ì': 'V', 'visa': 'V', 'Visa': 'V',
            'ë§ˆìŠ¤í„°ì¹´ë“œ': 'MA', 'mastercard': 'MA', 'Mastercard': 'MA',
            'í˜ì´íŒ”': 'PYPL', 'paypal': 'PYPL', 'PayPal': 'PYPL',
            'ìŠ¤í€˜ì–´': 'SQ', 'square': 'SQ', 'Square': 'SQ',
            'ì•„ë©”ë¦¬ì¹¸ìµìŠ¤í”„ë ˆìŠ¤': 'AXP', 'american express': 'AXP', 'American Express': 'AXP',
            'USë±…í¬': 'USB', 'us bank': 'USB', 'US Bank': 'USB',
            'PNC': 'PNC', 'pnc': 'PNC',
            'íŠ¸ëŸ¬ìŠ¤íŠ¸íŒŒì´ë‚¸ì…œ': 'TFC', 'truist': 'TFC', 'Truist': 'TFC',
            'ìºí”¼íƒˆì›': 'COF', 'capital one': 'COF', 'Capital One': 'COF',
            
            # í—¬ìŠ¤ì¼€ì–´/ì œì•½
            'ì¡´ìŠ¨ì•¤ì¡´ìŠ¨': 'JNJ', 'johnson & johnson': 'JNJ', 'Johnson & Johnson': 'JNJ',
            'í™”ì´ì': 'PFE', 'pfizer': 'PFE', 'Pfizer': 'PFE',
            'ìœ ë‚˜ì´í‹°ë“œí—¬ìŠ¤': 'UNH', 'unitedhealth': 'UNH', 'UnitedHealth': 'UNH',
            'ì• ë¸Œë¹„': 'ABBV', 'abbvie': 'ABBV', 'AbbVie': 'ABBV',
            'ë¨¸í¬': 'MRK', 'merck': 'MRK', 'Merck': 'MRK',
            'ì¨ëª¨í”¼ì…”': 'TMO', 'thermo fisher': 'TMO', 'Thermo Fisher': 'TMO',
            'ì• ë³´íŠ¸': 'ABT', 'abbott': 'ABT', 'Abbott': 'ABT',
            'ë‹¤ë‚˜í—ˆ': 'DHR', 'danaher': 'DHR', 'Danaher': 'DHR',
            'ë¸Œë¦¬ìŠ¤í†¨ë§ˆì´ì–´ìŠ¤': 'BMY', 'bristol myers': 'BMY', 'Bristol Myers': 'BMY',
            'ì•”ì  ': 'AMGN', 'amgen': 'AMGN', 'Amgen': 'AMGN',
            'ê¸¸ë¦¬ì•„ë“œ': 'GILD', 'gilead': 'GILD', 'Gilead': 'GILD',
            'ë°”ì´ì˜¤ì  ': 'BIIB', 'biogen': 'BIIB', 'Biogen': 'BIIB',
            'CVS': 'CVS', 'cvs': 'CVS',
            'ì‹œê·¸ë‚˜': 'CI', 'cigna': 'CI', 'Cigna': 'CI',
            'ì•¤í…œ': 'ANTM', 'anthem': 'ANTM', 'Anthem': 'ANTM',
            'íœ´ë§ˆë‚˜': 'HUM', 'humana': 'HUM', 'Humana': 'HUM',
            
            # ì†Œë¹„ì¬ (í•„ìˆ˜/ì„ì˜)
            'í”„ë¡í„°ì•¤ê°¬ë¸”': 'PG', 'procter & gamble': 'PG', 'Procter & Gamble': 'PG',
            'ì½”ì¹´ì½œë¼': 'KO', 'coca cola': 'KO', 'Coca Cola': 'KO',
            'í©ì‹œ': 'PEP', 'pepsi': 'PEP', 'Pepsi': 'PEP',
            'ì›”ë§ˆíŠ¸': 'WMT', 'walmart': 'WMT', 'Walmart': 'WMT',
            'í™ˆë””í¬': 'HD', 'home depot': 'HD', 'Home Depot': 'HD',
            'ë§¥ë„ë‚ ë“œ': 'MCD', 'mcdonalds': 'MCD', 'McDonalds': 'MCD',
            'ë‚˜ì´í‚¤': 'NKE', 'nike': 'NKE', 'Nike': 'NKE',
            'ìŠ¤íƒ€ë²…ìŠ¤': 'SBUX', 'starbucks': 'SBUX', 'Starbucks': 'SBUX',
            'íƒ€ê²Ÿ': 'TGT', 'target': 'TGT', 'Target': 'TGT',
            'ë¡œìš°ìŠ¤': 'LOW', 'lowes': 'LOW', 'Lowes': 'LOW',
            'ì½”ìŠ¤íŠ¸ì½”': 'COST', 'costco': 'COST', 'Costco': 'COST',
            'ë””ì¦ˆë‹ˆ': 'DIS', 'disney': 'DIS', 'Disney': 'DIS',
            'ì•Œë¦¬ë°”ë°”': 'BABA', 'alibaba': 'BABA', 'Alibaba': 'BABA',
            'ì´ë² ì´': 'EBAY', 'ebay': 'EBAY', 'eBay': 'EBAY',
            'ì—ì¸ ì´': 'ETSY', 'etsy': 'ETSY', 'Etsy': 'ETSY',
            
            # ì—ë„ˆì§€
            'ì—‘ìŠ¨ëª¨ë¹Œ': 'XOM', 'exxon mobil': 'XOM', 'Exxon Mobil': 'XOM',
            'ì²´ë¸Œë¡ ': 'CVX', 'chevron': 'CVX', 'Chevron': 'CVX',
            'ì½˜ì½”í•„ë¦½ìŠ¤': 'COP', 'conocophillips': 'COP', 'ConocoPhillips': 'COP',
            'EOG': 'EOG', 'eog': 'EOG',
            'ìŠëŸ¼ë²„ê±°': 'SLB', 'schlumberger': 'SLB', 'Schlumberger': 'SLB',
            'í•„ë¦½ìŠ¤66': 'PSX', 'phillips 66': 'PSX', 'Phillips 66': 'PSX',
            'ë°œë ˆë¡œ': 'VLO', 'valero': 'VLO', 'Valero': 'VLO',
            'í‚¨ë”ëª¨ê±´': 'KMI', 'kinder morgan': 'KMI', 'Kinder Morgan': 'KMI',
            'ì›ì—ë„ˆì§€': 'OKE', 'oneok': 'OKE', 'Oneok': 'OKE',
            'ìœŒë¦¬ì—„': 'WMB', 'williams': 'WMB', 'Williams': 'WMB',
            
            # ì‚°ì—…ì¬/í•­ê³µ
            'ë³´ì‰': 'BA', 'boeing': 'BA', 'Boeing': 'BA',
            'ìºí„°í•„ëŸ¬': 'CAT', 'caterpillar': 'CAT', 'Caterpillar': 'CAT',
            'ë””ì–´': 'DE', 'deere': 'DE', 'Deere': 'DE',
            'ì œë„ˆëŸ´ì¼ë ‰íŠ¸ë¦­': 'GE', 'general electric': 'GE', 'General Electric': 'GE',
            'í•˜ë‹ˆì›°': 'HON', 'honeywell': 'HON', 'Honeywell': 'HON',
            '3M': 'MMM', 'mmm': 'MMM',
            'UPS': 'UPS', 'ups': 'UPS',
            'í˜ë±ìŠ¤': 'FDX', 'fedex': 'FDX', 'FedEx': 'FDX',
            'ë¡íˆë“œë§ˆí‹´': 'LMT', 'lockheed martin': 'LMT', 'Lockheed Martin': 'LMT',
            'ë ˆì´ì‹œì˜¨': 'RTX', 'raytheon': 'RTX', 'Raytheon': 'RTX',
            'ì•„ë©”ë¦¬ì¹¸í•­ê³µ': 'AAL', 'american airlines': 'AAL', 'American Airlines': 'AAL',
            'ë¸íƒ€í•­ê³µ': 'DAL', 'delta': 'DAL', 'Delta': 'DAL',
            'ìœ ë‚˜ì´í‹°ë“œí•­ê³µ': 'UAL', 'united airlines': 'UAL', 'United Airlines': 'UAL',
            'ì‚¬ìš°ìŠ¤ì›¨ìŠ¤íŠ¸': 'LUV', 'southwest': 'LUV', 'Southwest': 'LUV',
            
            # í†µì‹ 
            'ë²„ë¼ì´ì¦Œ': 'VZ', 'verizon': 'VZ', 'Verizon': 'VZ',
            'AT&T': 'T', 'at&t': 'T', 'at&t': 'T',
            'Tëª¨ë°”ì¼': 'TMUS', 't mobile': 'TMUS', 'T Mobile': 'TMUS',
            'ì°¨í„°': 'CHTR', 'charter': 'CHTR', 'Charter': 'CHTR',
            'ì»´ìºìŠ¤íŠ¸': 'CMCSA', 'comcast': 'CMCSA', 'Comcast': 'CMCSA',
            'ë””ì‹œ': 'DISH', 'dish': 'DISH', 'Dish': 'DISH',
            
            # ìë™ì°¨
            'í¬ë“œ': 'F', 'ford': 'F', 'Ford': 'F',
            'ì œë„ˆëŸ´ëª¨í„°ìŠ¤': 'GM', 'general motors': 'GM', 'General Motors': 'GM',
            'ë¦¬ë¹„ì•ˆ': 'RIVN', 'rivian': 'RIVN', 'Rivian': 'RIVN',
            'ë£¨ì‹œë“œ': 'LCID', 'lucid': 'LCID', 'Lucid': 'LCID',
            'ë‹ˆì˜¤': 'NIO', 'nio': 'NIO', 'NIO': 'NIO',
            'XPeng': 'XPEV', 'xpeng': 'XPEV', 'xpev': 'XPEV',
            'ë¦¬': 'LI', 'li': 'LI', 'Li': 'LI',
            
            # ë¶€ë™ì‚° REITs
            'ì•„ë©”ë¦¬ì¹¸íƒ€ì›Œ': 'AMT', 'american tower': 'AMT', 'American Tower': 'AMT',
            'í”„ë¡œë¡œì§€ìŠ¤': 'PLD', 'prologis': 'PLD', 'Prologis': 'PLD',
            'í¬ë¼ìš´ìºìŠ¬': 'CCI', 'crown castle': 'CCI', 'Crown Castle': 'CCI',
            'ì´í€´ë‹‰ìŠ¤': 'EQIX', 'equinix': 'EQIX', 'Equinix': 'EQIX',
            'ì‚¬ì´ë¨¼í”„ë¡œí¼í‹°': 'SPG', 'simon property': 'SPG', 'Simon Property': 'SPG',
            'ë¦¬ì–¼í‹°ì¸ì»´': 'O', 'realty income': 'O', 'Realty Income': 'O',
            'ì›°íƒ€ì›Œ': 'WELL', 'welltower': 'WELL', 'Welltower': 'WELL',
            'ì—‘ìŠ¤íŠ¸ë¼ìŠ¤í˜ì´ìŠ¤': 'EXR', 'extra space': 'EXR', 'Extra Space': 'EXR',
            'ì• ë²Œë¡ ë² ì´': 'AVB', 'avalonbay': 'AVB', 'AvalonBay': 'AVB',
            'ì—í€´í‹°ë ˆì§€ë˜ì…œ': 'EQR', 'equity residential': 'EQR', 'Equity Residential': 'EQR',
            
            # ìœ í‹¸ë¦¬í‹°
            'ë„¥ìŠ¤íŠ¸ì—ë¼': 'NEE', 'next era': 'NEE', 'Next Era': 'NEE',
            'ë“€í¬ì—ë„ˆì§€': 'DUK', 'duke energy': 'DUK', 'Duke Energy': 'DUK',
            'ì„œë˜': 'SO', 'southern': 'SO', 'Southern': 'SO',
            'ë„ë¯¸ë‹ˆì–¸': 'D', 'dominion': 'D', 'Dominion': 'D',
            'ì•„ë©”ë¦¬ì¹¸ì¼ë ‰íŠ¸ë¦­': 'AEP', 'american electric': 'AEP', 'American Electric': 'AEP',
            'ì—‘ì…€ë¡ ': 'EXC', 'exelon': 'EXC', 'Exelon': 'EXC',
            'ì—‘ì…€ì—ë„ˆì§€': 'XEL', 'xcel energy': 'XEL', 'Xcel Energy': 'XEL',
            'Sempra': 'SRE', 'sempra': 'SRE',
            'í¼ë¸”ë¦­ì„œë¹„ìŠ¤': 'PEG', 'public service': 'PEG', 'Public Service': 'PEG',
            'ì»¨ì†”ë¦¬ë°ì´í‹°ë“œì—ë””ìŠ¨': 'ED', 'consolidated edison': 'ED', 'Consolidated Edison': 'ED',
            
            # ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´
            'ë¡œì¿ ': 'ROKU', 'roku': 'ROKU', 'Roku': 'ROKU',
            'ìŠ¤í¬í‹°íŒŒì´': 'SPOT', 'spotify': 'SPOT', 'Spotify': 'SPOT',
            'ì›Œë„ˆë¸Œë¼ë”ìŠ¤': 'WBD', 'warner bros': 'WBD', 'Warner Bros': 'WBD',
            'íŒŒë¼ë§ˆìš´íŠ¸': 'PARA', 'paramount': 'PARA', 'Paramount': 'PARA',
            'í­ìŠ¤': 'FOX', 'fox': 'FOX', 'Fox': 'FOX',
            'í­ìŠ¤ë‰´ìŠ¤': 'FOXA', 'fox news': 'FOXA', 'Fox News': 'FOXA',
            
            # ì¤‘êµ­ ADR
            'ì§•ë™': 'JD', 'jd': 'JD', 'JD': 'JD',
            'í•€ë‘¬ë‘¬': 'PDD', 'pinduoduo': 'PDD', 'Pinduoduo': 'PDD',
            'ë°”ì´ë‘': 'BIDU', 'baidu': 'BIDU', 'Baidu': 'BIDU',
            'ë¹Œë¦¬ë¹Œë¦¬': 'BILI', 'bilibili': 'BILI', 'Bilibili': 'BILI',
            'ë””ë””': 'DIDI', 'didi': 'DIDI', 'DiDi': 'DIDI',
            'í…ì„¼íŠ¸ë®¤ì§': 'TME', 'tencent music': 'TME', 'Tencent Music': 'TME',
            
            # ì•”í˜¸í™”í ê´€ë ¨
            'ì½”ì¸ë² ì´ìŠ¤': 'COIN', 'coinbase': 'COIN', 'Coinbase': 'COIN',
            'ë§ˆì´í¬ë¡œìŠ¤íŠ¸ë˜í‹°ì§€': 'MSTR', 'microstrategy': 'MSTR', 'MicroStrategy': 'MSTR',
            'ë¼ì´ì—‡': 'RIOT', 'riot': 'RIOT', 'Riot': 'RIOT',
            'ë§ˆë¼': 'MARA', 'mara': 'MARA', 'Mara': 'MARA',
            'ë¹„íŠ¸ì½”ì¸': 'BITB', 'bitcoin': 'BITB', 'Bitcoin': 'BITB',
            'ì•„ì´ì…°ì–´': 'IBIT', 'ishares': 'IBIT', 'iShares': 'IBIT',
            
            # ê¸°íƒ€ ì£¼ìš” ê¸°ì—…ë“¤
            'ìš°ë²„': 'UBER', 'uber': 'UBER', 'Uber': 'UBER',
            'ë¼ì´í”„íŠ¸': 'LYFT', 'lyft': 'LYFT', 'Lyft': 'LYFT',
            'ìŠ¤ëƒ…': 'SNAP', 'snap': 'SNAP', 'Snap': 'SNAP',
            'íŠ¸ìœ„í„°': 'TWTR', 'twitter': 'TWTR', 'Twitter': 'TWTR',
            'ì¤Œ': 'ZOOM', 'zoom': 'ZOOM', 'Zoom': 'ZOOM',
            'ë„ì¿ ì‚¬ì¸': 'DOCU', 'docusign': 'DOCU', 'DocuSign': 'DOCU',
            'íŒ”ë€í‹°ì–´': 'PLTR', 'palantir': 'PLTR', 'Palantir': 'PLTR',
            'ìŠ¤ë…¸ìš°í”Œë ˆì´í¬': 'SNOW', 'snowflake': 'SNOW', 'Snowflake': 'SNOW',
            'ìŠ¬ë™': 'WORK', 'slack': 'WORK', 'Slack': 'WORK',
            'í ë¡œí†¤': 'PTON', 'peloton': 'PTON', 'Peloton': 'PTON',
            'ì•„í¬': 'ARKK', 'ark': 'ARKK', 'ARK': 'ARKK',
            'ì•„í¬ê²Œë†ˆ': 'ARKG', 'ark genome': 'ARKG', 'ARK Genome': 'ARKG',
            'ì•„í¬ì›¹': 'ARKW', 'ark web': 'ARKW', 'ARK Web': 'ARKW',
            
            # í•œêµ­ ì£¼ì‹
            'ì‚¼ì„±ì „ì': '005930', 'samsung': '005930', 'Samsung': '005930',
            'í˜„ëŒ€ìë™ì°¨': '005380', 'hyundai': '005380', 'Hyundai': '005380',
            'SKí•˜ì´ë‹‰ìŠ¤': '000660', 'sk hynix': '000660', 'SK Hynix': '000660',
            'LGì—ë„ˆì§€ì†”ë£¨ì…˜': '373220', 'lg energy': '373220', 'LG Energy': '373220',
            'ë„¤ì´ë²„': '035420', 'naver': '035420', 'Naver': '035420',
            'ì¹´ì¹´ì˜¤': '035720', 'kakao': '035720', 'Kakao': '035720',
        }
        
        # ê²€ìƒ‰ì–´ ì •ê·œí™” (ì†Œë¬¸ì ë³€í™˜)
        normalized_query = query.lower().strip()
        
        # ë§¤í•‘ëœ ì‹¬ë³¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if normalized_query in stock_mappings:
            symbol = stock_mappings[normalized_query]
            # ì›ë³¸ ê²€ìƒ‰ì–´ + ì‹¬ë³¼ë¡œ í™•ì¥
            expanded_query = f"{query} {symbol}"
            Logger.debug(f"ê²€ìƒ‰ì–´ í™•ì¥: '{query}' â†’ '{expanded_query}'")
            Logger.debug("log.test rag.preprocess_query.expand")
            return expanded_query
        
        # ì›ë³¸ ê²€ìƒ‰ì–´ ë°˜í™˜
        Logger.debug("log.test rag.preprocess_query.no_change")
        return query

    @classmethod
    async def retrieve(cls, 
                      query: str, 
                      top_k: Optional[int] = None, 
                      hybrid: bool = True,
                      bm25_weight: float = 0.2,
                      vector_weight: float = 0.2) -> List[Dict[str, Any]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - BM25ì™€ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¡°í•©
        
        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’: config.default_k)
            hybrid: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            bm25_weight: BM25 ì ìˆ˜ ê°€ì¤‘ì¹˜
            vector_weight: ë²¡í„° ì ìˆ˜ ê°€ì¤‘ì¹˜
            
        Returns:
            List[Dict]: ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        Logger.debug(f"log.test rag.retrieve.enter k={top_k} hybrid={hybrid} bw={bm25_weight} vw={vector_weight}")
        if not cls._initialized:
            raise RuntimeError("RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        
        start_time = time.time()
        k = top_k or cls._config.default_k
        
        # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬ ì¶”ê°€
        processed_query = cls._preprocess_query(query)
        
        Logger.info(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘: '{query}' â†’ '{processed_query}' (k={k}, hybrid={hybrid})")
        
        try:
            cls._stats["search_requests"] += 1
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì‚¬ìš©)
            if hybrid and cls._search_available and cls._vector_available:
                cls._stats["hybrid_searches"] += 1
                results = await cls._hybrid_search(processed_query, k, bm25_weight, vector_weight)
            elif cls._vector_available:
                Logger.info("ë²¡í„° ê²€ìƒ‰ ëª¨ë“œ")
                results = await cls._vector_search_only(processed_query, k)
            elif cls._search_available:
                Logger.info("í‚¤ì›Œë“œ ê²€ìƒ‰ ëª¨ë“œ")
                results = await cls._bm25_search_only(processed_query, k)
            else:
                Logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ê²€ìƒ‰ ì„œë¹„ìŠ¤ê°€ ì—†ìŒ")
                return []
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            search_time = time.time() - start_time
            cls._update_search_stats(search_time)
            
            Logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ({search_time:.3f}ì´ˆ)")
            Logger.debug(f"log.test rag.retrieve.ok results={len(results)} time={search_time:.3f}")
            # ê²°ê³¼ ì¼ë¶€ ìƒì„¸ í‘œì‹œ (ìƒìœ„ 5ê°œ)
            try:
                preview_count = min(5, len(results))
                for idx in range(preview_count):
                    r = results[idx]
                    md = r.get("metadata", {}) if isinstance(r, dict) else {}
                    title = md.get("title") or (r.get("content", "")[:60] if isinstance(r, dict) else "")
                    source = md.get("source", "unknown")
                    score_val = r.get("score", 0.0) if isinstance(r, dict) else 0.0
                    s_type = r.get("search_type", "unknown") if isinstance(r, dict) else "unknown"
                    Logger.debug(f"log.test rag.result#{idx+1} type={s_type} score={score_val:.3f} source='{source}' title='{title}'")
            except Exception as _e:
                Logger.debug(f"log.test rag.retrieve.preview.fail err={str(_e)}")
            
            return results
            
        except Exception as e:
            Logger.error(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.retrieve.fail err={str(e)}")
            return []

    @classmethod
    async def _hybrid_search(cls, 
                           query: str, 
                           k: int, 
                           bm25_weight: float, 
                           vector_weight: float) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        Logger.debug("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ëª¨ë“œ ì‹œì‘")
        Logger.debug(f"log.test rag.hybrid.enter q='{query}' k={k} bw={bm25_weight} vw={vector_weight}")
        
        # ë³‘ë ¬ë¡œ ë‘ ê°€ì§€ ê²€ìƒ‰ ì‹¤í–‰
        bm25_task = cls._bm25_search_only(query, k * 2)  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ ë‹¤ì–‘ì„± í™•ë³´
        vector_task = cls._vector_search_only(query, k * 2)
        
        bm25_results, vector_results = await asyncio.gather(bm25_task, vector_task)
        
        # ì ìˆ˜ í•©ì„± ë° ìˆœìœ„ ê²°ì •
        combined_results = cls._fuse_search_results(
            bm25_results, vector_results, 
            bm25_weight, vector_weight
        )
        
        # ìƒìœ„ Kê°œ ë°˜í™˜
        out = combined_results[:k]
        Logger.debug(f"log.test rag.hybrid.ok returned={len(out)}")
        return out

    @classmethod
    async def _bm25_search_only(cls, query: str, k: int) -> List[Dict[str, Any]]:
        """BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ ì‹¤í–‰ - í¬ë¡¤ëŸ¬ êµ¬ì¡° ë° OpenSearch ì‘ë‹µ í˜•ì‹ ëª¨ë‘ ì§€ì›"""
        try:
            Logger.debug(f"log.test rag.bm25.enter q='{query}' k={k}")
            from service.search.search_service import SearchService

            # BM25 ì¿¼ë¦¬ ì‘ì„±
            search_query = {
                "query": {
                    "multi_match": {
                        "query": query,
                        "fields": ["title", "ticker", "source"],
                        "type": "best_fields"
                    }
                },
                "size": k
            }

            # ê²€ìƒ‰ ì‹¤í–‰
            response = await SearchService.search("yahoo_finance_news", search_query)
            if not response:
                return []

            Logger.debug(f"BM25 ê²€ìƒ‰ ê²°ê³¼: {response}")

            results: list[dict] = []

            # 1) wrapper: response.response.hits.hits
            hits = (((response.get("response") or {}).get("hits") or {}).get("hits"))
            if isinstance(hits, list) and hits:
                for hit in hits:
                    src = hit.get("_source") or {}
                    title = src.get("title") or "No title"
                    source = src.get("source") or "unknown"

                    metadata = {
                        "title": title,
                        "source": source,
                        "ticker": src.get("ticker", ""),
                        "date": src.get("date", ""),
                        "link": src.get("link") or "",
                        "content_type": src.get("content_type", ""),
                        "task_id": src.get("task_id", ""),
                        "collected_at": src.get("collected_at", ""),
                        "created_at": src.get("created_at", "")
                    }

                    content = src.get("content") or title
                    results.append({
                        "id": hit.get("_id"),
                        "content": content,
                        "metadata": metadata,
                        "score": hit.get("_score"),
                        "search_type": "bm25"
                    })
                Logger.debug(f"log.test rag.bm25.ok results={len(results)}")
                return results

            # 2) documents ë°°ì—´ë§Œ ìˆëŠ” ê²½ìš°
            docs = response.get("documents")
            if isinstance(docs, list) and docs:
                for doc in docs:
                    title = doc.get("title") or "No title"
                    source = doc.get("source") or "unknown"

                    metadata = {
                        "title": title,
                        "source": source,
                        "ticker": doc.get("ticker", ""),
                        "date": doc.get("date", ""),
                        "link": doc.get("link") or "",
                        "content_type": doc.get("content_type", ""),
                        "task_id": doc.get("task_id", ""),
                        "collected_at": doc.get("collected_at", ""),
                        "created_at": doc.get("created_at", "")
                    }

                    content = doc.get("content") or title
                    results.append({
                        "id": doc.get("id") or doc.get("task_id") or title,
                        "content": content,
                        "metadata": metadata,
                        "score": doc.get("score"),
                        "search_type": "bm25"
                    })
                Logger.debug(f"log.test rag.bm25.ok results={len(results)}")
                return results

            # 3) OpenSearch ì›ë³¸(top-level hits)
            hits_top = (response.get("hits") or {}).get("hits")
            if isinstance(hits_top, list) and hits_top:
                for hit in hits_top:
                    src = hit.get("_source") or {}
                    title = src.get("title") or "No title"
                    source = src.get("source") or "unknown"

                    metadata = {
                        "title": title,
                        "source": source,
                        "ticker": src.get("ticker", ""),
                        "date": src.get("date", ""),
                        "link": src.get("link") or "",
                        "content_type": src.get("content_type", ""),
                        "task_id": src.get("task_id", ""),
                        "collected_at": src.get("collected_at", ""),
                        "created_at": src.get("created_at", "")
                    }

                    content = src.get("content") or title
                    results.append({
                        "id": hit.get("_id"),
                        "content": content,
                        "metadata": metadata,
                        "score": hit.get("_score"),
                        "search_type": "bm25"
                    })
                return results

            Logger.warning("BM25 ì‘ë‹µì—ì„œ hits/documentsë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []

        except Exception as e:
            Logger.error(f"BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.bm25.fail err={str(e)}")
            return []


    @classmethod
    async def _vector_search_only(cls, query: str, k: int) -> List[Dict[str, Any]]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ë§Œ ì‹¤í–‰ - RAG ì „ìš© í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"""
        try:
            Logger.debug(f"log.test rag.vector.enter q='{query}' k={k}")
            # RAG ì „ìš© ë²¡í„° í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© (coroutine ì¬ì‚¬ìš© ë¬¸ì œ í•´ê²°)
            if cls._rag_vector_client:
                vector_results = await cls._rag_vector_client.similarity_search(
                    query=query,
                    top_k=k
                )
                
                if not vector_results.get("success"):
                    Logger.warn(f"RAG ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {vector_results.get('error', 'Unknown error')}")
                    return []
                
                results = []
                for result in vector_results.get("results", []):
                    doc_result = {
                        "id": result.get("id", ""),
                        "content": result.get("content", ""),
                        "metadata": result.get("metadata", {}),
                        "score": result.get("score", 0.0),
                        "search_type": "vector"
                    }
                    results.append(doc_result)
                
                Logger.debug(f"log.test rag.vector.ok results={len(results)}")
                return results
            
            else:
                # Fallback: ê¸°ì¡´ VectorDbService ì‚¬ìš©
                from service.vectordb.vectordb_service import VectorDbService
                
                # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                embed_result = await VectorDbService.embed_text(query)
                
                if not embed_result.get("success"):
                    Logger.warn("ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    return []
                
                # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
                vector_results = await VectorDbService.similarity_search(
                    query=query,
                    top_k=k
                )
                
                if not vector_results:
                    return []
                
                results = []
                for result in vector_results.get("results", []):
                    doc_result = {
                        "id": result.get("id", ""),
                        "content": result.get("content", ""),
                        "metadata": result.get("metadata", {}),
                        "score": result.get("score", 0.0),
                        "search_type": "vector"
                    }
                    results.append(doc_result)
                
                Logger.debug(f"log.test rag.vector.ok results={len(results)}")
                return results
            
        except Exception as e:
            Logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.vector.fail err={str(e)}")
            return []

    @classmethod
    def _fuse_search_results(cls,
                           bm25_results: List[Dict[str, Any]],
                           vector_results: List[Dict[str, Any]],
                           bm25_weight: float,
                           vector_weight: float) -> List[Dict[str, Any]]:
        """BM25ì™€ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ í•©ì„±"""
        Logger.debug(f"log.test rag.fuse.enter bm25={len(bm25_results)} vector={len(vector_results)} bw={bm25_weight} vw={vector_weight}")
        
        # ê²°ê³¼ë¥¼ IDë¡œ ë§¤í•‘
        bm25_map = {r["id"]: r for r in bm25_results}
        vector_map = {r["id"]: r for r in vector_results}
        
        # ëª¨ë“  ê³ ìœ í•œ ë¬¸ì„œ ID ìˆ˜ì§‘
        all_ids = set(bm25_map.keys()) | set(vector_map.keys())
        
        combined = []
        
        for doc_id in all_ids:
            bm25_result = bm25_map.get(doc_id)
            vector_result = vector_map.get(doc_id)
            
            # ì ìˆ˜ ì •ê·œí™” ë° í•©ì„±
            bm25_score = cls._normalize_bm25_score(bm25_result["score"]) if bm25_result else 0.0
            vector_score = vector_result["score"] if vector_result else 0.0
            
            # ê°€ì¤‘ í‰ê·  ì ìˆ˜ ê³„ì‚°
            combined_score = (bm25_score * bm25_weight) + (vector_score * vector_weight)
            
            # ìš°ì„ ìˆœìœ„: BM25 > Vector (ë©”íƒ€ë°ì´í„° í’ë¶€ë„ ê¸°ì¤€)
            primary_result = bm25_result or vector_result
            
            combined_result = {
                **primary_result,
                "score": combined_score,
                "search_type": "hybrid",
                "score_details": {
                    "bm25_score": bm25_score,
                    "vector_score": vector_score,
                    "bm25_weight": bm25_weight,
                    "vector_weight": vector_weight
                }
            }
            
            combined.append(combined_result)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        combined.sort(key=lambda x: x["score"], reverse=True)
        Logger.debug(f"log.test rag.fuse.ok combined={len(combined)}")
        return combined

    @classmethod
    def _normalize_bm25_score(cls, score: float) -> float:
        """BM25 ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
        # ê°„ë‹¨í•œ sigmoid ì •ê·œí™” (ì‹¤ì œë¡œëŠ” ë°ì´í„°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
        import math
        value = 1 / (1 + math.exp(-score / 10))
        Logger.debug(f"log.test rag.normalize score_in={score:.3f} score_out={value:.3f}")
        return value

    @classmethod
    def _update_search_stats(cls, search_time: float):
        """ê²€ìƒ‰ ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        current_avg = cls._stats["avg_search_time"]
        search_count = cls._stats["search_requests"]
        
        # ì´ë™ í‰ê·  ê³„ì‚°
        cls._stats["avg_search_time"] = (
            (current_avg * (search_count - 1)) + search_time
        ) / search_count

    @classmethod
    def is_initialized(cls) -> bool:
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ìƒíƒœ í™•ì¸ (111 íŒ¨í„´)"""
        return cls._initialized

    @classmethod
    def get_stats(cls) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        Logger.debug("log.test rag.get_stats.enter")
        out = {
            **cls._stats,
            "initialized": cls._initialized,
            "search_available": cls._search_available,
            "vector_available": cls._vector_available,
            "hybrid_capable": cls._search_available and cls._vector_available
        }
        Logger.debug(f"log.test rag.get_stats.ok initialized={out['initialized']} hybrid={out['hybrid_capable']}")
        return out

    @classmethod
    def get_config(cls) -> Optional[RagConfig]:
        """í˜„ì¬ í™œì„±í™”ëœ RAG ì„¤ì • ë°˜í™˜"""
        return getattr(cls, "_config", None)

    @classmethod
    async def health_check(cls) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        Logger.debug("log.test rag.health.enter")
        if not cls._initialized:
            out = {
                "status": "not_initialized",
                "message": "RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"
            }
            Logger.debug("log.test rag.health.not_initialized")
            return out
        
        # ì˜ì¡´ ì„œë¹„ìŠ¤ ìƒíƒœ ì¬í™•ì¸
        try:
            from service.search.search_service import SearchService
            from service.vectordb.vectordb_service import VectorDbService
            
            search_status = SearchService.is_initialized()
            vector_status = VectorDbService.is_initialized()
            
            if search_status and vector_status:
                status = "healthy"
            elif search_status or vector_status:
                status = "degraded"
            else:
                status = "critical"
            
            out = {
                "status": status,
                "initialized": cls._initialized,
                "dependencies": {
                    "search_service": search_status,
                    "vector_service": vector_status
                },
                "capabilities": {
                    "hybrid_search": search_status and vector_status,
                    "keyword_search": search_status,
                    "vector_search": vector_status
                },
                "stats": cls.get_stats()
            }
            Logger.debug(f"log.test rag.health.ok status={status}")
            return out
            
        except Exception as e:
            Logger.error(f"RAG ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.health.fail err={str(e)}")
            return {
                "status": "error",
                "message": f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}"
            }

    @classmethod
    async def shutdown(cls) -> bool:
        """ì„œë¹„ìŠ¤ ì¢…ë£Œ (111 íŒ¨í„´)"""
        try:
            Logger.debug("log.test rag.shutdown.enter")
            if not cls._initialized:
                Logger.info("RAG ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì¢…ë£Œ ìŠ¤í‚µ")
                return True
            
            Logger.info("ğŸ”„ RAG ì„œë¹„ìŠ¤ ì¢…ë£Œ ì‹œì‘...")
            
            # í†µê³„ ë¡œê·¸ ì¶œë ¥
            stats = cls.get_stats()
            Logger.info(f"RAG ì„œë¹„ìŠ¤ í†µê³„:")
            Logger.info(f"  - ì¸ë±ì‹±ëœ ë¬¸ì„œ: {stats['documents_indexed']}ê°œ")
            Logger.info(f"  - ê²€ìƒ‰ ìš”ì²­: {stats['search_requests']}ê°œ")
            Logger.info(f"  - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: {stats['hybrid_searches']}ê°œ")
            Logger.info(f"  - í‰ê·  ê²€ìƒ‰ ì‹œê°„: {stats['avg_search_time']:.3f}ì´ˆ")
            
            # RAG ì „ìš© ë²¡í„° í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ
            if cls._rag_vector_client:
                try:
                    await cls._rag_vector_client.close()
                    Logger.info("âœ… RAG ì „ìš© ë²¡í„° í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì™„ë£Œ")
                except Exception as e:
                    Logger.error(f"âŒ RAG ì „ìš© ë²¡í„° í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            
            # ìƒíƒœ ì´ˆê¸°í™”
            cls._initialized = False
            cls._config = None
            cls._search_available = False
            cls._vector_available = False
            cls._rag_vector_client = None
            cls._reset_stats()
            
            Logger.info("âœ… RAG ì„œë¹„ìŠ¤ ì¢…ë£Œ ì™„ë£Œ")
            Logger.debug("log.test rag.shutdown.ok")
            return True
            
        except Exception as e:
            Logger.error(f"âŒ RAG ì„œë¹„ìŠ¤ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
            Logger.debug(f"log.test rag.shutdown.fail err={str(e)}")
            return False