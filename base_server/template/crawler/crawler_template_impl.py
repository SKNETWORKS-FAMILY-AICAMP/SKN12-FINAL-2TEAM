import hashlib
import uuid
import json
import time
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

# í…œí”Œë¦¿ ê¸°ë³¸ í´ë˜ìŠ¤
from template.base.template.crawler_template import CrawlerTemplate
from template.base.template_context import TemplateContext
from template.base.template_type import TemplateType

# ì§ë ¬í™” í´ë˜ìŠ¤ (Request/Response)
from template.crawler.common.crawler_serialize import (
    CrawlerExecuteRequest, CrawlerExecuteResponse,
    CrawlerStatusRequest, CrawlerStatusResponse,
    CrawlerHealthRequest, CrawlerHealthResponse,
    CrawlerStopRequest, CrawlerStopResponse,
    CrawlerDataRequest, CrawlerDataResponse,
    CrawlerYahooFinanceRequest, CrawlerYahooFinanceResponse,
    CrawlerScheduleRequest, CrawlerScheduleResponse
)

# ëª¨ë¸ í´ë˜ìŠ¤
from template.crawler.common.crawler_model import CrawlerTask, CrawlerData, NewsData

# ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
from service.service_container import ServiceContainer
from service.core.logger import Logger
from service.cache.cache_service import CacheService
from service.search.search_service import SearchService
from service.vectordb.vectordb_service import VectorDbService
from service.external.external_service import ExternalService
from service.scheduler.scheduler_service import SchedulerService
from service.lock.lock_service import LockService
from service.storage.storage_service import StorageService

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import yfinance as yf
import pandas as pd

class CrawlerTemplateImpl(CrawlerTemplate):
    def __init__(self):
        super().__init__()
        self.v_active_tasks = {}  # task_id -> CrawlerTask
        self.v_processed_hashes = set()  # ì²˜ë¦¬ëœ ë‰´ìŠ¤ í•´ì‹œí‚¤ ì„¸íŠ¸
        self.v_news_cache = []  # ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìºì‹œ
        self.v_scheduler_task_id = None  # ìŠ¤ì¼€ì¤„ëŸ¬ ì‘ì—… ID
        
    def on_load_data(self, config):
        """í¬ë¡¤ëŸ¬ í…œí”Œë¦¿ ì „ìš© ë°ì´í„° ë¡œë”©"""
        try:
            Logger.info("Crawler í…œí”Œë¦¿ ë°ì´í„° ë¡œë“œ ì‹œì‘")
            
            # ìºì‹œì—ì„œ ê¸°ì¡´ í•´ì‹œí‚¤ ë³µì›
            # self._restore_hash_cache()
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
            # self._initialize_scheduler()
            
            Logger.info("Crawler í…œí”Œë¦¿ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            Logger.error(f"Crawler í…œí”Œë¦¿ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            
    def on_client_create(self, db_client, client_session):
        """ì‹ ê·œ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œ í˜¸ì¶œ"""
        try:
            Logger.info(f"Crawler: ì‹ ê·œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±")
            # í¬ë¡¤ëŸ¬ëŠ” ì‚¬ìš©ìë³„ ì´ˆê¸°í™”ê°€ í•„ìš” ì—†ìŒ
        except Exception as e:
            Logger.error(f"Crawler í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
    def on_client_update(self, db_client, client_session):
        """í´ë¼ì´ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ì‹œ í˜¸ì¶œ"""
        try:
            Logger.info(f"Crawler: í´ë¼ì´ì–¸íŠ¸ ì—…ë°ì´íŠ¸")
            # í¬ë¡¤ëŸ¬ëŠ” ì‚¬ìš©ìë³„ ì—…ë°ì´íŠ¸ê°€ í•„ìš” ì—†ìŒ
        except Exception as e:
            Logger.error(f"Crawler í´ë¼ì´ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    def _restore_hash_cache(self):
        """ìºì‹œì—ì„œ ê¸°ì¡´ í•´ì‹œí‚¤ ë³µì›"""
        try:
            # CacheService ì§ì ‘ ì‚¬ìš© (ServiceContainer.get_cache_service()ê°€ CacheService í´ë˜ìŠ¤ë¥¼ ë°˜í™˜)
            cache_service = ServiceContainer.get_cache_service()
            if cache_service and cache_service.is_initialized():
                # ì§€ë‚œ 7ì¼ê°„ì˜ í•´ì‹œí‚¤ ë³µì›
                v_cache_key = "crawler:processed_hashes"
                
                # get_client()ë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì–»ê³  get_string() ì‚¬ìš©
                async def restore_hashes():
                    try:
                        async with cache_service.get_client() as client:
                            v_cached_hashes = await client.get_string(v_cache_key)
                            if v_cached_hashes:
                                self.v_processed_hashes = set(json.loads(v_cached_hashes))
                                Logger.info(f"ìºì‹œì—ì„œ {len(self.v_processed_hashes)}ê°œ í•´ì‹œí‚¤ ë³µì›")
                    except Exception as e:
                        Logger.error(f"ë¹„ë™ê¸° í•´ì‹œí‚¤ ë³µì› ì‹¤íŒ¨: {e}")
                
                # ë™ê¸° ë©”ì„œë“œì—ì„œ ë¹„ë™ê¸° í˜¸ì¶œ ì²˜ë¦¬
                import asyncio
                try:
                    # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                    loop = asyncio.get_running_loop()
                    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰ (ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ)
                    loop.create_task(restore_hashes())
                    Logger.debug("í•´ì‹œí‚¤ ë³µì› ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘")
                except RuntimeError:
                    # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ì‹¤í–‰
                    try:
                        asyncio.run(restore_hashes())
                        Logger.debug("í•´ì‹œí‚¤ ë³µì› ì‘ì—… ì™„ë£Œ")
                    except Exception as run_error:
                        Logger.warn(f"í•´ì‹œí‚¤ ìºì‹œ ë³µì› ì‹¤í–‰ ì‹¤íŒ¨: {run_error}")
                    
        except Exception as e:
            Logger.error(f"í•´ì‹œí‚¤ ìºì‹œ ë³µì› ì‹¤íŒ¨: {e}")

    def _initialize_scheduler(self):
        """1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
        try:
            # SchedulerServiceë¥¼ ì§ì ‘ ì‚¬ìš© (ServiceContainer.get_scheduler_service()ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŒ)
            from service.scheduler.scheduler_service import SchedulerService
            from service.scheduler.base_scheduler import ScheduleJob, ScheduleType
            
            if SchedulerService.is_initialized():
                # 1ì‹œê°„(3600ì´ˆ)ë§ˆë‹¤ í¬ë¡¤ë§ ì‹¤í–‰í•˜ëŠ” ì‘ì—… ìƒì„±
                crawler_job = ScheduleJob(
                    job_id="yahoo_finance_crawler",
                    name="Yahoo Finance ë‰´ìŠ¤ í¬ë¡¤ë§",
                    schedule_type=ScheduleType.INTERVAL,  # ì£¼ê¸°ì  ì‹¤í–‰
                    schedule_value=3600,  # 1ì‹œê°„(3600ì´ˆ)
                    callback=self._scheduled_crawling_task,
                    enabled=True,  # í™œì„±í™”
                    use_distributed_lock=True,  # ë¶„ì‚°ë½ ì‚¬ìš©
                    lock_key="crawler_yahoo_finance_lock",  # ë½ í‚¤
                    lock_ttl=1800  # 30ë¶„ (í¬ë¡¤ë§ ìµœëŒ€ ì‹œê°„)
                )
                
                # ë¹„ë™ê¸° ì‘ì—… ì¶”ê°€ë¥¼ ìœ„í•œ ë˜í¼
                async def add_crawler_job():
                    try:
                        await SchedulerService.add_job(crawler_job)
                        self.v_scheduler_task_id = crawler_job.job_id
                        Logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ - Task ID: {self.v_scheduler_task_id}")
                        
                        # ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰ (immediate_run íš¨ê³¼)
                        Logger.info("í¬ë¡¤ëŸ¬ ì¦‰ì‹œ ì‹¤í–‰ ì‹œì‘")
                        await self._scheduled_crawling_task()
                        
                    except Exception as e:
                        Logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‘ì—… ì¶”ê°€ ì‹¤íŒ¨: {e}")
                
                # ë¹„ë™ê¸° ì‘ì—… ìŠ¤ì¼€ì¤„ë§
                import asyncio
                try:
                    # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                    loop = asyncio.get_running_loop()
                    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰ (ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ)
                    loop.create_task(add_crawler_job())
                    Logger.debug("ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘")
                except RuntimeError:
                    # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ì‹¤í–‰
                    try:
                        asyncio.run(add_crawler_job())
                        Logger.debug("ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì‘ì—… ì™„ë£Œ")
                    except Exception as run_error:
                        Logger.warn(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì‹¤í–‰ ì‹¤íŒ¨: {run_error}")
            else:
                Logger.warn("SchedulerServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
        except Exception as e:
            Logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _scheduled_crawling_task(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ì— ì˜í•´ í˜¸ì¶œë˜ëŠ” í¬ë¡¤ë§ ì‘ì—…"""
        Logger.info("â° ìŠ¤ì¼€ì¤„ëŸ¬ì— ì˜í•œ ìë™ í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # ìë™ í¬ë¡¤ë§ ìš”ì²­ ìƒì„± (ëª¨ë“  í•„ìˆ˜ í•„ë“œë¥¼ ìƒì„± ì‹œì ì— ì œê³µ)
            p_request = CrawlerYahooFinanceRequest(
                task_id=f"scheduled_{int(time.time())}",
                task_type="yahoo_finance_auto",
                symbols=[]  # ì „ì²´ ì‹¬ë³¼ ìˆ˜ì§‘
            )
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            await self.on_crawler_yahoo_finance_req(None, p_request)
            
        except Exception as e:
            Logger.error(f"ìŠ¤ì¼€ì¤„ëŸ¬ í¬ë¡¤ë§ ì‘ì—… ì‹¤íŒ¨: {e}")

    def _generate_hash_key(self, p_title: str) -> str:
        """ë‰´ìŠ¤ ì œëª©ìœ¼ë¡œ í•´ì‹œí‚¤ ìƒì„±"""
        try:
            if not p_title or p_title.strip() == '':
                return ''
            return hashlib.md5(p_title.encode('utf-8')).hexdigest()
        except Exception as e:
            Logger.error(f"í•´ì‹œí‚¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return ''

    def _save_hash_cache(self):
        """ì²˜ë¦¬ëœ í•´ì‹œí‚¤ë¥¼ ìºì‹œì— ì €ì¥"""
        try:
            cache_service = ServiceContainer.get_cache_service()
            if cache_service and cache_service.is_initialized() and self.v_processed_hashes:
                v_cache_key = "crawler:processed_hashes"
                v_hash_list = list(self.v_processed_hashes)
                
                # ë¹„ë™ê¸° ì €ì¥ ì‘ì—…
                async def save_hashes():
                    try:
                        async with cache_service.get_client() as client:
                            await client.set_string(v_cache_key, json.dumps(v_hash_list), 604800)  # 7ì¼ê°„ ë³´ê´€
                            Logger.info(f"ìºì‹œì— {len(v_hash_list)}ê°œ í•´ì‹œí‚¤ ì €ì¥")
                    except Exception as e:
                        Logger.error(f"ë¹„ë™ê¸° í•´ì‹œí‚¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                # ë¹„ë™ê¸° ì‘ì—… ìŠ¤ì¼€ì¤„ë§
                import asyncio
                try:
                    asyncio.create_task(save_hashes())
                except RuntimeError:
                    Logger.warn("ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ì–´ í•´ì‹œí‚¤ ìºì‹œ ì €ì¥ì„ ê±´ë„ˆëœ€")
                    
        except Exception as e:
            Logger.error(f"í•´ì‹œí‚¤ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def on_crawler_yahoo_finance_req(self, client_session, request: CrawlerYahooFinanceRequest):
        """Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘ ìš”ì²­ ì²˜ë¦¬"""
        response = CrawlerYahooFinanceResponse()
        response.sequence = request.sequence
        
        Logger.info(f"Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘ ìš”ì²­ ìˆ˜ì‹ : {request.task_id}")
        
        try:
            # 1. ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (Lock ì‚¬ìš©)
            v_lock_key = f"crawler_yahoo_finance_{request.task_id}"
            lock_service = ServiceContainer.get_lock_service()
            v_lock_token = None
            
            if lock_service:
                v_lock_token = await lock_service.acquire(v_lock_key, ttl=7200, timeout=5)
                if not v_lock_token:
                    response.errorCode = 5001  # Crawler ì—ëŸ¬ ì½”ë“œ ì‹œì‘
                    response.message = "ë‹¤ë¥¸ í¬ë¡¤ë§ ì‘ì—…ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤"
                    Logger.warn(f"Lock íšë“ ì‹¤íŒ¨: {v_lock_key}")
                    return response
            
            # 2. Yahoo Finance ë°ì´í„° ìˆ˜ì§‘
            v_collected_news = await self._collect_yahoo_finance_data(request)
            
            # 3. ì¤‘ë³µ ì œê±° ì²˜ë¦¬
            v_filtered_news = await self._process_duplicate_removal(v_collected_news)
            
            # 4. OpenSearchì— ì €ì¥
            v_opensearch_result = await self._store_to_opensearch(v_filtered_news, request.task_id)
            
            # 5. VectorDBì— ì €ì¥ (S3 ì—…ë¡œë“œ + Knowledge Base ë™ê¸°í™”)
            v_vectordb_result = await self._store_to_vectordb(v_filtered_news, request.task_id)
            
            # 6. ìºì‹œ ì—…ë°ì´íŠ¸
            self._save_hash_cache()
            
            # 7. ì„±ê³µ ì‘ë‹µ ì„¤ì •
            response.errorCode = 0
            response.task_id = request.task_id
            response.collected_count = len(v_collected_news)
            response.new_count = len(v_filtered_news)
            response.duplicate_count = len(v_collected_news) - len(v_filtered_news)
            response.opensearch_stored = v_opensearch_result.get('success', False)
            response.vectordb_stored = v_vectordb_result.get('success', False)
            response.message = "Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ"
            
            Logger.info(f"Yahoo Finance ìˆ˜ì§‘ ì™„ë£Œ: ì´ {response.collected_count}ê°œ, ì‹ ê·œ {response.new_count}ê°œ")
            
        except Exception as e:
            response.errorCode = 5000  # ì„œë²„ ì˜¤ë¥˜
            response.message = "ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            Logger.error(f"Yahoo Finance ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        finally:
            # Lock í•´ì œ
            if v_lock_token and lock_service:
                try:
                    await lock_service.release(v_lock_key, v_lock_token)
                except Exception as e:
                    Logger.error(f"Lock í•´ì œ ì‹¤íŒ¨: {e}")
        
        return response

    async def _collect_yahoo_finance_data(self, p_request: CrawlerYahooFinanceRequest) -> List[NewsData]:
        """Yahoo Financeì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
        Logger.info("Yahoo Finance ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        
        try:
            # ìˆ˜ì§‘í•  ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’ ë˜ëŠ” ìš”ì²­ì—ì„œ ì§€ì •)
            v_symbols = p_request.symbols if p_request.symbols else [# ëŒ€í˜• í…Œí¬ì£¼ (FAANG+)
         "AAPL", "MSFT", "GOOG", "GOOGL", "AMZN", "META", "TSLA", "NVDA", "NFLX", "ORCL", "CRM", "ADBE", "INTC", "AMD", "CSCO", "IBM",
    
    # ë°˜ë„ì²´
    "QCOM", "AVGO", "TXN", "MU", "AMAT", "LRCX", "ADI", "MCHP", "KLAC", "MRVL",
    
    # ì£¼ìš” ETFë“¤
    "SPY", "QQQ", "VTI", "IWM", "VEA", "VWO", "EFA", "GLD", "SLV", "TLT", "HYG", "LQD", "XLF", "XLK", "XLE", "XLV", "XLI", "XLP", "XLU", "XLRE",
    
    # ê¸ˆìœµ (ì€í–‰, ë³´í—˜, í•€í…Œí¬)
    "JPM", "BAC", "WFC", "C", "GS", "MS", "BRK-B", "V", "MA", "PYPL", "SQ", "AXP", "USB", "PNC", "TFC", "COF",
    
    # í—¬ìŠ¤ì¼€ì–´/ì œì•½
    "JNJ", "PFE", "UNH", "ABBV", "MRK", "TMO", "ABT", "DHR", "BMY", "AMGN", "GILD", "BIIB", "CVS", "CI", "ANTM", "HUM",
    
    # ì†Œë¹„ì¬ (í•„ìˆ˜/ì„ì˜)
    "PG", "KO", "PEP", "WMT", "HD", "MCD", "NKE", "SBUX", "TGT", "LOW", "COST", "DIS", "BABA", "AMZN", "EBAY", "ETSY",
    
    # ì—ë„ˆì§€
    "XOM", "CVX", "COP", "EOG", "SLB", "PSX", "VLO", "KMI", "OKE", "WMB",
    
    # ì‚°ì—…ì¬/í•­ê³µ
    "BA", "CAT", "DE", "GE", "HON", "MMM", "UPS", "FDX", "LMT", "RTX", "AAL", "DAL", "UAL", "LUV",
    
    # í†µì‹ 
    "VZ", "T", "TMUS", "CHTR", "CMCSA", "DISH",
    
    # ìë™ì°¨
    "F", "GM", "RIVN", "LCID", "NIO", "XPEV", "LI",
    
    # ë¶€ë™ì‚° REITs
    "AMT", "PLD", "CCI", "EQIX", "SPG", "O", "WELL", "EXR", "AVB", "EQR",
    
    # ìœ í‹¸ë¦¬í‹°
    "NEE", "DUK", "SO", "D", "AEP", "EXC", "XEL", "SRE", "PEG", "ED",
    
    # ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´
    "DIS", "NFLX", "ROKU", "SPOT", "WBD", "PARA", "FOX", "FOXA",
    
    # ì¤‘êµ­ ADR
    "BABA", "JD", "PDD", "BIDU", "BILI", "DIDI", "TME",
    
    # ì•”í˜¸í™”í ê´€ë ¨
    "COIN", "MSTR", "RIOT", "MARA", "BITB", "IBIT",
    
    # ê¸°íƒ€ ì£¼ìš” ê¸°ì—…ë“¤
    "TSLA", "UBER", "LYFT", "SNAP", "TWTR", "ZOOM", "DOCU", "PLTR", "SNOW", "CRM", "WORK", "ZM", "PTON", "ARKK", "ARKG", "ARKW"
    ]
            
            v_collected_news = []
            v_total_symbols = len(v_symbols)
            v_processed_symbols = 0
            
            for v_symbol in v_symbols:
                try:
                    v_processed_symbols += 1
                    Logger.info(f"[{v_processed_symbols}/{v_total_symbols}] {v_symbol} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
                    
                    # yfinanceë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰)
                    loop = asyncio.get_running_loop()
                    v_ticker = await loop.run_in_executor(None, yf.Ticker, v_symbol)
                    v_news_list = await loop.run_in_executor(None, lambda: v_ticker.news)
                    
                    if not v_news_list:
                        Logger.info(f"{v_symbol}: ë‰´ìŠ¤ ì—†ìŒ")
                        continue
                    
                    # ë‰´ìŠ¤ íŒŒì‹±
                    v_parsed_count = 0
                    for v_news_item in v_news_list:
                        try:
                            v_parsed_news = self._parse_yahoo_news_item(v_news_item, v_symbol)
                            if v_parsed_news:
                                v_collected_news.append(v_parsed_news)
                                v_parsed_count += 1
                        except Exception as e:
                            Logger.error(f"ë‰´ìŠ¤ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue
                    
                    Logger.info(f"{v_symbol}: {v_parsed_count}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
                    await asyncio.sleep(0.5)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                    
                except Exception as e:
                    Logger.error(f"{v_symbol} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                    continue
            
            Logger.info(f"ì´ {len(v_collected_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return v_collected_news
            
        except Exception as e:
            Logger.error(f"Yahoo Finance ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def _parse_yahoo_news_item(self, p_news_item: dict, p_symbol: str) -> NewsData:
        """Yahoo Finance ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹±"""
        try:
            if not p_news_item or not isinstance(p_news_item, dict):
                return None
            
            v_content = p_news_item.get('content', {})
            if not v_content or not isinstance(v_content, dict):
                return None
            
            # ì œëª© ì¶”ì¶œ
            v_title = v_content.get('title', '').strip()
            if not v_title or v_title == 'N/A':
                return None
            
            # ì¼ì£¼ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ í•„í„°ë§
            v_pub_date_str = v_content.get('pubDate', '')
            v_formatted_date = self._parse_and_filter_date(v_pub_date_str)
            if not v_formatted_date:
                return None  # ì˜¤ë˜ëœ ë‰´ìŠ¤ ì œì™¸
            
            # ê¸°íƒ€ ì •ë³´ ì¶”ì¶œ
            v_provider = v_content.get('provider', {})
            v_click_url = v_content.get('clickThroughUrl', {})
            
            # NewsData ê°ì²´ ìƒì„±
            v_news_data = NewsData(
                data_id=str(uuid.uuid4()),
                title=v_title,
                ticker=p_symbol,
                date=v_formatted_date,
                source=v_provider.get('displayName', 'Yahoo Finance') if isinstance(v_provider, dict) else 'Yahoo Finance',
                link=v_click_url.get('url', 'N/A') if isinstance(v_click_url, dict) else 'N/A',
                collected_at=datetime.now().isoformat()
            )
            
            return v_news_data
            
        except Exception as e:
            Logger.error(f"ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def _parse_and_filter_date(self, p_date_str: str) -> str:
        """ë‚ ì§œ íŒŒì‹± ë° ì¼ì£¼ì¼ ì´ë‚´ í•„í„°ë§"""
        try:
            if not p_date_str:
                return None
            
            # ë‚ ì§œ íŒŒì‹±
            v_pub_date = pd.to_datetime(p_date_str)
            if v_pub_date.tz is None:
                v_pub_date = v_pub_date.tz_localize('UTC')
            
            # ì¼ì£¼ì¼ ì´ë‚´ ì²´í¬
            v_now = datetime.now()
            if v_now.tzinfo is None:
                v_now = v_now.replace(tzinfo=v_pub_date.tzinfo)
            
            v_one_week_ago = v_now - timedelta(days=7)
            if v_pub_date < v_one_week_ago:
                return None  # ì˜¤ë˜ëœ ë‰´ìŠ¤
            
            return v_pub_date.strftime('%Y-%m-%d %H:%M')
            
        except Exception as e:
            Logger.error(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return datetime.now().strftime('%Y-%m-%d %H:%M')

    async def _process_duplicate_removal(self, p_news_list: List[NewsData]) -> List[NewsData]:
        """ë‰´ìŠ¤ ì¤‘ë³µ ì œê±° ì²˜ë¦¬"""
        Logger.info(f"ë‰´ìŠ¤ ì¤‘ë³µ ì œê±° ì²˜ë¦¬ ì‹œì‘: {len(p_news_list)}ê°œ")
        
        try:
            v_filtered_news = []
            v_new_count = 0
            v_duplicate_count = 0
            
            for v_news in p_news_list:
                # í•´ì‹œí‚¤ ìƒì„±
                v_hash_key = self._generate_hash_key(v_news.title)
                if not v_hash_key:
                    continue
                
                # ì¤‘ë³µ ì²´í¬
                if v_hash_key in self.v_processed_hashes:
                    v_duplicate_count += 1
                    Logger.debug(f"ì¤‘ë³µ ë‰´ìŠ¤ ì œì™¸: {v_news.title[:50]}...")
                else:
                    # ìƒˆ ë‰´ìŠ¤ ì¶”ê°€
                    self.v_processed_hashes.add(v_hash_key)
                    v_filtered_news.append(v_news)
                    v_new_count += 1
                    Logger.debug(f"ìƒˆ ë‰´ìŠ¤ ì¶”ê°€: {v_news.title[:50]}...")
            
            Logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: ìƒˆ ë‰´ìŠ¤ {v_new_count}ê°œ, ì¤‘ë³µ {v_duplicate_count}ê°œ")
            return v_filtered_news
            
        except Exception as e:
            Logger.error(f"ì¤‘ë³µ ì œê±° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

    async def _store_to_opensearch(self, p_news_list: List[NewsData], p_task_id: str) -> Dict[str, Any]:
        """OpenSearchì— ë‰´ìŠ¤ ë°ì´í„° ì €ì¥"""
        Logger.info(f"OpenSearchì— {len(p_news_list)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì‹œì‘")
        
        try:
            if not SearchService.is_initialized():
                Logger.warn("SearchServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return {'success': False, 'error': 'SearchService not initialized'}
            
            v_index_name = 'yahoo_finance_news'
            v_successful_count = 0
            v_failed_count = 0
            
            # ì¸ë±ìŠ¤ ìƒì„± (ì—†ëŠ” ê²½ìš°)
            await self._ensure_opensearch_index(v_index_name)
            
            # ê° ë‰´ìŠ¤ë¥¼ OpenSearchì— ì €ì¥
            for v_news in p_news_list:
                try:
                    v_doc = {
                        'task_id': p_task_id,
                        'ticker': v_news.ticker,
                        'title': v_news.title,
                        'source': v_news.source,
                        'date': v_news.date,
                        'link': v_news.link,
                        'collected_at': v_news.collected_at,
                        'created_at': datetime.now().isoformat(),
                        'content_type': 'yahoo_finance_news'
                    }
                    
                    v_result = await SearchService.index_document(
                        index=v_index_name,
                        document=v_doc,
                        doc_id=v_news.data_id
                    )
                    
                    if v_result.get('success', False):
                        v_successful_count += 1
                    else:
                        v_failed_count += 1
                        Logger.warn(f"OpenSearch ì €ì¥ ì‹¤íŒ¨: {v_result.get('error', 'Unknown')}")
                        
                except Exception as e:
                    v_failed_count += 1
                    Logger.error(f"OpenSearch ê°œë³„ ì €ì¥ ì˜¤ë¥˜: {e}")
            
            Logger.info(f"OpenSearch ì €ì¥ ì™„ë£Œ: ì„±ê³µ {v_successful_count}ê°œ, ì‹¤íŒ¨ {v_failed_count}ê°œ")
            return {
                'success': v_successful_count > 0,
                'successful_count': v_successful_count,
                'failed_count': v_failed_count
            }
            
        except Exception as e:
            Logger.error(f"OpenSearch ì €ì¥ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def _ensure_opensearch_index(self, p_index_name: str):
        """OpenSearch ì¸ë±ìŠ¤ ìƒì„± í™•ì¸"""
        try:
            v_index_check = await SearchService.index_exists(p_index_name)
            if not v_index_check.get('exists', False):
                Logger.info(f"OpenSearch ì¸ë±ìŠ¤ ìƒì„±: {p_index_name}")
                
                v_mappings = {
                    "properties": {
                        "task_id": {"type": "keyword"},
                        "ticker": {"type": "keyword"},
                        "title": {
                            "type": "text",
                            "analyzer": "standard",
                            "fields": {"keyword": {"type": "keyword", "ignore_above": 256}}
                        },
                        "source": {"type": "keyword"},
                        "date": {"type": "date", "format": "yyyy-MM-dd HH:mm||yyyy-MM-dd||epoch_millis"},
                        "link": {"type": "keyword", "index": False},
                        "collected_at": {"type": "date"},
                        "created_at": {"type": "date"},
                        "content_type": {"type": "keyword"}
                    }
                }
                
                await SearchService.create_index(index=p_index_name, mappings=v_mappings)
                
        except Exception as e:
            Logger.error(f"OpenSearch ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")

    async def _store_to_vectordb(self, p_news_list: List[NewsData], p_task_id: str) -> Dict[str, Any]:
        """VectorDBì— ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ (S3 ì—…ë¡œë“œ + Knowledge Base ë™ê¸°í™”)"""
        Logger.info(f"VectorDB(Knowledge Base)ì— {len(p_news_list)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì‹œì‘")
        
        try:
            # 1. VectorDB ì„œë¹„ìŠ¤ ì´ˆê¸°í™” í™•ì¸
            if not VectorDbService.is_initialized():
                Logger.warn("VectorDbServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return {'success': False, 'error': 'VectorDbService not initialized'}
            
            # 2. Storage ì„œë¹„ìŠ¤ ì´ˆê¸°í™” í™•ì¸
            if not StorageService.is_initialized():
                Logger.warn("StorageServiceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
                return {'success': False, 'error': 'StorageService not initialized'}
            
            # 3. Knowledge Base ìƒíƒœ í™•ì¸
            v_kb_status = await VectorDbService.get_knowledge_base_status()
            if not v_kb_status.get('success', False):
                Logger.error(f"Knowledge Base ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {v_kb_status.get('error', 'Unknown')}")
                return {'success': False, 'error': f"Knowledge Base unavailable: {v_kb_status.get('error', 'Unknown')}"}
            
            Logger.info(f"Knowledge Base ìƒíƒœ: {v_kb_status.get('status', 'Unknown')}")
            
            # 4. ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            v_json_documents = []
            for v_news in p_news_list:
                try:
                    # Knowledge Base ì¹œí™”ì ì¸ í…ìŠ¤íŠ¸ êµ¬ì„±
                    v_content_text = f"""
Title: {v_news.title}
Ticker: {v_news.ticker}
Source: {v_news.source}
Date: {v_news.date}
URL: {v_news.link}
Content Type: Financial News
Task ID: {p_task_id}
Collected At: {v_news.collected_at}

This is a financial news article about {v_news.ticker} from {v_news.source}. 
The news title is: {v_news.title}
Published on: {v_news.date}
"""
                    
                    v_json_doc = {
                        "id": v_news.data_id,
                        "title": v_news.title,
                        "content": v_news.title,
                        "metadata": {
                            "ticker": v_news.ticker,
                            "source": v_news.source,
                            "date": v_news.date,
                            "url": v_news.link,
                            "task_id": p_task_id,
                            "content_type": "yahoo_finance_news",
                            "collected_at": v_news.collected_at
                        }
                    }
                    v_json_documents.append(v_json_doc)
                    
                except Exception as e:
                    Logger.error(f"ë‰´ìŠ¤ JSON ë³€í™˜ ì˜¤ë¥˜: {e}")
                    continue
            
            if not v_json_documents:
                return {'success': False, 'error': 'No valid documents to upload'}
            
            # 5. S3ì— JSON íŒŒì¼ ì—…ë¡œë“œ
            v_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # S3 ì—…ë¡œë“œ (ì„¤ì •ì—ì„œ ë²„í‚· ì´ë¦„ ê°€ì ¸ì˜¤ê¸°)
            try:
                # VectorDB ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                v_vectordb_config = VectorDbService._config
                if not v_vectordb_config:
                    raise ValueError("VectorDB config not available")
                
                v_s3_bucket = getattr(v_vectordb_config, 's3_bucket', None)
                if not v_s3_bucket:
                    raise ValueError("S3 bucket not configured in VectorDB settings")
                
                v_s3_prefix = getattr(v_vectordb_config, 's3_prefix', 'knowledge-base-data/')
                v_s3_key = f"{v_s3_prefix}yahoo_finance_news/{p_task_id}_{v_timestamp}.json"
                
            except Exception as e:
                Logger.error(f"VectorDB ì„¤ì • ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                return {'success': False, 'error': f"VectorDB configuration error: {e}"}
            
            # ì„ì‹œ íŒŒì¼ì— JSON ë°ì´í„° ì €ì¥
            import tempfile
            import json
            import os
            
            v_temp_file = None
            try:
                # JSON ë°ì´í„° ê°ì²´ ìƒì„± (AWS Bedrock Knowledge Baseìš© ì§ì ‘ ë°°ì—´)
                v_upload_data = v_json_documents
                
                # ì„ì‹œ íŒŒì¼ì— ì €ì¥
                with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
                    json.dump(v_upload_data, f, ensure_ascii=False, indent=2)
                    v_temp_file = f.name
                
                # S3 ì—…ë¡œë“œ (ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ë²„í‚·ëª… ì‚¬ìš©)
                v_upload_result = await StorageService.upload_file(
                    bucket=v_s3_bucket,
                    key=v_s3_key,
                    file_path=v_temp_file,
                    extra_args={
                        'ContentType': 'application/json',
                        'Metadata': {
                            'task_id': p_task_id,
                            'content_type': 'yahoo_finance_news',
                            'document_count': str(len(v_json_documents))
                        }
                    }
                )
                
                if not v_upload_result.get('success', False):
                    Logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {v_upload_result.get('error', 'Unknown')}")
                    return {'success': False, 'error': f"S3 upload failed: {v_upload_result.get('error', 'Unknown')}"}
                
                Logger.info(f"S3 ì—…ë¡œë“œ ì„±ê³µ: s3://{v_s3_bucket}/{v_s3_key}")
                
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                if v_temp_file and os.path.exists(v_temp_file):
                    try:
                        os.unlink(v_temp_file)
                    except Exception as e:
                        Logger.warn(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            
            # 6. Knowledge Base ë™ê¸°í™” ì‹œì‘
            try:
                v_data_source_id = getattr(v_vectordb_config, 'data_source_id', None)
                if not v_data_source_id:
                    Logger.warn("Data source ID not configured - skipping Knowledge Base sync")
                    return {
                        'success': True,  # S3 ì—…ë¡œë“œëŠ” ì„±ê³µ
                        'uploaded_documents': len(v_json_documents),
                        's3_bucket': v_s3_bucket,
                        's3_key': v_s3_key,
                        'ingestion_job_id': None,
                        'ingestion_error': 'Data source ID not configured',
                        'message': f'Uploaded {len(v_json_documents)} documents to S3 but skipped Knowledge Base sync (data_source_id not configured)'
                    }
                
            except Exception as e:
                Logger.error(f"ë°ì´í„° ì†ŒìŠ¤ ID ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                return {'success': False, 'error': f"Data source configuration error: {e}"}
            
            # ğŸ” Knowledge Base ë™ê¸°í™” ì‹œì‘ (ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€)
            Logger.info(f"ğŸš€ Knowledge Base ë™ê¸°í™” ì‹œì‘ ì¤€ë¹„: data_source_id = {v_data_source_id}")
            Logger.info(f"ğŸ“Š VectorDbService ì´ˆê¸°í™” ìƒíƒœ: {VectorDbService.is_initialized()}")
            
            try:
                Logger.info("ğŸ”„ VectorDbService.start_ingestion_job í˜¸ì¶œ ì‹œì‘...")
                v_ingestion_result = await VectorDbService.start_ingestion_job(v_data_source_id)
                Logger.info(f"âœ… VectorDbService.start_ingestion_job í˜¸ì¶œ ì™„ë£Œ: {v_ingestion_result}")
                
            except Exception as ingestion_error:
                Logger.error(f"âŒ start_ingestion_job í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {ingestion_error}")
                Logger.error(f"ğŸ“‹ ì—ëŸ¬ íƒ€ì…: {type(ingestion_error).__name__}")
                import traceback
                Logger.error(f"ğŸ” ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
                
                # S3 ì—…ë¡œë“œëŠ” ì„±ê³µí–ˆì§€ë§Œ ingestion job ì‹¤íŒ¨
                return {
                    'success': True,  # S3 ì—…ë¡œë“œëŠ” ì„±ê³µ
                    'uploaded_documents': len(v_json_documents),
                    's3_bucket': v_s3_bucket,
                    's3_key': v_s3_key,
                    'ingestion_job_id': None,
                    'ingestion_error': f"Ingestion job failed: {str(ingestion_error)}",
                    'message': f'Uploaded {len(v_json_documents)} documents to S3 but failed to start Knowledge Base sync due to error'
                }
            
            if v_ingestion_result.get('success', False):
                v_job_id = v_ingestion_result.get('job_id', '')
                Logger.info(f"Knowledge Base ë™ê¸°í™” ì‹œì‘: Job ID = {v_job_id}")
                
                return {
                    'success': True,
                    'uploaded_documents': len(v_json_documents),
                    's3_bucket': v_s3_bucket,
                    's3_key': v_s3_key,
                    'ingestion_job_id': v_job_id,
                    'ingestion_status': v_ingestion_result.get('status', 'STARTING'),
                    'message': f'Successfully uploaded {len(v_json_documents)} documents to S3 and started Knowledge Base sync'
                }
            else:
                # S3 ì—…ë¡œë“œëŠ” ì„±ê³µí–ˆì§€ë§Œ ë™ê¸°í™” ì‹¤íŒ¨
                Logger.warn(f"Knowledge Base ë™ê¸°í™” ì‹œì‘ ì‹¤íŒ¨: {v_ingestion_result.get('error', 'Unknown')}")
                return {
                    'success': True,  # S3 ì—…ë¡œë“œëŠ” ì„±ê³µ
                    'uploaded_documents': len(v_json_documents),
                    's3_bucket': v_s3_bucket,
                    's3_key': v_s3_key,
                    'ingestion_job_id': None,
                    'ingestion_error': v_ingestion_result.get('error', 'Unknown'),
                    'message': f'Uploaded {len(v_json_documents)} documents to S3 but failed to start Knowledge Base sync'
                }
            
        except Exception as e:
            Logger.error(f"VectorDB ì €ì¥ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def check_vectordb_connection(self) -> Dict[str, Any]:
        """VectorDB(Knowledge Base) ì—°ê²° ë° ì„¤ì • ìƒíƒœ í™•ì¸"""
        Logger.info("VectorDB ì—°ê²° ìƒíƒœ í™•ì¸ ì‹œì‘")
        
        try:
            # 1. VectorDB ì„œë¹„ìŠ¤ ì´ˆê¸°í™” í™•ì¸
            if not VectorDbService.is_initialized():
                return {
                    'success': False,
                    'error': 'VectorDbService not initialized',
                    'details': {
                        'vectordb_service': False,
                        'storage_service': StorageService.is_initialized() if 'StorageService' in globals() else False
                    }
                }
            
            # 2. ì„¤ì • í™•ì¸
            v_vectordb_config = VectorDbService._config
            v_config_check = {
                'config_available': v_vectordb_config is not None,
                'knowledge_base_id': getattr(v_vectordb_config, 'knowledge_base_id', None),
                'data_source_id': getattr(v_vectordb_config, 'data_source_id', None),
                's3_bucket': getattr(v_vectordb_config, 's3_bucket', None),
                's3_prefix': getattr(v_vectordb_config, 's3_prefix', 'knowledge-base-data/'),
                'region_name': getattr(v_vectordb_config, 'region_name', 'unknown'),
                'embedding_model': getattr(v_vectordb_config, 'embedding_model', 'unknown')
            }
            
            # 3. Knowledge Base ìƒíƒœ í™•ì¸
            v_kb_status = await VectorDbService.get_knowledge_base_status()
            
            # 4. Storage Service í™•ì¸
            v_storage_available = StorageService.is_initialized()
            
            return {
                'success': True,
                'services': {
                    'vectordb_service': True,
                    'storage_service': v_storage_available,
                    'knowledge_base_accessible': v_kb_status.get('success', False)
                },
                'configuration': v_config_check,
                'knowledge_base': {
                    'status': v_kb_status.get('status', 'unknown'),
                    'name': v_kb_status.get('name', 'unknown'),
                    'error': v_kb_status.get('error') if not v_kb_status.get('success') else None
                },
                'ready_for_upload': all([
                    v_vectordb_config is not None,
                    v_config_check['s3_bucket'] is not None,
                    v_storage_available,
                    v_kb_status.get('success', False)
                ]),
                'ready_for_sync': all([
                    v_config_check['data_source_id'] is not None,
                    v_kb_status.get('success', False)
                ])
            }
            
        except Exception as e:
            Logger.error(f"VectorDB ì—°ê²° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'services': {
                    'vectordb_service': VectorDbService.is_initialized(),
                    'storage_service': StorageService.is_initialized() if 'StorageService' in globals() else False
                }
            }

    async def on_crawler_execute_req(self, client_session, request: CrawlerExecuteRequest):
        """í¬ë¡¤ëŸ¬ ì‘ì—… ì‹¤í–‰ ìš”ì²­ ì²˜ë¦¬"""
        response = CrawlerExecuteResponse()
        response.sequence = request.sequence
        
        Logger.info(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ìš”ì²­ ìˆ˜ì‹ : {request.task_type}")
        
        try:
            # Yahoo Finance í¬ë¡¤ë§ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
            if request.task_type == "yahoo_finance":
                v_yahoo_request = CrawlerYahooFinanceRequest()
                v_yahoo_request.task_id = request.task_id
                v_yahoo_request.task_type = request.task_type
                v_yahoo_request.symbols = request.parameters.get('symbols', []) if request.parameters else []
                
                v_yahoo_response = await self.on_crawler_yahoo_finance_req(client_session, v_yahoo_request)
                
                response.errorCode = v_yahoo_response.errorCode
                response.task_id = v_yahoo_response.task_id
                response.status = "completed" if v_yahoo_response.errorCode == 0 else "failed"
                response.message = v_yahoo_response.message
                response.started_at = datetime.now().isoformat()
                response.lock_acquired = True
            else:
                response.errorCode = 5002
                response.message = f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—… íƒ€ì…: {request.task_type}"
                
        except Exception as e:
            response.errorCode = 5000
            response.message = "í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            Logger.error(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        return response

    async def on_crawler_status_req(self, client_session, request: CrawlerStatusRequest):
        """í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ìš”ì²­ ì²˜ë¦¬"""
        response = CrawlerStatusResponse()
        response.sequence = request.sequence
        
        Logger.info("í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ìš”ì²­ ìˆ˜ì‹ ")
        
        try:
            v_tasks = []
            
            # í™œì„± ì‘ì—… ì •ë³´ êµ¬ì„±
            for v_task_id, v_task in self.v_active_tasks.items():
                if request.task_id and v_task_id != request.task_id:
                    continue
                if request.status and v_task.status != request.status:
                    continue
                
                v_tasks.append({
                    "task_id": v_task_id,
                    "task_type": v_task.task_type,
                    "status": v_task.status,
                    "started_at": v_task.started_at,
                    "completed_at": getattr(v_task, 'completed_at', None)
                })
            
            # ì œí•œ ì ìš©
            if request.limit and request.limit > 0:
                v_tasks = v_tasks[:request.limit]
            
            response.errorCode = 0
            response.tasks = v_tasks
            response.total_count = len(v_tasks)
            response.scheduler_active = self.v_scheduler_task_id is not None
            response.processed_hashes_count = len(self.v_processed_hashes)
            
            Logger.info(f"í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ì™„ë£Œ: {len(v_tasks)}ê°œ ì‘ì—…")
            
        except Exception as e:
            response.errorCode = 5000
            response.message = "ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            Logger.error(f"í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        
        return response

    async def on_crawler_health_req(self, client_session, request: CrawlerHealthRequest):
        """í¬ë¡¤ëŸ¬ í—¬ìŠ¤ì²´í¬ ìš”ì²­ ì²˜ë¦¬"""
        response = CrawlerHealthResponse()
        response.sequence = request.sequence
        
        Logger.info("í¬ë¡¤ëŸ¬ í—¬ìŠ¤ì²´í¬ ìš”ì²­ ìˆ˜ì‹ ")
        
        try:
            response.errorCode = 0
            response.status = "healthy"
            response.timestamp = datetime.now().isoformat()
            response.active_tasks = len([t for t in self.v_active_tasks.values() if t.status == "running"])
            response.processed_hashes_count = len(self.v_processed_hashes)
            response.scheduler_active = self.v_scheduler_task_id is not None
            
            # ì„œë¹„ìŠ¤ ìƒíƒœ ì²´í¬
            if request.check_services:
                v_services = {}
                v_services["cache_service"] = ServiceContainer.get_cache_service() is not None
                v_services["search_service"] = SearchService.is_initialized()
                v_services["vectordb_service"] = VectorDbService.is_initialized()
                v_services["scheduler_service"] = SchedulerService.is_initialized()
                response.services = v_services
            
            Logger.info(f"í¬ë¡¤ëŸ¬ í—¬ìŠ¤ì²´í¬ ì™„ë£Œ: {response.status}")
            
        except Exception as e:
            response.errorCode = 5000
            response.status = "unhealthy"
            response.message = "í—¬ìŠ¤ì²´í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            Logger.error(f"í¬ë¡¤ëŸ¬ í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {e}")
        
        return response

    async def on_crawler_data_req(self, client_session, request: CrawlerDataRequest):
        """í¬ë¡¤ëŸ¬ ë°ì´í„° ì¡°íšŒ ìš”ì²­ ì²˜ë¦¬"""
        response = CrawlerDataResponse()
        response.sequence = request.sequence
        
        Logger.info("í¬ë¡¤ëŸ¬ ë°ì´í„° ì¡°íšŒ ìš”ì²­ ìˆ˜ì‹ ")
        
        try:
            v_filtered_news = []
            
            # ìºì‹œì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ
            for v_news in self.v_news_cache:
                if request.task_id and hasattr(v_news, 'task_id') and v_news.task_id != request.task_id:
                    continue
                v_filtered_news.append(v_news)
            
            # ì œí•œ ì ìš©
            if request.limit and request.limit > 0:
                v_limited_news = v_filtered_news[:request.limit]
            else:
                v_limited_news = v_filtered_news
            
            # ì‘ë‹µ ë°ì´í„° êµ¬ì„±
            v_response_data = []
            for i, v_news in enumerate(v_limited_news):
                v_news_item = {
                    "news_index": i + 1,
                    "title": getattr(v_news, 'title', 'N/A'),
                    "ticker": getattr(v_news, 'ticker', 'N/A'),
                    "date": getattr(v_news, 'date', 'N/A'),
                    "source": getattr(v_news, 'source', 'N/A'),
                    "link": getattr(v_news, 'link', 'N/A'),
                    "collected_at": getattr(v_news, 'collected_at', 'N/A')
                }
                v_response_data.append(v_news_item)
            
            response.errorCode = 0
            response.data = v_response_data
            response.total_count = len(v_filtered_news)
            
            Logger.info(f"í¬ë¡¤ëŸ¬ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(v_limited_news)}ê°œ ë‰´ìŠ¤ ë°˜í™˜")
            
        except Exception as e:
            response.errorCode = 5000
            response.message = "ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            Logger.error(f"í¬ë¡¤ëŸ¬ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        
        return response

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ì€ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ë˜ deprecated í‘œì‹œ
    async def on_crawler_stop_req(self, client_session, request: CrawlerStopRequest):
        """í¬ë¡¤ëŸ¬ ì‘ì—… ì¤‘ë‹¨ ìš”ì²­ ì²˜ë¦¬ (Deprecated)"""
        response = CrawlerStopResponse()
        response.sequence = request.sequence
        response.task_id = request.task_id
        response.stopped = True
        response.message = "í¬ë¡¤ëŸ¬ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ ê°œë³„ ì¤‘ë‹¨ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤"
        return response

    # ============================================================================
    # ì €ì¥ í™•ì¸ ë©”ì„œë“œë“¤ (OpenSearch & VectorDB ì €ì¥ ê²€ì¦)
    # ============================================================================

    async def verify_opensearch_storage(self, p_task_id: str = None, p_limit: int = 10) -> Dict[str, Any]:
        """OpenSearch ì €ì¥ í™•ì¸"""
        Logger.info("OpenSearch ì €ì¥ í™•ì¸ ì‹œì‘")
        
        try:
            if not SearchService.is_initialized():
                return {'success': False, 'error': 'SearchService not available'}
            
            v_index_name = 'yahoo_finance_news'
            
            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
            v_index_check = await SearchService.index_exists(v_index_name)
            if not v_index_check.get('exists', False):
                return {'success': False, 'error': f'Index {v_index_name} does not exist'}
            
            # ë¬¸ì„œ ìˆ˜ ì¡°íšŒ
            v_count_query = {"query": {"match_all": {}}}
            if p_task_id:
                v_count_query = {"query": {"term": {"task_id": p_task_id}}}
            
            v_count_result = await SearchService.count_documents(v_index_name, v_count_query)
            v_total_count = v_count_result.get('count', 0) if v_count_result.get('success') else 0
            
            # ìµœê·¼ ë¬¸ì„œ ì¡°íšŒ
            v_search_query = {
                "query": v_count_query["query"],
                "sort": [{"created_at": {"order": "desc"}}],
                "size": p_limit
            }
            
            v_search_result = await SearchService.search_documents(v_index_name, v_search_query)
            v_documents = v_search_result.get('documents', []) if v_search_result.get('success') else []
            
            Logger.info(f"OpenSearch í™•ì¸ ì™„ë£Œ: ì´ {v_total_count}ê°œ ë¬¸ì„œ, ìµœê·¼ {len(v_documents)}ê°œ ì¡°íšŒ")
            
            return {
                'success': True,
                'index_name': v_index_name,
                'total_count': v_total_count,
                'recent_documents': v_documents,
                'sample_titles': [doc.get('_source', {}).get('title', 'N/A') for doc in v_documents[:5]]
            }
            
        except Exception as e:
            Logger.error(f"OpenSearch ì €ì¥ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def verify_vectordb_storage(self, p_task_id: str = None, p_limit: int = 10) -> Dict[str, Any]:
        """VectorDB(Knowledge Base) ì €ì¥ í™•ì¸"""
        Logger.info("VectorDB(Knowledge Base) ì €ì¥ í™•ì¸ ì‹œì‘")
        
        try:
            if not VectorDbService.is_initialized():
                return {'success': False, 'error': 'VectorDbService not available'}
            
            # 1. Knowledge Base ìƒíƒœ í™•ì¸
            v_kb_status = await VectorDbService.get_knowledge_base_status()
            if not v_kb_status.get('success', False):
                return {
                    'success': False, 
                    'error': 'Knowledge Base not available',
                    'kb_error': v_kb_status.get('error', 'Unknown')
                }
            
            # 2. Knowledge Baseì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰
            v_search_queries = [
                "financial news",
                "yahoo finance news", 
                "stock market news"
            ]
            
            v_all_documents = []
            v_search_results = {}
            
            for v_query in v_search_queries:
                try:
                    v_search_result = await VectorDbService.similarity_search(
                        query=v_query,
                        top_k=p_limit
                    )
                    
                    if v_search_result.get('success'):
                        v_docs = v_search_result.get('results', [])
                        v_search_results[v_query] = len(v_docs)
                        v_all_documents.extend(v_docs)
                    else:
                        v_search_results[v_query] = 0
                        Logger.warn(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({v_query}): {v_search_result.get('error', 'Unknown')}")
                        
                except Exception as e:
                    v_search_results[v_query] = 0
                    Logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜ ({v_query}): {e}")
            
            # 3. ì¤‘ë³µ ì œê±° ë° í†µê³„ ìˆ˜ì§‘
            v_unique_docs = {}
            for v_doc in v_all_documents:
                v_content = v_doc.get('content', '')
                if v_content and len(v_content) > 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    v_doc_hash = hashlib.md5(v_content.encode()).hexdigest()
                    if v_doc_hash not in v_unique_docs:
                        v_unique_docs[v_doc_hash] = v_doc
            
            v_unique_documents = list(v_unique_docs.values())
            
            # 4. ìƒ˜í”Œ ì œëª© ì¶”ì¶œ
            v_sample_titles = []
            v_ticker_counts = {}
            
            for v_doc in v_unique_documents[:10]:  # ìƒìœ„ 10ê°œë§Œ
                v_content = v_doc.get('content', '')
                
                # ì œëª© ì¶”ì¶œ ì‹œë„
                v_title = 'N/A'
                if 'Title:' in v_content:
                    v_lines = v_content.split('\n')
                    for v_line in v_lines:
                        if v_line.strip().startswith('Title:'):
                            v_title = v_line.replace('Title:', '').strip()
                            break
                
                v_sample_titles.append(v_title)
                
                # í‹°ì»¤ ì¹´ìš´íŠ¸
                if 'Ticker:' in v_content:
                    v_lines = v_content.split('\n')
                    for v_line in v_lines:
                        if v_line.strip().startswith('Ticker:'):
                            v_ticker = v_line.replace('Ticker:', '').strip()
                            v_ticker_counts[v_ticker] = v_ticker_counts.get(v_ticker, 0) + 1
                            break
            
            Logger.info(f"VectorDB í™•ì¸ ì™„ë£Œ: {len(v_unique_documents)}ê°œ ê³ ìœ  ë¬¸ì„œ ë°œê²¬")
            
            return {
                'success': True,
                'knowledge_base_id': v_kb_status.get('knowledge_base_id', 'Unknown'),
                'knowledge_base_status': v_kb_status.get('status', 'Unknown'),
                'knowledge_base_name': v_kb_status.get('name', 'Unknown'),
                'total_documents_found': len(v_unique_documents),
                'search_results_by_query': v_search_results,
                'sample_titles': v_sample_titles[:5],
                'top_tickers': dict(sorted(v_ticker_counts.items(), key=lambda x: x[1], reverse=True)[:5]),
                'kb_created_at': v_kb_status.get('created_at'),
                'kb_updated_at': v_kb_status.get('updated_at'),
                'operation_time': v_kb_status.get('operation_time', 0)
            }
            
        except Exception as e:
            Logger.error(f"VectorDB ì €ì¥ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    async def verify_storage_health(self, p_task_id: str = None) -> Dict[str, Any]:
        """ì „ì²´ ì €ì¥ì†Œ ìƒíƒœ í™•ì¸ (OpenSearch + VectorDB)"""
        Logger.info("ì „ì²´ ì €ì¥ì†Œ ìƒíƒœ í™•ì¸ ì‹œì‘")
        
        try:
            # OpenSearch í™•ì¸
            v_opensearch_result = await self.verify_opensearch_storage(p_task_id, 5)
            
            # VectorDB í™•ì¸
            v_vectordb_result = await self.verify_vectordb_storage(p_task_id, 5)
            
            # ì „ì²´ ìƒíƒœ íŒì •
            v_overall_health = "healthy"
            if not v_opensearch_result.get('success') or not v_vectordb_result.get('success'):
                v_overall_health = "degraded"
            
            v_result = {
                'success': True,
                'overall_health': v_overall_health,
                'timestamp': datetime.now().isoformat(),
                'opensearch': v_opensearch_result,
                'vectordb': v_vectordb_result,
                'recommendations': []
            }
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            if not v_opensearch_result.get('success'):
                v_result['recommendations'].append("OpenSearch ì—°ê²° ë° ì¸ë±ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”")
            
            if not v_vectordb_result.get('success'):
                v_result['recommendations'].append("VectorDB Knowledge Base ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”")
            
            if v_opensearch_result.get('total_count', 0) == 0:
                v_result['recommendations'].append("OpenSearchì— ì €ì¥ëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”")
            
            if v_vectordb_result.get('documents_count', 0) == 0:
                v_result['recommendations'].append("VectorDBì— ì €ì¥ëœ ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ì„¸ìš”")
            
            Logger.info(f"ì „ì²´ ì €ì¥ì†Œ ìƒíƒœ í™•ì¸ ì™„ë£Œ: {v_overall_health}")
            return v_result
            
        except Exception as e:
            Logger.error(f"ì „ì²´ ì €ì¥ì†Œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'overall_health': 'unhealthy',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }