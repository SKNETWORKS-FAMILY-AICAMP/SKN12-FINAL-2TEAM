"""
LSTM ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ë°ì´í„° ìˆ˜ì§‘, ì „ì²˜ë¦¬, ëª¨ë¸ í•™ìŠµì„ í†µí•©í•˜ì—¬ ì‹¤í–‰
"""

import os
import sys
import logging
import argparse
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from data_collector import StockDataCollector
from data_preprocessor import StockDataPreprocessor
from pytorch_lstm_model import PyTorchStockLSTM
from config import get_model_save_dir, get_workspace_path, is_runpod_environment

class ModelTrainer:
    def __init__(self, 
                 data_dir: str = None,
                 model_dir: str = None,
                 log_dir: str = None):
        """
        ëª¨ë¸ í•™ìŠµê¸° ì´ˆê¸°í™”
        
        Args:
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì‹œ í™˜ê²½ì— ë”°ë¼ ìë™ ì„¤ì •)
            model_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì‹œ í™˜ê²½ì— ë”°ë¼ ìë™ ì„¤ì •)
            log_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì‹œ í™˜ê²½ì— ë”°ë¼ ìë™ ì„¤ì •)
        """
        # RunPod í™˜ê²½ ê³ ë ¤í•œ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
        if is_runpod_environment():
            workspace = get_workspace_path()
            self.data_dir = data_dir or f"{workspace}/SKN12-FINAL-2TEAM/base_server/template/model/data"
            self.model_dir = model_dir or f"{workspace}/SKN12-FINAL-2TEAM/base_server/template/model/models"
            self.log_dir = log_dir or f"{workspace}/SKN12-FINAL-2TEAM/base_server/template/model/logs"
        else:
            self.data_dir = data_dir or "data"
            self.model_dir = model_dir or "models"
            self.log_dir = log_dir or "logs"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for directory in [self.data_dir, self.model_dir, self.log_dir]:
            os.makedirs(directory, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.data_collector = StockDataCollector()
        self.preprocessor = StockDataPreprocessor()
        self.model = None
        
        self.logger = logging.getLogger(__name__)
    
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = os.path.join(self.log_dir, f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        # PyTorch ì‚¬ìš©ìœ¼ë¡œ TensorFlow ë¡œê¹… ì„¤ì • ì œê±°
        # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
        # logging.getLogger('tensorflow').setLevel(logging.WARNING)
    
    def collect_data(self, force_reload: bool = False) -> dict[str, pd.DataFrame]:
        """
        í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            force_reload: ê°•ì œë¡œ ë°ì´í„° ì¬ìˆ˜ì§‘ ì—¬ë¶€
            
        Returns:
            ìˆ˜ì§‘ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        data_file = os.path.join(self.data_dir, "collected_data.pkl")
        
        if os.path.exists(data_file) and not force_reload:
            self.logger.info("Loading existing data...")
            with open(data_file, 'rb') as f:
                data = pickle.load(f)
            self.logger.info(f"Loaded data for {len(data)} symbols")
            return data
        
        self.logger.info("Collecting new data...")
        data = self.data_collector.collect_top_100_data()
        
        # ë°ì´í„° ì €ì¥
        with open(data_file, 'wb') as f:
            pickle.dump(data, f)
        
        # CSVë¡œë„ ì €ì¥
        self.data_collector.save_data_to_csv(data, self.data_dir)
        
        self.logger.info(f"Data collection completed: {len(data)} symbols")
        return data
    
    def preprocess_data(self, raw_data: dict[str, pd.DataFrame]) -> tuple[np.ndarray, np.ndarray]:
        """
        ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ ìƒì„±
        
        Args:
            raw_data: ì›ë³¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            (X, y) - í•™ìŠµìš© ì‹œí€€ìŠ¤ ë°ì´í„°
        """
        self.logger.info("Starting data preprocessing...")
        
        all_X = []
        all_y = []
        
        for symbol, df in raw_data.items():
            try:
                # ìµœì†Œ ë°ì´í„° ê¸¸ì´ í™•ì¸ (60ì¼ + 5ì¼ ì˜ˆì¸¡ + ì—¬ìœ ë¶„)
                if len(df) < 100:
                    self.logger.warning(f"Insufficient data for {symbol}: {len(df)} records")
                    continue
                
                # ì „ì²˜ë¦¬
                processed_df = self.preprocessor.preprocess_data(df)
                
                # ì‹œí€€ìŠ¤ ìƒì„± (ì¢…ëª©ë³„ ê°œë³„ ì •ê·œí™”)
                X, y = self.preprocessor.create_sequences(processed_df, symbol)
                
                if len(X) > 0:
                    all_X.append(X)
                    all_y.append(y)
                    self.logger.info(f"Processed {symbol}: {X.shape[0]} sequences")
                
            except Exception as e:
                self.logger.error(f"Error processing {symbol}: {str(e)}")
                continue
        
        if not all_X:
            raise ValueError("No valid data sequences generated")
        
        # ëª¨ë“  ì‹œí€€ìŠ¤ ê²°í•©
        X_combined = np.vstack(all_X)
        y_combined = np.vstack(all_y)
        
        self.logger.info(f"Combined sequences - X: {X_combined.shape}, y: {y_combined.shape}")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ (ì¢…ëª©ë³„ ìŠ¤ì¼€ì¼ëŸ¬ í¬í•¨)
        processed_data_file = os.path.join(self.data_dir, "processed_sequences.pkl")
        with open(processed_data_file, 'wb') as f:
            pickle.dump({
                'X': X_combined,
                'y': y_combined,
                'scaler': self.preprocessor.scaler,  # ê¸°ì¡´ í˜¸í™˜ì„±
                'symbol_scalers': self.preprocessor.symbol_scalers,  # ì¢…ëª©ë³„ í”¼ì²˜ ìŠ¤ì¼€ì¼ëŸ¬
                'target_scalers': self.preprocessor.target_scalers   # ì¢…ëª©ë³„ íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ëŸ¬
            }, f)
        
        return X_combined, y_combined
    
    def prepare_training_data(self, X: np.ndarray, y: np.ndarray, 
                            test_size: float = 0.2, 
                            val_size: float = 0.1) -> tuple:
        """
        í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        
        Args:
            X: ì…ë ¥ ì‹œí€€ìŠ¤
            y: íƒ€ê²Ÿ ì‹œí€€ìŠ¤
            test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
            val_size: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            
        Returns:
            (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        self.logger.info("Splitting data into train/validation/test sets...")
        
        # ë¨¼ì € train+valê³¼ testë¡œ ë¶„í• 
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, shuffle=True
        )
        
        # trainê³¼ validationìœ¼ë¡œ ë¶„í• 
        val_ratio = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_ratio, random_state=42, shuffle=True
        )
        
        self.logger.info(f"Data split - Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def train_model(self, 
                   X_train: np.ndarray, 
                   y_train: np.ndarray,
                   X_val: np.ndarray, 
                   y_val: np.ndarray,
                   model_type: str = "lstm_attention",
                   epochs: int = 100,
                   batch_size: int = 128) -> dict:  # ğŸ”¥ RTX 4090 ìµœì í™”
        """
        ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
        
        Args:
            X_train, y_train: í•™ìŠµ ë°ì´í„°
            X_val, y_val: ê²€ì¦ ë°ì´í„°
            model_type: ëª¨ë¸ íƒ€ì…
            epochs: í•™ìŠµ ì—í¬í¬
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            í•™ìŠµ íˆìŠ¤í† ë¦¬
        """
        self.logger.info(f"Starting model training with {model_type}")
        
        # ğŸš€ PyTorch ëª¨ë¸ ì´ˆê¸°í™”
        self.model = PyTorchStockLSTM(
            sequence_length=X_train.shape[1],
            prediction_length=y_train.shape[1],
            num_features=X_train.shape[2],
            num_targets=y_train.shape[2]
        )
        
        # ğŸ”¥ RTX 4090 ìµœì í™” ëª¨ë¸ êµ¬ì¶•
        pytorch_model = self.model.build_model(hidden_size=512, num_layers=4, dropout=0.2)
        print(f"ğŸš€ RTX 4090 ìµœì í™” ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ!")
        
        # í•™ìŠµ ì‹¤í–‰ (PyTorch)
        history = self.model.train_model(
            X_train, y_train,
            X_val, y_val,
            epochs=epochs,
            batch_size=batch_size,
            patience=15
        )
        
        # ëª¨ë¸ ì €ì¥ (PyTorch)
        model_save_path = os.path.join(self.model_dir, "pytorch_model.pth")
        self.model.save_model(model_save_path)
        
        # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”
        plot_path = os.path.join(self.model_dir, "training_history.png")
        self.model.plot_training_history(plot_path)
        
        self.logger.info("Model training completed")
        return history
    
    def evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray) -> dict[str, float]:
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            X_test: í…ŒìŠ¤íŠ¸ ì…ë ¥
            y_test: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ
            
        Returns:
            í‰ê°€ ì§€í‘œ
        """
        if self.model is None:
            raise ValueError("Model not trained")
        
        self.logger.info("Evaluating model on test set...")
        metrics = self.model.evaluate(X_test, y_test)
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥
        metrics_file = os.path.join(self.model_dir, "evaluation_metrics.pkl")
        with open(metrics_file, 'wb') as f:
            pickle.dump(metrics, f)
        
        # í‰ê°€ ê²°ê³¼ ë¡œê·¸
        for metric_name, value in metrics.items():
            self.logger.info(f"{metric_name}: {value:.6f}")
        
        return metrics
    
    def save_training_artifacts(self):
        """í•™ìŠµ ê²°ê³¼ë¬¼ ì €ì¥"""
        if self.model is None:
            return
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥ (PyTorch)
        final_model_path = os.path.join(self.model_dir, "final_model.pth")
        self.model.save_model(final_model_path)
        
        # ì „ì²˜ë¦¬ê¸° ì €ì¥
        preprocessor_path = os.path.join(self.model_dir, "preprocessor.pkl")
        with open(preprocessor_path, 'wb') as f:
            pickle.dump(self.preprocessor, f)
        
        self.logger.info(f"Training artifacts saved to {self.model_dir}")
    
    def run_full_training_pipeline(self,
                                 force_reload_data: bool = False,
                                 model_type: str = "lstm_attention",
                                 epochs: int = 100,
                                 batch_size: int = 128):  # ğŸ”¥ RTX 4090 ìµœì í™”
        """
        ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            force_reload_data: ë°ì´í„° ê°•ì œ ì¬ìˆ˜ì§‘
            model_type: ëª¨ë¸ íƒ€ì…
            epochs: í•™ìŠµ ì—í¬í¬
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        try:
            self.logger.info("=" * 50)
            self.logger.info("STARTING FULL TRAINING PIPELINE")
            self.logger.info("=" * 50)
            
            # 1. ë°ì´í„° ìˆ˜ì§‘
            raw_data = self.collect_data(force_reload_data)
            
            # 2. ë°ì´í„° ì „ì²˜ë¦¬
            X, y = self.preprocess_data(raw_data)
            
            # 3. ë°ì´í„° ë¶„í• 
            X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_training_data(X, y)
            
            # 4. ëª¨ë¸ í•™ìŠµ
            history = self.train_model(X_train, y_train, X_val, y_val, 
                                     model_type, epochs, batch_size)
            
            # 5. ëª¨ë¸ í‰ê°€
            metrics = self.evaluate_model(X_test, y_test)
            
            # 6. ê²°ê³¼ë¬¼ ì €ì¥
            self.save_training_artifacts()
            
            self.logger.info("=" * 50)
            self.logger.info("TRAINING PIPELINE COMPLETED SUCCESSFULLY")
            self.logger.info("=" * 50)
            
            return {
                'history': history,
                'metrics': metrics,
                'model_path': os.path.join(self.model_dir, "final_model.pth")
            }
            
        except Exception as e:
            self.logger.error(f"Training pipeline failed: {str(e)}")
            raise


def main():
    parser = argparse.ArgumentParser(description="Stock LSTM Model Training")
    parser.add_argument("--data-dir", default=None, help="Data directory (auto-detect if not specified)")
    parser.add_argument("--model-dir", default=None, help="Model save directory (auto-detect if not specified)")
    parser.add_argument("--log-dir", default=None, help="Log directory (auto-detect if not specified)")
    parser.add_argument("--force-reload", action="store_true", help="Force reload data")
    parser.add_argument("--model-type", default="lstm_attention", 
                       choices=["lstm", "lstm_attention", "lstm_ensemble"],
                       help="Model type to train")
    parser.add_argument("--epochs", type=int, default=100, help="Training epochs")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size")
    
    args = parser.parse_args()
    
    # í•™ìŠµ ì‹¤í–‰
    trainer = ModelTrainer(args.data_dir, args.model_dir, args.log_dir)
    
    results = trainer.run_full_training_pipeline(
        force_reload_data=args.force_reload,
        model_type=args.model_type,
        epochs=args.epochs,
        batch_size=args.batch_size
    )
    
    print(f"\nTraining completed!")
    print(f"Model saved at: {results['model_path']}")
    print(f"Final test metrics: {results['metrics']}")


if __name__ == "__main__":
    main()