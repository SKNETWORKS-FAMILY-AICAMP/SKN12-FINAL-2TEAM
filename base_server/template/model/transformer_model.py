"""
Transformer ê¸°ë°˜ ì£¼ì‹ ì˜ˆì¸¡ ëª¨ë¸
ì‹œê³„ì—´ ë°ì´í„°ì— ìµœì í™”ëœ Transformer ì•„í‚¤í…ì²˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Optional, Tuple, Dict, List
import logging

class PositionalEncoding(nn.Module):
    """
    ì‹œê³„ì—´ ë°ì´í„°ìš© ìœ„ì¹˜ ì¸ì½”ë”©
    ì£¼ì‹ ë°ì´í„°ëŠ” ìˆœì„œê°€ ë§¤ìš° ì¤‘ìš”í•˜ë¯€ë¡œ ê°•í™”ëœ ìœ„ì¹˜ ì¸ì½”ë”© ì‚¬ìš©
    """
    def __init__(self, d_model: int, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (seq_len, batch_size, d_model)
        """
        return x + self.pe[:x.size(0), :]

class MultiHeadFinancialAttention(nn.Module):
    """
    ì£¼ì‹ ë°ì´í„°ì— íŠ¹í™”ëœ ë©€í‹°í—¤ë“œ ì–´í…ì…˜
    - ì‹œê°„ì  ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
    - ë³€ë™ì„± ê¸°ë°˜ ì–´í…ì…˜ ìŠ¤ì¼€ì¼ë§
    """
    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):
        super(MultiHeadFinancialAttention, self).__init__()
        
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
        # ğŸš€ ì£¼ì‹ íŠ¹í™”: ì‹œê°„ ê±°ë¦¬ë³„ ê°€ì¤‘ì¹˜
        self.time_decay = nn.Parameter(torch.tensor(0.1))
        
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            query, key, value: (batch_size, seq_len, d_model)
            mask: (batch_size, seq_len, seq_len)
        """
        batch_size, seq_len, _ = query.size()
        
        # Multi-head ë¶„í• 
        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # Scaled Dot-Product Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # ğŸš€ ì£¼ì‹ íŠ¹í™”: ì‹œê°„ ê±°ë¦¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
        time_matrix = self._create_time_decay_matrix(seq_len, query.device)
        scores = scores + time_matrix.unsqueeze(0).unsqueeze(0)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Attention ì ìš©
        context = torch.matmul(attention_weights, V)
        
        # Multi-head ê²°í•©
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        output = self.w_o(context)
        
        return output
    
    def _create_time_decay_matrix(self, seq_len: int, device: torch.device) -> torch.Tensor:
        """ì‹œê°„ ê±°ë¦¬ì— ë”°ë¥¸ ê°ì‡  í–‰ë ¬ ìƒì„±"""
        positions = torch.arange(seq_len, device=device).float()
        time_diff = positions.unsqueeze(1) - positions.unsqueeze(0)
        
        # ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ (ìŒìˆ˜ëŠ” ë¯¸ë˜ ë°ì´í„°ì´ë¯€ë¡œ ë§ˆìŠ¤í‚¹)
        decay_matrix = -self.time_decay * torch.abs(time_diff)
        
        return decay_matrix

class TransformerEncoderLayer(nn.Module):
    """
    ì£¼ì‹ ì˜ˆì¸¡ìš© Transformer ì¸ì½”ë” ë ˆì´ì–´
    """
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super(TransformerEncoderLayer, self).__init__()
        
        self.self_attention = MultiHeadFinancialAttention(d_model, n_heads, dropout)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),  # GELUê°€ ê¸ˆìœµ ë°ì´í„°ì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Self-Attention with residual connection
        attn_output = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed Forward with residual connection  
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class StockTransformer(nn.Module):
    """
    ì£¼ì‹ ì˜ˆì¸¡ìš© Transformer ëª¨ë¸
    - ì‹œê³„ì—´ íŠ¹í™” ì•„í‚¤í…ì²˜
    - ë‹¤ì¤‘ ëª©í‘œ ì˜ˆì¸¡ (Close, BB_Upper, BB_Lower)
    """
    def __init__(self, 
                 input_size: int = 42,          # ê³ ê¸‰ í”¼ì²˜ ìˆ˜
                 d_model: int = 512,            # ëª¨ë¸ ì°¨ì›
                 n_heads: int = 8,              # ì–´í…ì…˜ í—¤ë“œ ìˆ˜
                 n_layers: int = 6,             # ì¸ì½”ë” ë ˆì´ì–´ ìˆ˜
                 d_ff: int = 2048,              # Feed Forward ì°¨ì›
                 max_seq_len: int = 60,         # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
                 prediction_length: int = 5,     # ì˜ˆì¸¡ ê¸¸ì´
                 num_targets: int = 3,          # ì˜ˆì¸¡ íƒ€ê²Ÿ ìˆ˜
                 dropout: float = 0.1):
        super(StockTransformer, self).__init__()
        
        self.input_size = input_size
        self.d_model = d_model
        self.prediction_length = prediction_length
        self.num_targets = num_targets
        
        # ì…ë ¥ ì„ë² ë”©
        self.input_embedding = nn.Linear(input_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # Transformer Encoder ìŠ¤íƒ
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])
        
        # ì¶œë ¥ í—¤ë“œë“¤
        self.output_projection = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, prediction_length * num_targets)
        )
        
        # ğŸš€ ì£¼ì‹ íŠ¹í™”: íƒ€ê²Ÿë³„ ì „ë¬¸í™” í—¤ë“œ
        self.close_head = nn.Linear(d_model, prediction_length)
        self.bb_upper_head = nn.Linear(d_model, prediction_length)  
        self.bb_lower_head = nn.Linear(d_model, prediction_length)
        
        self.dropout = nn.Dropout(dropout)
        
        # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _initialize_weights(self):
        """Xavier ì´ˆê¸°í™” ì ìš©"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Args:
            x: (batch_size, seq_len, input_size)
            mask: (batch_size, seq_len, seq_len)
        Returns:
            (batch_size, prediction_length, num_targets)
        """
        batch_size, seq_len, _ = x.size()
        
        # ì…ë ¥ ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©
        x = self.input_embedding(x) * math.sqrt(self.d_model)
        x = x.transpose(0, 1)  # (seq_len, batch_size, d_model)
        x = self.positional_encoding(x)
        x = x.transpose(0, 1)  # (batch_size, seq_len, d_model)
        
        x = self.dropout(x)
        
        # Transformer Encoder ë ˆì´ì–´ë“¤ í†µê³¼
        for encoder_layer in self.encoder_layers:
            x = encoder_layer(x, mask)
        
        # ğŸš€ Global Average Pooling (ì‹œê³„ì—´ì˜ ëª¨ë“  ì •ë³´ í™œìš©)
        pooled = x.mean(dim=1)  # (batch_size, d_model)
        
        # ğŸš€ íƒ€ê²Ÿë³„ ì „ë¬¸í™” ì˜ˆì¸¡
        close_pred = self.close_head(pooled)        # (batch_size, prediction_length)
        bb_upper_pred = self.bb_upper_head(pooled)  # (batch_size, prediction_length)
        bb_lower_pred = self.bb_lower_head(pooled)  # (batch_size, prediction_length)
        
        # ê²°í•©í•˜ì—¬ ìµœì¢… ì¶œë ¥
        output = torch.stack([close_pred, bb_upper_pred, bb_lower_pred], dim=-1)
        # (batch_size, prediction_length, num_targets)
        
        return output

class PyTorchStockTransformer:
    """
    Transformer ê¸°ë°˜ ì£¼ì‹ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
    """
    def __init__(self,
                 sequence_length: int = 60,
                 prediction_length: int = 5,
                 num_features: int = 42,
                 num_targets: int = 3,
                 device: str = None):
        
        self.sequence_length = sequence_length
        self.prediction_length = prediction_length
        self.num_features = num_features
        self.num_targets = num_targets
        
        # GPU ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ Transformer Using device: {self.device}")
        
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.history = {'train_loss': [], 'val_loss': []}
        
        self.logger = logging.getLogger(__name__)
    
    def build_model(self,
                   d_model: int = 512,
                   n_heads: int = 8,
                   n_layers: int = 6,
                   d_ff: int = 2048,
                   dropout: float = 0.1,
                   loss_type: str = "multi_target"):
        """Transformer ëª¨ë¸ êµ¬ì¶•"""
        
        self.model = StockTransformer(
            input_size=self.num_features,
            d_model=d_model,
            n_heads=n_heads,
            n_layers=n_layers,
            d_ff=d_ff,
            max_seq_len=self.sequence_length,
            prediction_length=self.prediction_length,
            num_targets=self.num_targets,
            dropout=dropout
        ).to(self.device)
        
        # ì˜µí‹°ë§ˆì´ì € (Transformerì—ëŠ” AdamWê°€ ë” ì í•©)
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(), 
            lr=0.0001,  # TransformerëŠ” ë” ì‘ì€ í•™ìŠµë¥  ì‚¬ìš©
            weight_decay=1e-4,
            betas=(0.9, 0.999)
        )
        
        # ğŸš€ ê³ ê¸‰ ì†ì‹¤í•¨ìˆ˜ ì ìš©
        from advanced_metrics import get_advanced_loss_function
        if loss_type != "mse":
            self.criterion = get_advanced_loss_function(loss_type).to(self.device)
        else:
            self.criterion = nn.MSELoss()
            
        self.loss_type = loss_type
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ—ï¸ Transformer built with {total_params:,} trainable parameters")
        
        self.logger.info(f"ğŸš€ Transformer model built with {loss_type} loss function")
        
        return self.model
    
    def create_padding_mask(self, seq_len: int) -> torch.Tensor:
        """íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„± (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        mask = torch.ones(seq_len, seq_len, device=self.device)
        return mask
    
    def train_model(self, X_train: np.ndarray, y_train: np.ndarray,
                   X_val: np.ndarray, y_val: np.ndarray,
                   epochs: int = 100, batch_size: int = 64, patience: int = 15) -> Dict[str, List[float]]:
        """
        Transformer ëª¨ë¸ í•™ìŠµ
        
        Args:
            X_train, y_train: í•™ìŠµ ë°ì´í„°
            X_val, y_val: ê²€ì¦ ë°ì´í„°
            epochs: í•™ìŠµ ì—í¬í¬
            batch_size: ë°°ì¹˜ í¬ê¸° (TransformerëŠ” ë” ì‘ì€ ë°°ì¹˜ ì‚¬ìš©)
            patience: ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´
        
        Returns:
            í•™ìŠµ íˆìŠ¤í† ë¦¬
        """
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")
        
        from torch.utils.data import DataLoader, TensorDataset
        
        # ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜
        X_train_tensor = torch.FloatTensor(X_train)
        y_train_tensor = torch.FloatTensor(y_train)
        X_val_tensor = torch.FloatTensor(X_val)
        y_val_tensor = torch.FloatTensor(y_val)
        
        # DataLoader ìƒì„±
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (Transformerì— ì í•©í•œ ì›œì—… ìŠ¤ì¼€ì¤„ëŸ¬)
        from torch.optim.lr_scheduler import OneCycleLR
        scheduler = OneCycleLR(
            self.optimizer,
            max_lr=0.001,
            epochs=epochs,
            steps_per_epoch=len(train_loader),
            pct_start=0.1  # 10% ì›œì—…
        )
        
        # í•™ìŠµ ê¸°ë¡
        train_losses = []
        val_losses = []
        best_val_loss = float('inf')
        patience_counter = 0
        
        self.logger.info(f"ğŸš€ Starting Transformer training for {epochs} epochs...")
        
        for epoch in range(epochs):
            # í•™ìŠµ ëª¨ë“œ
            self.model.train()
            train_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                
                self.optimizer.zero_grad()
                
                # ìˆœì „íŒŒ
                outputs = self.model(batch_X)
                
                # ì†ì‹¤ ê³„ì‚° (ì†ì‹¤í•¨ìˆ˜ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
                if self.loss_type == "multi_target":
                    loss = self.criterion(outputs, batch_y)
                elif self.loss_type == "directional":
                    # ì´ì „ ê°€ê²© ì •ë³´ê°€ í•„ìš”í•˜ë¯€ë¡œ ê¸°ë³¸ MSE ì‚¬ìš©
                    loss = torch.nn.MSELoss()(outputs, batch_y)
                else:
                    loss = self.criterion(outputs, batch_y)
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (Transformer ì•ˆì •ì„±)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                scheduler.step()  # ìŠ¤í…ë§ˆë‹¤ í•™ìŠµë¥  ì—…ë°ì´íŠ¸
                
                train_loss += loss.item()
            
            # ê²€ì¦ ëª¨ë“œ
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                    
                    outputs = self.model(batch_X)
                    
                    if self.loss_type == "multi_target":
                        loss = self.criterion(outputs, batch_y)
                    else:
                        loss = torch.nn.MSELoss()(outputs, batch_y)
                    
                    val_loss += loss.item()
            
            # í‰ê·  ì†ì‹¤ ê³„ì‚°
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # ë¡œê¹…
            if (epoch + 1) % 10 == 0:
                current_lr = scheduler.get_last_lr()[0]
                self.logger.info(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f}, "
                               f"Val Loss: {avg_val_loss:.6f}, LR: {current_lr:.6f}")
            
            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # ìµœê³  ëª¨ë¸ ì €ì¥ (ë©”ëª¨ë¦¬ì—)
                self.best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1
            
            if patience_counter >= patience:
                self.logger.info(f"Early stopping at epoch {epoch+1}")
                break
        
        # ìµœê³  ëª¨ë¸ ë³µì›
        if hasattr(self, 'best_model_state'):
            self.model.load_state_dict(self.best_model_state)
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.history = {
            'train_loss': train_losses,
            'val_loss': val_losses
        }
        
        self.logger.info(f"âœ… Transformer training completed. Best val loss: {best_val_loss:.6f}")
        
        return self.history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")
        
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            predictions = self.model(X_tensor)
            
        return predictions.cpu().numpy()
    
    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            raise ValueError("Model not built.")
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'sequence_length': self.sequence_length,
                'prediction_length': self.prediction_length,
                'num_features': self.num_features,
                'num_targets': self.num_targets
            },
            'history': self.history
        }, filepath)
        
        self.logger.info(f"Transformer model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(filepath, map_location=self.device)
        
        # ì„¤ì • ë³µì›
        config = checkpoint['model_config']
        self.sequence_length = config['sequence_length']
        self.prediction_length = config['prediction_length']
        self.num_features = config['num_features']
        self.num_targets = config['num_targets']
        
        # ëª¨ë¸ ë¹Œë“œ (ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ)
        self.build_model()
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # íˆìŠ¤í† ë¦¬ ë³µì›
        if 'history' in checkpoint:
            self.history = checkpoint['history']
        
        self.logger.info(f"Transformer model loaded from {filepath}")
    
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€ (ê³ ê¸‰ í‰ê°€ì§€í‘œ ì‚¬ìš©)"""
        predictions = self.predict(X_test)
        
        # ê³ ê¸‰ í‰ê°€ì§€í‘œ ê³„ì‚°
        if hasattr(self, 'metrics_calculator'):
            # Close ê°€ê²©ì— ëŒ€í•œ í‰ê°€ (ì²« ë²ˆì§¸ íƒ€ê²Ÿ)
            close_predictions = predictions[:, :, 0].flatten()
            close_targets = y_test[:, :, 0].flatten()
            
            # ì´ì „ ê°€ê²© ì¶”ì • (ì²« ë²ˆì§¸ ê°’ìœ¼ë¡œ ê·¼ì‚¬)
            previous_prices = np.roll(close_targets, 1)
            previous_prices[0] = close_targets[0]
            
            # ì¢…í•© í‰ê°€ì§€í‘œ ê³„ì‚°
            metrics = self.metrics_calculator.calculate_comprehensive_metrics(
                predictions=close_predictions,
                targets=close_targets,
                previous_prices=previous_prices
            )
            
            self.logger.info(f"âœ… Transformer evaluation completed with {len(metrics)} metrics")
            return metrics
        else:
            # ê¸°ë³¸ í‰ê°€ì§€í‘œ
            from sklearn.metrics import mean_squared_error, mean_absolute_error
            
            mse = mean_squared_error(y_test.reshape(-1), predictions.reshape(-1))
            mae = mean_absolute_error(y_test.reshape(-1), predictions.reshape(-1))
            
            return {
                'MSE': mse,
                'MAE': mae,
                'RMSE': np.sqrt(mse)
            }
    
    def get_model_info(self) -> Dict[str, any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if self.model is None:
            return {"status": "Model not built"}
            
        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            "model_type": "Stock Transformer",
            "architecture": "Multi-Head Self-Attention",
            "sequence_length": self.sequence_length,
            "prediction_length": self.prediction_length,
            "num_features": self.num_features,
            "num_targets": self.num_targets,
            "total_parameters": total_params,
            "device": str(self.device),
            "loss_function": getattr(self, 'loss_type', 'unknown')
        }

# ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìœ í‹¸ë¦¬í‹°
class ModelComparison:
    """LSTM vs Transformer ì„±ëŠ¥ ë¹„êµ"""
    
    @staticmethod
    def compare_architectures():
        """ì•„í‚¤í…ì²˜ ë¹„êµ ì •ë³´"""
        comparison = {
            "LSTM": {
                "ì¥ì ": ["ìˆœì°¨ ì²˜ë¦¬", "ë©”ëª¨ë¦¬ íš¨ìœ¨ì ", "ì˜ ê²€ì¦ëœ êµ¬ì¡°"],
                "ë‹¨ì ": ["ìˆœì°¨ ì²˜ë¦¬ë¡œ ì¸í•œ ì†ë„ ì €í•˜", "ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì •ë³´ ì†ì‹¤", "ë³‘ë ¬ ì²˜ë¦¬ ì–´ë ¤ì›€"],
                "íŒŒë¼ë¯¸í„°": "~2M",
                "í•™ìŠµ ì‹œê°„": "ì¤‘ê°„"
            },
            "Transformer": {
                "ì¥ì ": ["ë³‘ë ¬ ì²˜ë¦¬", "ê¸´ ì‹œí€€ìŠ¤ í•™ìŠµ", "ì–´í…ì…˜ìœ¼ë¡œ ì¤‘ìš”ë„ íŒŒì•…", "ìµœì‹  ê¸°ìˆ "],
                "ë‹¨ì ": ["ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ", "ê³¼ì í•© ìœ„í—˜", "í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë³µì¡"],
                "íŒŒë¼ë¯¸í„°": "~5M",
                "í•™ìŠµ ì‹œê°„": "ë¹ ë¦„ (ë³‘ë ¬ ì²˜ë¦¬)"
            }
        }
        return comparison

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # Transformer ëª¨ë¸ í…ŒìŠ¤íŠ¸
    transformer = PyTorchStockTransformer()
    model = transformer.build_model(
        d_model=512,
        n_heads=8,
        n_layers=6
    )
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    info = transformer.get_model_info()
    print("\n=== Transformer Model Info ===")
    for key, value in info.items():
        print(f"{key}: {value}")
    
    # ì•„í‚¤í…ì²˜ ë¹„êµ
    comparison = ModelComparison.compare_architectures()
    print("\n=== LSTM vs Transformer Comparison ===")
    for model_type, details in comparison.items():
        print(f"\n{model_type}:")
        for aspect, values in details.items():
            print(f"  {aspect}: {values}")
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    dummy_input = np.random.randn(4, 60, 42)  # (batch, seq_len, features)
    predictions = transformer.predict(dummy_input)
    print(f"\nğŸ”® Prediction shape: {predictions.shape}")
    print(f"ğŸ”® Sample prediction: {predictions[0, :, 0]}")  # ì²« ë²ˆì§¸ ë°°ì¹˜, Close ì˜ˆì¸¡