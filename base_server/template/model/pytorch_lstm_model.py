"""
PyTorch LSTM ëª¨ë¸ êµ¬í˜„
TensorFlow ëŒ€ì‹  PyTorch ì‚¬ìš©ìœ¼ë¡œ GPU ë¬¸ì œ í•´ê²°
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
from typing import Tuple, Dict, List, Optional
import logging
import pickle
import os
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

# ğŸš€ ê³ ê¸‰ ì†ì‹¤í•¨ìˆ˜ ë° í‰ê°€ì§€í‘œ import
# Support both package and script execution contexts for advanced_metrics
try:
    from .advanced_metrics import (
        DirectionalLoss, VolatilityAwareLoss, MultiTargetLoss,
        AdvancedMetrics, get_advanced_loss_function
    )
except ImportError:  # executed when run as a script from this folder
    from advanced_metrics import (
        DirectionalLoss, VolatilityAwareLoss, MultiTargetLoss,
        AdvancedMetrics, get_advanced_loss_function
    )

class StockLSTM(nn.Module):
    def __init__(self, 
                 input_size: int = 42,       # ğŸš€ ê³ ê¸‰ í”¼ì²˜ í™•ì¥ (18 â†’ 42)
                 hidden_size: int = 512,     # ğŸ”¥ 4ë°° ì¦ê°€ (RTX 4090 í™œìš©)
                 num_layers: int = 4,        # ğŸ”¥ 2ë°° ì¦ê°€ 
                 output_size: int = 15,      # 5ì¼ * 3íƒ€ê²Ÿ
                 dropout: float = 0.2):
        super(StockLSTM, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        
        # ğŸ”¥ RTX 4090 ìµœì í™” LSTM ìŠ¤íƒ (4ì¸µ)
        self.lstm1 = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True,
            dropout=0,
            bidirectional=False
        )
        self.dropout1 = nn.Dropout(dropout)
        self.layernorm1 = nn.LayerNorm(hidden_size)
        
        self.lstm2 = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True,
            dropout=0,
            bidirectional=False
        )
        self.dropout2 = nn.Dropout(dropout)
        self.layernorm2 = nn.LayerNorm(hidden_size)
        
        self.lstm3 = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True,
            dropout=0,
            bidirectional=False
        )
        self.dropout3 = nn.Dropout(dropout)
        self.layernorm3 = nn.LayerNorm(hidden_size)
        
        self.lstm4 = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size//2,
            num_layers=1,
            batch_first=True,
            dropout=0,
            bidirectional=False
        )
        self.dropout4 = nn.Dropout(dropout)
        self.layernorm4 = nn.LayerNorm(hidden_size//2)
        
        # ğŸ”¥ RTX 4090 ìµœì í™” Dense ìŠ¤íƒ (ë” í° ë ˆì´ì–´ë“¤)
        self.fc1 = nn.Linear(hidden_size//2, hidden_size*2)
        self.dropout5 = nn.Dropout(dropout)
        
        self.fc2 = nn.Linear(hidden_size*2, hidden_size)
        self.dropout6 = nn.Dropout(dropout)
        
        self.fc3 = nn.Linear(hidden_size, hidden_size//2)
        self.dropout7 = nn.Dropout(dropout)
        
        self.fc4 = nn.Linear(hidden_size//2, 128)
        self.dropout8 = nn.Dropout(dropout)
        
        self.output_layer = nn.Linear(128, output_size)
        
        # ğŸ”¥ ë” ë³µì¡í•œ í™œì„±í™” í•¨ìˆ˜
        self.relu = nn.ReLU()
        self.gelu = nn.GELU()
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        
        # ğŸ”¥ 4ì¸µ LSTM ìŠ¤íƒ (RTX 4090 ìµœì í™”)
        # LSTM 1
        lstm1_out, _ = self.lstm1(x)
        lstm1_out = self.dropout1(lstm1_out)
        lstm1_out = self.layernorm1(lstm1_out)
        
        # LSTM 2
        lstm2_out, _ = self.lstm2(lstm1_out)
        lstm2_out = self.dropout2(lstm2_out)
        lstm2_out = self.layernorm2(lstm2_out)
        
        # LSTM 3
        lstm3_out, _ = self.lstm3(lstm2_out)
        lstm3_out = self.dropout3(lstm3_out)
        lstm3_out = self.layernorm3(lstm3_out)
        
        # LSTM 4
        lstm4_out, _ = self.lstm4(lstm3_out)
        lstm4_out = self.dropout4(lstm4_out)
        lstm4_out = self.layernorm4(lstm4_out)
        
        # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ë§Œ ì‚¬ìš©
        last_output = lstm4_out[:, -1, :]
        
        # ğŸ”¥ 5ì¸µ Dense ìŠ¤íƒ (RTX 4090 ìµœì í™”)
        x = self.gelu(self.fc1(last_output))
        x = self.dropout5(x)
        
        x = self.relu(self.fc2(x))
        x = self.dropout6(x)
        
        x = self.gelu(self.fc3(x))
        x = self.dropout7(x)
        
        x = self.relu(self.fc4(x))
        x = self.dropout8(x)
        
        # ì¶œë ¥
        output = self.output_layer(x)
        
        # (batch_size, 15) -> (batch_size, 5, 3) í˜•íƒœë¡œ ë³€í™˜
        output = output.view(-1, 5, 3)
        
        return output

class PyTorchStockLSTM:
    def __init__(self, 
                 sequence_length: int = 60,
                 prediction_length: int = 5,
                 num_features: int = 42,     # ğŸš€ ê³ ê¸‰ í”¼ì²˜ í™•ì¥ (18 â†’ 42)
                 num_targets: int = 3,
                 device: str = None):
        
        self.sequence_length = sequence_length
        self.prediction_length = prediction_length
        self.num_features = num_features
        self.num_targets = num_targets
        
        # GPU ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ Using device: {self.device}")
        if self.device.type == 'cuda':
            print(f"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.history = {'train_loss': [], 'val_loss': []}
        
        self.logger = logging.getLogger(__name__)
        
    def build_model(self, 
                   hidden_size: int = 512,    # ğŸ”¥ RTX 4090 ìµœì í™” (4ë°° ì¦ê°€)
                   num_layers: int = 4,       # ğŸ”¥ RTX 4090 ìµœì í™” (2ë°° ì¦ê°€)
                   dropout: float = 0.2,
                   loss_type: str = "multi_target"):  # ğŸš€ ê³ ê¸‰ ì†ì‹¤í•¨ìˆ˜ ì„ íƒ
        """ëª¨ë¸ êµ¬ì¶• (ê³ ê¸‰ ì†ì‹¤í•¨ìˆ˜ ì ìš©)"""
        
        output_size = self.prediction_length * self.num_targets
        
        self.model = StockLSTM(
            input_size=self.num_features,
            hidden_size=hidden_size,
            num_layers=num_layers,
            output_size=output_size,
            dropout=dropout
        ).to(self.device)  # GPUë¡œ ì´ë™
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-5)
        
        # ğŸš€ ê³ ê¸‰ ì†ì‹¤í•¨ìˆ˜ ì ìš©
        if loss_type == "multi_target":
            self.criterion = MultiTargetLoss(
                close_weight=0.6,      # Close ê°€ê²©ì— 60% ê°€ì¤‘ì¹˜
                bb_upper_weight=0.2,   # BB_Upperì— 20% ê°€ì¤‘ì¹˜  
                bb_lower_weight=0.2    # BB_Lowerì— 20% ê°€ì¤‘ì¹˜
            ).to(self.device)
            self.loss_type = "multi_target"
        elif loss_type == "directional":
            self.criterion = DirectionalLoss(
                mse_weight=0.7,        # MSEì— 70% ê°€ì¤‘ì¹˜
                direction_weight=0.3   # ë°©í–¥ì„±ì— 30% ê°€ì¤‘ì¹˜
            ).to(self.device)
            self.loss_type = "directional"
        elif loss_type == "volatility_aware":
            self.criterion = VolatilityAwareLoss(
                base_weight=1.0,
                volatility_factor=0.5
            ).to(self.device)
            self.loss_type = "volatility_aware"
        else:
            # ê¸°ë³¸ MSE (í•˜ìœ„ í˜¸í™˜ì„±)
            self.criterion = nn.MSELoss()
            self.loss_type = "mse"
            
        # ğŸš€ ê³ ê¸‰ í‰ê°€ì§€í‘œ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        self.metrics_calculator = AdvancedMetrics()
        
        self.logger.info(f"ğŸš€ Model built with {loss_type} loss function")
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ—ï¸ Model built with {total_params:,} trainable parameters")
        
        return self.model
    
    def train_model(self,
                   X_train: np.ndarray,
                   y_train: np.ndarray,
                   X_val: np.ndarray = None,
                   y_val: np.ndarray = None,
                   epochs: int = 100,
                   batch_size: int = 128,    # ğŸ”¥ RTX 4090 ìµœì í™” (4ë°° ì¦ê°€)
                   patience: int = 15):
        """ëª¨ë¸ í•™ìŠµ"""
        
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")
        
        # ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ê³  GPUë¡œ ì´ë™
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train).to(self.device)
        
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # ê²€ì¦ ë°ì´í„° ì¤€ë¹„
        val_loader = None
        if X_val is not None and y_val is not None:
            X_val_tensor = torch.FloatTensor(X_val).to(self.device)
            y_val_tensor = torch.FloatTensor(y_val).to(self.device)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # í•™ìŠµ ì‹œì‘
        print(f"ğŸš€ Starting training on {self.device}")
        print(f"ğŸ“Š Training samples: {len(X_train)}")
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            # í•™ìŠµ ëª¨ë“œ
            self.model.train()
            train_loss = 0.0
            
            # ğŸ”¥ GPU ì‚¬ìš© ìƒíƒœ ì¶œë ¥
            if epoch == 0 and self.device.type == 'cuda':
                print(f"ğŸš€ GPU í•™ìŠµ ì‹œì‘ - ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            
            batch_count = 0
            for batch_X, batch_y in train_loader:
                batch_count += 1
                
                # ğŸ”¥ GPU ë””ë°”ì´ìŠ¤ í™•ì¸ (ì²« ë°°ì¹˜ì—ì„œë§Œ)
                if epoch == 0 and batch_count == 1:
                    print(f"ğŸ“Š Batch X device: {batch_X.device}")
                    print(f"ğŸ“Š Batch y device: {batch_y.device}")
                    print(f"ğŸ“Š Model device: {next(self.model.parameters()).device}")
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
                self.optimizer.zero_grad()
                
                # ìˆœì „íŒŒ
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                
                # ğŸ”¥ ì²« ë°°ì¹˜ì—ì„œ ì¶œë ¥ ë””ë°”ì´ìŠ¤ í™•ì¸
                if epoch == 0 and batch_count == 1:
                    print(f"ğŸ“Š Outputs device: {outputs.device}")
                    print(f"ğŸ“Š Loss device: {loss.device}")
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                self.optimizer.step()
                
                train_loss += loss.item()
                
                # ğŸ”¥ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ê¸°ì  ì¶œë ¥
                if batch_count % 10 == 0 and self.device.type == 'cuda':
                    print(f"  Batch {batch_count}: GPU ë©”ëª¨ë¦¬ {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            
            avg_train_loss = train_loss / len(train_loader)
            self.history['train_loss'].append(avg_train_loss)
            
            # ê²€ì¦
            val_loss = 0.0
            if val_loader is not None:
                self.model.eval()
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        outputs = self.model(batch_X)
                        loss = self.criterion(outputs, batch_y)
                        val_loss += loss.item()
                
                avg_val_loss = val_loss / len(val_loader)
                self.history['val_loss'].append(avg_val_loss)
                
                print(f"Epoch {epoch+1}/{epochs} - "
                      f"Train Loss: {avg_train_loss:.6f} - "
                      f"Val Loss: {avg_val_loss:.6f}")
                
                # Early stopping
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"Early stopping at epoch {epoch+1}")
                        break
            else:
                print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.6f}")
        
        print("ğŸ‰ Training completed!")
        return self.history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        if self.model is None:
            raise ValueError("Model not trained. Train the model first.")
        
        self.model.eval()
        
        X_tensor = torch.FloatTensor(X).to(self.device)
        
        with torch.no_grad():
            predictions = self.model(X_tensor)
        
        # CPUë¡œ ì´ë™í•˜ê³  numpyë¡œ ë³€í™˜
        return predictions.cpu().numpy()
    
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray, 
                 previous_prices: Optional[np.ndarray] = None) -> Dict[str, float]:
        """
        ğŸš€ ê³ ê¸‰ í‰ê°€ì§€í‘œë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ í‰ê°€
        
        Args:
            X_test: í…ŒìŠ¤íŠ¸ ì…ë ¥ ë°ì´í„°
            y_test: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„° (ì •ê·œí™”ëœ ìƒíƒœ)
            previous_prices: ì´ì „ ê°€ê²© ë°ì´í„° (ë°©í–¥ì„± ê³„ì‚°ìš©)
            
        Returns:
            ì¢…í•© í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        self.logger.info("ğŸ” Starting comprehensive model evaluation...")
        
        predictions = self.predict(X_test)
        
        # 1. ê¸°ë³¸ ë©”íŠ¸ë¦­ (í•˜ìœ„ í˜¸í™˜ì„±)
        normalized_mse = mean_squared_error(y_test.reshape(-1), predictions.reshape(-1))
        normalized_mae = mean_absolute_error(y_test.reshape(-1), predictions.reshape(-1))
        normalized_rmse = np.sqrt(normalized_mse)
        
        epsilon = 1e-8  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        mape = np.mean(np.abs((y_test - predictions) / (y_test + epsilon))) * 100
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        metrics = {
            'MSE': normalized_mse,
            'MAE': normalized_mae,
            'RMSE': normalized_rmse,
            'MAPE': mape,
        }
        
        # 2. ğŸš€ ê³ ê¸‰ í‰ê°€ì§€í‘œ ê³„ì‚°
        if hasattr(self, 'metrics_calculator'):
            # Close ê°€ê²©ì— ëŒ€í•œ ê³ ê¸‰ ì§€í‘œ (ì²« ë²ˆì§¸ íƒ€ê²Ÿ)
            close_predictions = predictions[:, :, 0].flatten()  # Close ì˜ˆì¸¡
            close_targets = y_test[:, :, 0].flatten()           # Close ì‹¤ì œ
            
            # ì´ì „ ê°€ê²©ì´ ì—†ìœ¼ë©´ í˜„ì¬ ë°ì´í„°ì—ì„œ ì¶”ì •
            if previous_prices is None:
                # ì²« ë²ˆì§¸ ì‹œì ì˜ ì´ì „ ê°€ê²©ì„ í˜„ì¬ ì²« ê°’ìœ¼ë¡œ ê·¼ì‚¬
                previous_prices = np.roll(close_targets, 1)
                previous_prices[0] = close_targets[0]  # ì²« ê°’ ë³´ì •
            else:
                previous_prices = previous_prices[:, :, 0].flatten()
            
            # ğŸš€ ì¢…í•© ê³ ê¸‰ ì§€í‘œ ê³„ì‚°
            advanced_metrics = self.metrics_calculator.calculate_comprehensive_metrics(
                predictions=close_predictions,
                targets=close_targets,
                previous_prices=previous_prices
            )
            
            # ê³ ê¸‰ ì§€í‘œë¥¼ ë©”ì¸ ì§€í‘œì— í†µí•©
            metrics.update(advanced_metrics)
            
            self.logger.info(f"âœ… Advanced metrics calculated: {len(advanced_metrics)} indicators")
        
        # 3. íƒ€ê²Ÿë³„ ìƒì„¸ í‰ê°€
        target_names = ['Close', 'BB_Upper', 'BB_Lower']
        
        for i, target_name in enumerate(target_names):
            target_true = y_test[:, :, i]
            target_pred = predictions[:, :, i]
            
            # ì •ê·œí™”ëœ ìƒíƒœ ë©”íŠ¸ë¦­
            target_mse = mean_squared_error(target_true.reshape(-1), target_pred.reshape(-1))
            target_mae = mean_absolute_error(target_true.reshape(-1), target_pred.reshape(-1))
            
            # MAPE (íƒ€ê²Ÿë³„)
            target_mape = np.mean(np.abs((target_true - target_pred) / (target_true + epsilon))) * 100
            
            # ë°©í–¥ì„± ì •í™•ë„ (íƒ€ê²Ÿë³„)
            pred_dir = np.sign(np.diff(target_pred, axis=1))
            true_dir = np.sign(np.diff(target_true, axis=1))
            target_direction_acc = np.mean(pred_dir == true_dir) * 100
            
            # RÂ² Score (ê²°ì •ê³„ìˆ˜)
            ss_res = np.sum((target_true - target_pred) ** 2)
            ss_tot = np.sum((target_true - np.mean(target_true)) ** 2)
            r2_score = 1 - (ss_res / (ss_tot + epsilon))
            
            metrics.update({
                f'Normalized_{target_name}_MSE': target_mse,
                f'Normalized_{target_name}_MAE': target_mae,
                f'{target_name}_MAPE': target_mape,
                f'{target_name}_Direction_Accuracy': target_direction_acc,
                f'{target_name}_R2_Score': r2_score
            })
        
        return metrics
    
    def save_model(self, save_path: str):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            raise ValueError("No model to save")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'history': self.history,
            'model_config': {
                'sequence_length': self.sequence_length,
                'prediction_length': self.prediction_length,
                'num_features': self.num_features,
                'num_targets': self.num_targets
            }
        }, save_path)
        
        print(f"ğŸ’¾ Model saved to {save_path}")
    
    def load_model(self, load_path: str, hidden_size: int = 128):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(load_path, map_location=self.device)
        
        # ëª¨ë¸ êµ¬ì¶•
        self.build_model(hidden_size=hidden_size)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.history = checkpoint['history']
        
        print(f"ğŸ“¥ Model loaded from {load_path}")
        
    def plot_training_history(self, save_path: str = None):
        """í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”"""
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(self.history['train_loss'], label='Train Loss')
        if 'val_loss' in self.history and self.history['val_loss']:
            plt.plot(self.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        
        if save_path:
            plt.savefig(save_path)
            print(f"ğŸ“Š Training history saved to {save_path}")
        else:
            plt.show()

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=== PyTorch LSTM ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    
    # ğŸ”¥ RTX 4090 ìµœì í™” ë”ë¯¸ ë°ì´í„° ìƒì„±
    batch_size = 1000              # ğŸ”¥ 10ë°° ì¦ê°€
    sequence_length = 60
    num_features = 18
    
    X_dummy = np.random.randn(batch_size, sequence_length, num_features).astype(np.float32)
    y_dummy = np.random.randn(batch_size, 5, 3).astype(np.float32)
    
    print(f"ğŸ“Š RTX 4090 Test data shape: X={X_dummy.shape}, y={y_dummy.shape}")
    print(f"ğŸ’¾ ì˜ˆìƒ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~{batch_size * sequence_length * num_features * 4 / 1024**3:.2f}GB")
    
    # ğŸ”¥ RTX 4090 ìµœì í™” ëª¨ë¸ ìƒì„±
    model = PyTorchStockLSTM()
    model.build_model(hidden_size=512, num_layers=4)  # í° ëª¨ë¸
    
    print("\nğŸš€ RTX 4090 ìµœì í™” í•™ìŠµ ì‹œì‘...")
    print("ğŸ’¡ ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ nvidia-smi í™•ì¸í•˜ì„¸ìš”!")
    history = model.train_model(X_dummy, y_dummy, epochs=10, batch_size=128)
    
    print("\nğŸ”® Testing prediction...")
    predictions = model.predict(X_dummy[:10])
    print(f"Predictions shape: {predictions.shape}")
    
    print("\nâœ… RTX 4090 ìµœì í™” PyTorch LSTM ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("GPU ì‚¬ìš©ë¥ ì´ 90% ì´ìƒ ì˜¬ë¼ê°”ë‚˜ìš”? ğŸ”¥")