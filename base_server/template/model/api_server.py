"""
FastAPI ê¸°ë°˜ ì£¼ì‹ ì˜ˆì¸¡ API ì„œë²„
ë°°ì¹˜ ì¶”ë¡  ë° ì‹¤ì‹œê°„ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ ì œê³µ
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
import uvicorn
import asyncio
import logging
import os
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import json
from contextlib import asynccontextmanager

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from data_collector import StockDataCollector
from data_preprocessor import StockDataPreprocessor
from pytorch_lstm_model import PyTorchStockLSTM
from config import get_model_paths

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜
model = None
preprocessor = None
data_collector = None

class PredictionRequest(BaseModel):
    """ë‹¨ì¼ ì˜ˆì¸¡ ìš”ì²­"""
    symbol: str = Field(..., description="ì£¼ì‹ ì‹¬ë³¼ (ì˜ˆ: AAPL)")
    days: int = Field(60, description="ì‚¬ìš©í•  ê³¼ê±° ë°ì´í„° ì¼ìˆ˜")

class BatchPredictionRequest(BaseModel):
    """ë°°ì¹˜ ì˜ˆì¸¡ ìš”ì²­"""
    symbols: List[str] = Field(..., description="ì£¼ì‹ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸")
    days: int = Field(60, description="ì‚¬ìš©í•  ê³¼ê±° ë°ì´í„° ì¼ìˆ˜")
    batch_size: int = Field(5, description="ë°°ì¹˜ í¬ê¸°")

class PredictionResult(BaseModel):
    """ì˜ˆì¸¡ ê²°ê³¼"""
    symbol: str
    prediction_date: str
    current_price: float
    predictions: List[Dict[str, Any]]  # 5ì¼ê°„ì˜ ì˜ˆì¸¡ ê²°ê³¼
    bollinger_bands: List[Dict[str, float]]  # 5ì¼ê°„ì˜ ë³¼ë¦°ì € ë°´ë“œ
    confidence_score: float
    status: str

class BatchPredictionResult(BaseModel):
    """ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼"""
    request_id: str
    total_symbols: int
    completed_symbols: int
    failed_symbols: int
    results: List[PredictionResult]
    processing_time: float
    status: str

class HealthResponse(BaseModel):
    """í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ"""
    status: str
    model_loaded: bool
    gpu_available: bool
    timestamp: str

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    # ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
    await load_model_and_preprocessor()
    yield
    # ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‘ì—… (í•„ìš”í•œ ê²½ìš°)
    logger.info("Shutting down API server...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Stock Prediction API",
    description="LSTM ê¸°ë°˜ ì£¼ì‹ ê°€ê²© ë° ë³¼ë¦°ì € ë°´ë“œ ì˜ˆì¸¡ API",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def load_model_and_preprocessor():
    """ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ë¡œë“œ"""
    global model, preprocessor, data_collector
    
    try:
        logger.info("Loading model and preprocessor...")
        
        # ğŸš€ PyTorch ëª¨ë¸ ë¡œë“œ (í™˜ê²½ë³„ ê²½ë¡œ ìë™ ê°ì§€)
        model_path, preprocessor_path = get_model_paths()
        
        if os.path.exists(model_path):
            # PyTorch ëª¨ë¸ ì´ˆê¸°í™” í›„ ë¡œë“œ
            model = PyTorchStockLSTM(
                sequence_length=60,
                prediction_length=5,
                num_features=18,
                num_targets=3
            )
            model.load_model(model_path, hidden_size=512)  # ğŸ”¥ RTX 4090 ìµœì í™”
            logger.info(f"ğŸš€ PyTorch model loaded from: {model_path}")
        else:
            logger.warning(f"Model file not found: {model_path}")
            model = None
        
        # ì „ì²˜ë¦¬ê¸° ë¡œë“œ
        if os.path.exists(preprocessor_path):
            with open(preprocessor_path, 'rb') as f:
                preprocessor = pickle.load(f)
            logger.info("Preprocessor loaded successfully")
        else:
            logger.warning(f"Preprocessor file not found: {preprocessor_path}")
            preprocessor = StockDataPreprocessor()
        
        # ë°ì´í„° ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        data_collector = StockDataCollector()
        
        logger.info("All components loaded successfully")
        
    except Exception as e:
        logger.error(f"Error loading model/preprocessor: {str(e)}")
        model = None
        preprocessor = None

def format_prediction_result(symbol: str, 
                           current_data: pd.DataFrame,
                           predictions: np.ndarray,
                           confidence: float = 0.8) -> PredictionResult:
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    
    current_price = float(current_data['Close'].iloc[-1])
    prediction_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # 5ì¼ê°„ì˜ ì˜ˆì¸¡ ê²°ê³¼ í¬ë§·íŒ…
    prediction_list = []
    bollinger_list = []
    
    for i in range(5):
        day_prediction = {
            "day": i + 1,
            "date": (datetime.now() + timedelta(days=i+1)).strftime("%Y-%m-%d"),
            "predicted_close": float(predictions[0, i, 0]),  # Close ì˜ˆì¸¡
            "trend": "up" if predictions[0, i, 0] > current_price else "down"
        }
        prediction_list.append(day_prediction)
        
        bollinger_band = {
            "day": i + 1,
            "date": (datetime.now() + timedelta(days=i+1)).strftime("%Y-%m-%d"),
            "bb_upper": float(predictions[0, i, 1]),  # BB_Upper ì˜ˆì¸¡
            "bb_lower": float(predictions[0, i, 2]),  # BB_Lower ì˜ˆì¸¡
            "bb_middle": (float(predictions[0, i, 1]) + float(predictions[0, i, 2])) / 2
        }
        bollinger_list.append(bollinger_band)
    
    return PredictionResult(
        symbol=symbol,
        prediction_date=prediction_date,
        current_price=current_price,
        predictions=prediction_list,
        bollinger_bands=bollinger_list,
        confidence_score=confidence,
        status="success"
    )

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    import tensorflow as tf
    
    gpu_available = len(tf.config.experimental.list_physical_devices('GPU')) > 0
    
    return HealthResponse(
        status="healthy",
        model_loaded=model is not None,
        gpu_available=gpu_available,
        timestamp=datetime.now().isoformat()
    )

@app.post("/predict", response_model=PredictionResult)
async def predict_single_stock(request: PredictionRequest):
    """ë‹¨ì¼ ì¢…ëª© ì˜ˆì¸¡"""
    if model is None or preprocessor is None:
        raise HTTPException(status_code=503, detail="Model or preprocessor not loaded")
    
    try:
        logger.info(f"Processing prediction request for {request.symbol}")
        
        # ìµœê·¼ ë°ì´í„° ìˆ˜ì§‘
        recent_data = data_collector.get_recent_data(request.symbol, request.days)
        if recent_data is None or len(recent_data) < request.days:
            raise HTTPException(
                status_code=404, 
                detail=f"Insufficient data for symbol {request.symbol}"
            )
        
        # ì „ì²˜ë¦¬ ë° ì¶”ë¡ 
        input_sequence = preprocessor.preprocess_for_inference(recent_data)
        predictions = model.predict(input_sequence)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result = format_prediction_result(request.symbol, recent_data, predictions)
        
        logger.info(f"Prediction completed for {request.symbol}")
        return result
        
    except Exception as e:
        logger.error(f"Error predicting {request.symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict/batch", response_model=BatchPredictionResult)
async def predict_batch_stocks(request: BatchPredictionRequest):
    """ë°°ì¹˜ ì£¼ì‹ ì˜ˆì¸¡"""
    if model is None or preprocessor is None:
        raise HTTPException(status_code=503, detail="Model or preprocessor not loaded")
    
    request_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    start_time = datetime.now()
    
    logger.info(f"Processing batch prediction request {request_id} for {len(request.symbols)} symbols")
    
    results = []
    completed = 0
    failed = 0
    
    # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ì²˜ë¦¬
    for i in range(0, len(request.symbols), request.batch_size):
        batch_symbols = request.symbols[i:i + request.batch_size]
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for symbol in batch_symbols:
            task = asyncio.create_task(predict_single_symbol(symbol, request.days))
            tasks.append(task)
        
        # ë°°ì¹˜ ì‹¤í–‰
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for symbol, result in zip(batch_symbols, batch_results):
            if isinstance(result, Exception):
                logger.error(f"Failed to predict {symbol}: {str(result)}")
                failed += 1
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ê²°ê³¼ì— í¬í•¨
                error_result = PredictionResult(
                    symbol=symbol,
                    prediction_date=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    current_price=0.0,
                    predictions=[],
                    bollinger_bands=[],
                    confidence_score=0.0,
                    status=f"failed: {str(result)}"
                )
                results.append(error_result)
            else:
                results.append(result)
                completed += 1
        
        # ë°°ì¹˜ ê°„ ì ì‹œ ëŒ€ê¸° (API ì œí•œ ë°©ì§€)
        if i + request.batch_size < len(request.symbols):
            await asyncio.sleep(0.1)
    
    processing_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(f"Batch prediction {request_id} completed: {completed} success, {failed} failed")
    
    return BatchPredictionResult(
        request_id=request_id,
        total_symbols=len(request.symbols),
        completed_symbols=completed,
        failed_symbols=failed,
        results=results,
        processing_time=processing_time,
        status="completed"
    )

async def predict_single_symbol(symbol: str, days: int) -> PredictionResult:
    """ë‹¨ì¼ ì‹¬ë³¼ ì˜ˆì¸¡ (ë‚´ë¶€ í•¨ìˆ˜)"""
    try:
        # ìµœê·¼ ë°ì´í„° ìˆ˜ì§‘
        recent_data = data_collector.get_recent_data(symbol, days)
        if recent_data is None or len(recent_data) < days:
            raise ValueError(f"Insufficient data for symbol {symbol}")
        
        # ì „ì²˜ë¦¬ ë° ì¶”ë¡ 
        input_sequence = preprocessor.preprocess_for_inference(recent_data)
        predictions = model.predict(input_sequence)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result = format_prediction_result(symbol, recent_data, predictions)
        return result
        
    except Exception as e:
        raise Exception(f"Error predicting {symbol}: {str(e)}")

@app.get("/models/info")
async def get_model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        model_info = {
            "model_type": "LSTM with Attention",
            "sequence_length": 60,
            "prediction_length": 5,
            "features": [
                "Open", "High", "Low", "Close", "Volume",
                "MA_5", "MA_20", "MA_60",
                "BB_Upper", "BB_Middle", "BB_Lower", "BB_Percent", "BB_Width",
                "RSI", "MACD", "MACD_Signal", "Price_Change", "Volatility"
            ],
            "targets": ["Close", "BB_Upper", "BB_Lower"],
            "model_parameters": model.model.count_params() if model.model else 0
        }
        return model_info
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Stock Prediction API",
        "version": "1.0.0",
        "endpoints": {
            "health": "/health",
            "single_prediction": "/predict",
            "batch_prediction": "/predict/batch",
            "model_info": "/models/info"
        }
    }

if __name__ == "__main__":
    # ê°œë°œ í™˜ê²½ì—ì„œ ì‹¤í–‰
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )