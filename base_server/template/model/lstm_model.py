"""
LSTM ëª¨ë¸ êµ¬í˜„
60ì¼ ë°ì´í„°ë¡œ ë‹¤ìŒ 5ì¼ì˜ ì¶”ì„¸ì™€ Bollinger Band ì˜ˆì¸¡
"""

import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, LayerNormalization, MultiHeadAttention
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.regularizers import l2
import numpy as np
import pandas as pd
from typing import Tuple, Dict, List, Optional
import logging
import pickle
import os
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

class StockLSTMModel:
    def __init__(self, 
                 sequence_length: int = 60,
                 prediction_length: int = 5,
                 num_features: int = 18,
                 num_targets: int = 3):
        """
        LSTM ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            sequence_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (60ì¼)
            prediction_length: ì˜ˆì¸¡ ê¸¸ì´ (5ì¼)
            num_features: ì…ë ¥ í”¼ì²˜ ìˆ˜
            num_targets: ì˜ˆì¸¡ íƒ€ê²Ÿ ìˆ˜ (Close, BB_Upper, BB_Lower)
        """
        self.sequence_length = sequence_length
        self.prediction_length = prediction_length
        self.num_features = num_features
        self.num_targets = num_targets
        self.model = None
        self.history = None
        self.logger = logging.getLogger(__name__)
        
        # GPU ì‚¬ìš© ì„¤ì •
        self.setup_gpu()
    
    def setup_gpu(self):
        """GPU ì„¤ì • ë° í™•ì¸"""
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            try:
                # GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš©
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                self.logger.info(f"Found {len(gpus)} GPU(s): {[gpu.name for gpu in gpus]}")
            except RuntimeError as e:
                self.logger.error(f"GPU setup error: {e}")
        else:
            self.logger.warning("No GPU found, using CPU")
    
    def build_model(self, model_type: str = "lstm_attention") -> Model:
        """
        LSTM ëª¨ë¸ êµ¬ì¶•
        
        Args:
            model_type: ëª¨ë¸ íƒ€ì… ("lstm", "lstm_attention", "lstm_ensemble")
            
        Returns:
            ì»´íŒŒì¼ëœ Keras ëª¨ë¸
        """
        # ğŸš€ GPU ê°•ì œ ì‚¬ìš©ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ ìƒì„±
        with tf.device('/GPU:0'):
            if model_type == "lstm":
                model = self._build_basic_lstm()
            elif model_type == "lstm_attention":
                model = self._build_lstm_with_attention()
            elif model_type == "lstm_ensemble":
                model = self._build_lstm_ensemble()
            else:
                raise ValueError(f"Unknown model type: {model_type}")
            
            # ëª¨ë¸ ì»´íŒŒì¼ë„ GPUì—ì„œ
            model.compile(
                optimizer=Adam(learning_rate=0.001, clipnorm=1.0),
                loss='mse',
                metrics=['mae']
            )
        
        self.model = model
        self.logger.info(f"Built {model_type} model with {model.count_params():,} parameters")
        self.logger.info("ğŸš€ Model created with GPU enforcement!")
        return model
    
    def _build_basic_lstm(self) -> Model:
        """ê¸°ë³¸ LSTM ëª¨ë¸"""
        model = Sequential([
            Input(shape=(self.sequence_length, self.num_features)),
            
            LSTM(128, return_sequences=True, dropout=0.2),  # cuDNN ìµœì í™”ë¥¼ ìœ„í•´ recurrent_dropout ì œê±°
            LayerNormalization(),
            
            LSTM(64, return_sequences=True, dropout=0.2),
            LayerNormalization(),
            
            LSTM(32, return_sequences=False, dropout=0.2),
            LayerNormalization(),
            
            Dense(64, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.3),
            
            Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
            Dropout(0.3),
            
            # ì¶œë ¥: (5ì¼ ì˜ˆì¸¡ * 3ê°œ íƒ€ê²Ÿ)
            Dense(self.prediction_length * self.num_targets, activation='linear'),
            tf.keras.layers.Reshape((self.prediction_length, self.num_targets))
        ])
        
        return model
    
    def _build_lstm_with_attention(self) -> Model:
        """Attention ë©”ì»¤ë‹ˆì¦˜ì´ ìˆëŠ” LSTM ëª¨ë¸"""
        # ì…ë ¥
        inputs = Input(shape=(self.sequence_length, self.num_features))
        
        # LSTM ë ˆì´ì–´ë“¤
        lstm1 = LSTM(128, return_sequences=True, dropout=0.2)(inputs)  # cuDNN ìµœì í™”
        lstm1_norm = LayerNormalization()(lstm1)
        
        lstm2 = LSTM(64, return_sequences=True, dropout=0.2)(lstm1_norm)
        lstm2_norm = LayerNormalization()(lstm2)
        
        # Multi-Head Attention
        attention = MultiHeadAttention(
            num_heads=8, 
            key_dim=64,
            dropout=0.1
        )(lstm2_norm, lstm2_norm)
        
        # Attentionê³¼ LSTM ì¶œë ¥ ê²°í•©
        attention_norm = LayerNormalization()(attention)
        combined = tf.keras.layers.Add()([lstm2_norm, attention_norm])
        
        # Global Average Pooling
        pooled = tf.keras.layers.GlobalAveragePooling1D()(combined)
        
        # Dense ë ˆì´ì–´ë“¤
        dense1 = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(pooled)
        dropout1 = Dropout(0.3)(dense1)
        
        dense2 = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dropout1)
        dropout2 = Dropout(0.3)(dense2)
        
        # ì¶œë ¥
        outputs = Dense(self.prediction_length * self.num_targets, activation='linear')(dropout2)
        outputs_reshaped = tf.keras.layers.Reshape((self.prediction_length, self.num_targets))(outputs)
        
        model = Model(inputs=inputs, outputs=outputs_reshaped)
        return model
    
    def _build_lstm_ensemble(self) -> Model:
        """ì•™ìƒë¸” LSTM ëª¨ë¸"""
        inputs = Input(shape=(self.sequence_length, self.num_features))
        
        # ì—¬ëŸ¬ LSTM ë¸Œëœì¹˜
        branches = []
        
        # ë¸Œëœì¹˜ 1: ì§§ì€ ê¸°ê°„ íŒ¨í„´
        branch1 = LSTM(64, return_sequences=True, dropout=0.2)(inputs)
        branch1 = LSTM(32, return_sequences=False, dropout=0.2)(branch1)
        branches.append(branch1)
        
        # ë¸Œëœì¹˜ 2: ì¤‘ê°„ ê¸°ê°„ íŒ¨í„´  
        branch2 = LSTM(96, return_sequences=True, dropout=0.2)(inputs)
        branch2 = LSTM(48, return_sequences=False, dropout=0.2)(branch2)
        branches.append(branch2)
        
        # ë¸Œëœì¹˜ 3: ê¸´ ê¸°ê°„ íŒ¨í„´
        branch3 = LSTM(128, return_sequences=True, dropout=0.2)(inputs)
        branch3 = LSTM(64, return_sequences=False, dropout=0.2)(branch3)
        branches.append(branch3)
        
        # ë¸Œëœì¹˜ ê²°í•©
        combined = tf.keras.layers.Concatenate()(branches)
        combined_norm = LayerNormalization()(combined)
        
        # ìµœì¢… Dense ë ˆì´ì–´ë“¤
        dense1 = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined_norm)
        dropout1 = Dropout(0.3)(dense1)
        
        dense2 = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(dropout1)
        dropout2 = Dropout(0.3)(dense2)
        
        # ì¶œë ¥
        outputs = Dense(self.prediction_length * self.num_targets, activation='linear')(dropout2)
        outputs_reshaped = tf.keras.layers.Reshape((self.prediction_length, self.num_targets))(outputs)
        
        model = Model(inputs=inputs, outputs=outputs_reshaped)
        return model
    
    def train(self, 
              X_train: np.ndarray, 
              y_train: np.ndarray,
              X_val: np.ndarray = None,
              y_val: np.ndarray = None,
              epochs: int = 100,
              batch_size: int = 32,
              save_path: str = "models/") -> Dict:
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            X_train: í•™ìŠµ ë°ì´í„° ì…ë ¥
            y_train: í•™ìŠµ ë°ì´í„° íƒ€ê²Ÿ
            X_val: ê²€ì¦ ë°ì´í„° ì…ë ¥
            y_val: ê²€ì¦ ë°ì´í„° íƒ€ê²Ÿ
            epochs: í•™ìŠµ ì—í¬í¬
            batch_size: ë°°ì¹˜ í¬ê¸°
            save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            
        Returns:
            í•™ìŠµ íˆìŠ¤í† ë¦¬
        """
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor='val_loss' if X_val is not None else 'loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss' if X_val is not None else 'loss',
                factor=0.5,
                patience=10,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ìƒì„±
        os.makedirs(save_path, exist_ok=True)
        model_save_path = os.path.join(save_path, "best_model.keras")
        
        callbacks.append(
            ModelCheckpoint(
                model_save_path,
                monitor='val_loss' if X_val is not None else 'loss',
                save_best_only=True,
                verbose=1
            )
        )
        
        self.logger.info(f"Starting training with {len(X_train)} samples")
        
        # ğŸš€ GPU ê°•ì œ ì‚¬ìš©ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ
        with tf.device('/GPU:0'):
            validation_data = (X_val, y_val) if X_val is not None and y_val is not None else None
            
            self.history = self.model.fit(
                X_train, y_train,
                validation_data=validation_data,
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=1,
                shuffle=True
            )
            
        self.logger.info("ğŸš€ Training completed with GPU enforcement!")
        
        self.logger.info("Training completed")
        
        # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
        history_path = os.path.join(save_path, "training_history.pkl")
        with open(history_path, 'wb') as f:
            pickle.dump(self.history.history, f)
        
        return self.history.history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            X: ì…ë ¥ ì‹œí€€ìŠ¤ ë°ì´í„°
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ (batch_size, 5ì¼, 3ê°œ íƒ€ê²Ÿ)
        """
        if self.model is None:
            raise ValueError("Model not trained. Train the model first.")
        
        # ğŸš€ GPU ê°•ì œ ì‚¬ìš©ìœ¼ë¡œ ì˜ˆì¸¡
        with tf.device('/GPU:0'):
            predictions = self.model.predict(X, verbose=0)
        return predictions
    
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """
        ëª¨ë¸ í‰ê°€
        
        Args:
            X_test: í…ŒìŠ¤íŠ¸ ì…ë ¥ ë°ì´í„°
            y_test: í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„°
            
        Returns:
            í‰ê°€ ì§€í‘œ
        """
        predictions = self.predict(X_test)
        
        # ì „ì²´ í‰ê°€
        mse = mean_squared_error(y_test.reshape(-1), predictions.reshape(-1))
        mae = mean_absolute_error(y_test.reshape(-1), predictions.reshape(-1))
        
        # íƒ€ê²Ÿë³„ í‰ê°€ (Close, BB_Upper, BB_Lower)
        target_names = ['Close', 'BB_Upper', 'BB_Lower']
        target_metrics = {}
        
        for i, target_name in enumerate(target_names):
            target_true = y_test[:, :, i].reshape(-1)
            target_pred = predictions[:, :, i].reshape(-1)
            
            target_mse = mean_squared_error(target_true, target_pred)
            target_mae = mean_absolute_error(target_true, target_pred)
            
            target_metrics[f'{target_name}_MSE'] = target_mse
            target_metrics[f'{target_name}_MAE'] = target_mae
        
        metrics = {
            'Overall_MSE': mse,
            'Overall_MAE': mae,
            **target_metrics
        }
        
        self.logger.info(f"Evaluation metrics: {metrics}")
        return metrics
    
    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            raise ValueError("No model to save")
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        self.model.save(path)
        self.logger.info(f"Model saved to {path}")
    
    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ"""
        self.model = tf.keras.models.load_model(path)
        self.logger.info(f"Model loaded from {path}")
    
    def plot_training_history(self, save_path: str = None):
        """í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”"""
        if self.history is None:
            self.logger.warning("No training history available")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # Loss í”Œë¡¯
        ax1.plot(self.history.history['loss'], label='Training Loss')
        if 'val_loss' in self.history.history:
            ax1.plot(self.history.history['val_loss'], label='Validation Loss')
        ax1.set_title('Model Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        
        # MAE í”Œë¡¯
        ax2.plot(self.history.history['mae'], label='Training MAE')
        if 'val_mae' in self.history.history:
            ax2.plot(self.history.history['val_mae'], label='Validation MAE')
        ax2.set_title('Model MAE')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('MAE')
        ax2.legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path)
            self.logger.info(f"Training history plot saved to {save_path}")
        
        plt.show()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    logging.basicConfig(level=logging.INFO)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    batch_size = 100
    sequence_length = 60
    num_features = 18
    prediction_length = 5
    num_targets = 3
    
    X_sample = np.random.randn(batch_size, sequence_length, num_features)
    y_sample = np.random.randn(batch_size, prediction_length, num_targets)
    
    # ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸
    model = StockLSTMModel(
        sequence_length=sequence_length,
        prediction_length=prediction_length,
        num_features=num_features,
        num_targets=num_targets
    )
    
    # ëª¨ë¸ êµ¬ì¶•
    keras_model = model.build_model("lstm_attention")
    print(f"Model built successfully: {keras_model.summary()}")
    
    # ê°„ë‹¨í•œ í•™ìŠµ í…ŒìŠ¤íŠ¸
    history = model.train(X_sample[:80], y_sample[:80], 
                         X_sample[80:], y_sample[80:], 
                         epochs=2, batch_size=16)
    
    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    predictions = model.predict(X_sample[:10])
    print(f"Prediction shape: {predictions.shape}")
    
    # í‰ê°€ í…ŒìŠ¤íŠ¸
    metrics = model.evaluate(X_sample[80:], y_sample[80:])
    print(f"Evaluation metrics: {metrics}")
    
    print("LSTM model test completed successfully!")