"""
ì¶”ë¡  íŒŒì´í”„ë¼ì¸
ì‚¬ìš©ì ì…ë ¥ ì¢…ëª©ì˜ ìµœê·¼ 60ì¼ ë°ì´í„° ìˆ˜ì§‘ ë° ì¶”ë¡  ì‹¤í–‰
"""

import os
import logging
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import json
from dataclasses import dataclass, asdict
import asyncio
from concurrent.futures import ThreadPoolExecutor

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from data_collector import StockDataCollector
from data_preprocessor import StockDataPreprocessor
from pytorch_lstm_model import PyTorchStockLSTM
from config import get_model_paths

@dataclass
class PredictionOutput:
    """ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    symbol: str
    prediction_timestamp: str
    current_date: str
    current_price: float
    current_volume: int
    
    # í˜„ì¬ ê¸°ìˆ ì  ì§€í‘œ
    current_ma_5: float
    current_ma_20: float
    current_ma_60: float
    current_bb_upper: float
    current_bb_middle: float
    current_bb_lower: float
    current_rsi: float
    
    # 5ì¼ ì˜ˆì¸¡ ê²°ê³¼
    predicted_prices: List[float]  # 5ì¼ê°„ ì¢…ê°€ ì˜ˆì¸¡
    predicted_bb_upper: List[float]  # 5ì¼ê°„ ë³¼ë¦°ì € ë°´ë“œ ìƒí•œ
    predicted_bb_lower: List[float]  # 5ì¼ê°„ ë³¼ë¦°ì € ë°´ë“œ í•˜í•œ
    predicted_dates: List[str]  # ì˜ˆì¸¡ ë‚ ì§œë“¤
    
    # ì‹ í˜¸ ë° ë¶„ì„
    signal_strength: float  # 0-1 ì‚¬ì´ì˜ ì‹ í˜¸ ê°•ë„
    trend_direction: str  # "up", "down", "sideways"
    volatility_level: str  # "low", "medium", "high"
    
    # ë©”íƒ€ë°ì´í„°
    confidence_score: float
    model_version: str
    data_quality_score: float

class InferencePipeline:
    def __init__(self, 
                 model_path: str = None,
                 preprocessor_path: str = None):
        """
        ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (Noneì‹œ í™˜ê²½ì— ë”°ë¼ ìë™ ì„¤ì •)
            preprocessor_path: ì „ì²˜ë¦¬ê¸° ê²½ë¡œ (Noneì‹œ í™˜ê²½ì— ë”°ë¼ ìë™ ì„¤ì •)
        """
        # ê²½ë¡œ ìë™ ì„¤ì • (RunPod í™˜ê²½ ê³ ë ¤)
        if model_path is None or preprocessor_path is None:
            auto_model_path, auto_preprocessor_path = get_model_paths()
            self.model_path = model_path or auto_model_path
            self.preprocessor_path = preprocessor_path or auto_preprocessor_path
        else:
            self.model_path = model_path
            self.preprocessor_path = preprocessor_path
        self.logger = logging.getLogger(__name__)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.model = None
        self.preprocessor = None
        self.data_collector = StockDataCollector()
        
        # ëª¨ë¸ ë²„ì „ ì •ë³´
        self.model_version = "v1.0.0"
        
        # ë¡œë“œ
        self.load_components()
    
    def load_components(self):
        """ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ë¡œë“œ"""
        try:
            # ğŸš€ PyTorch ëª¨ë¸ ë¡œë“œ
            if os.path.exists(self.model_path):
                self.model = PyTorchStockLSTM(
                    sequence_length=60,
                    prediction_length=5,
                    num_features=18,
                    num_targets=3
                )
                self.model.load_model(self.model_path, hidden_size=512)  # ğŸ”¥ RTX 4090 ìµœì í™”
                self.logger.info("ğŸš€ PyTorch model loaded successfully")
            else:
                raise FileNotFoundError(f"Model file not found: {self.model_path}")
            
            # ì „ì²˜ë¦¬ê¸° ë¡œë“œ
            if os.path.exists(self.preprocessor_path):
                with open(self.preprocessor_path, 'rb') as f:
                    self.preprocessor = pickle.load(f)
                self.logger.info("Preprocessor loaded successfully")
            else:
                raise FileNotFoundError(f"Preprocessor file not found: {self.preprocessor_path}")
                
        except Exception as e:
            self.logger.error(f"Error loading components: {str(e)}")
            raise
    
    def collect_stock_data(self, symbol: str, days: int = 60) -> Optional[pd.DataFrame]:
        """
        ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            symbol: ì£¼ì‹ ì‹¬ë³¼
            days: ìˆ˜ì§‘í•  ì¼ìˆ˜
            
        Returns:
            ìˆ˜ì§‘ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            data = self.data_collector.get_recent_data(symbol, days)
            if data is None or len(data) < days:
                self.logger.warning(f"Insufficient data for {symbol}: got {len(data) if data is not None else 0} days, need {days}")
                return None
            
            return data
            
        except Exception as e:
            self.logger.error(f"Error collecting data for {symbol}: {str(e)}")
            return None
    
    def calculate_data_quality_score(self, data: pd.DataFrame) -> float:
        """
        ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        
        Args:
            data: ì£¼ì‹ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            í’ˆì§ˆ ì ìˆ˜ (0-1)
        """
        quality_score = 1.0
        
        # ê²°ì¸¡ê°’ í™•ì¸
        missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
        quality_score *= (1 - missing_ratio)
        
        # ë°ì´í„° ê¸¸ì´ í™•ì¸
        expected_length = 60
        length_ratio = min(len(data) / expected_length, 1.0)
        quality_score *= length_ratio
        
        # ê°€ê²© ë°ì´í„°ì˜ ì¼ê´€ì„± í™•ì¸
        price_consistency = 1.0
        if 'High' in data.columns and 'Low' in data.columns and 'Close' in data.columns:
            # High >= Low, High >= Close, Low <= Close í™•ì¸
            inconsistent_count = ((data['High'] < data['Low']) | 
                                (data['High'] < data['Close']) | 
                                (data['Low'] > data['Close'])).sum()
            price_consistency = 1 - (inconsistent_count / len(data))
        
        quality_score *= price_consistency
        
        return max(0.0, min(1.0, quality_score))
    
    
    def calculate_volatility_level(self, data: pd.DataFrame) -> str:
        """
        ë³€ë™ì„± ìˆ˜ì¤€ ê³„ì‚°
        
        Args:
            data: ì£¼ì‹ ë°ì´í„°í”„ë ˆì„
            
        Returns:
            ë³€ë™ì„± ìˆ˜ì¤€ ("low", "medium", "high")
        """
        if 'Volatility' in data.columns:
            current_volatility = data['Volatility'].iloc[-1]
            volatility_percentile = data['Volatility'].rank(pct=True).iloc[-1]
            
            if volatility_percentile < 0.33:
                return "low"
            elif volatility_percentile < 0.67:
                return "medium"
            else:
                return "high"
        
        # Volatility ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ê°€ê²© ë³€ë™ë¥ ë¡œ ê³„ì‚°
        price_changes = data['Close'].pct_change().abs()
        avg_volatility = price_changes.mean()
        
        if avg_volatility < 0.02:  # 2% ë¯¸ë§Œ
            return "low"
        elif avg_volatility < 0.04:  # 4% ë¯¸ë§Œ
            return "medium"
        else:
            return "high"
    
    def run_inference(self, symbol: str, days: int = 60) -> Optional[PredictionOutput]:
        """
        ë‹¨ì¼ ì¢…ëª©ì— ëŒ€í•œ ì¶”ë¡  ì‹¤í–‰
        
        Args:
            symbol: ì£¼ì‹ ì‹¬ë³¼
            days: ì‚¬ìš©í•  ê³¼ê±° ë°ì´í„° ì¼ìˆ˜
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
        """
        try:
            self.logger.info(f"Running inference for {symbol}")
            
            # 1. ë°ì´í„° ìˆ˜ì§‘
            raw_data = self.collect_stock_data(symbol, days)
            if raw_data is None:
                return None
            
            # 2. ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            data_quality_score = self.calculate_data_quality_score(raw_data)
            
            # 3. ì „ì²˜ë¦¬
            processed_data = self.preprocessor.preprocess_data(raw_data)
            
            # 4. ì¶”ë¡ ìš© ì‹œí€€ìŠ¤ ìƒì„±
            input_sequence = self.preprocessor.preprocess_for_inference(processed_data)
            
            # 5. ëª¨ë¸ ì¶”ë¡ 
            predictions = self.model.predict(input_sequence)
            
         
            
            # 7. ë³€ë™ì„± ìˆ˜ì¤€ ê³„ì‚°
            volatility_level = self.calculate_volatility_level(processed_data)
            
            # 8. í˜„ì¬ ë°ì´í„° ì¶”ì¶œ
            current_row = processed_data.iloc[-1]
            
            # 9. ì˜ˆì¸¡ ë‚ ì§œ ìƒì„±
            base_date = datetime.now()
            predicted_dates = [(base_date + timedelta(days=i+1)).strftime("%Y-%m-%d") for i in range(5)]
            
            # 10. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (ë°ì´í„° í’ˆì§ˆê³¼ ëª¨ë¸ ì„±ëŠ¥ ê¸°ë°˜)
            confidence_score = data_quality_score * 0.8  # ê¸°ë³¸ ì‹ ë¢°ë„
            
            # 11. ê²°ê³¼ ê°ì²´ ìƒì„±
            result = PredictionOutput(
                symbol=symbol,
                prediction_timestamp=datetime.now().isoformat(),
                current_date=base_date.strftime("%Y-%m-%d"),
                current_price=float(current_row['Close']),
                current_volume=int(current_row['Volume']),
                
                # í˜„ì¬ ê¸°ìˆ ì  ì§€í‘œ
                current_ma_5=float(current_row.get('MA_5', 0)),
                current_ma_20=float(current_row.get('MA_20', 0)),
                current_ma_60=float(current_row.get('MA_60', 0)),
                current_bb_upper=float(current_row.get('BB_Upper', 0)),
                current_bb_middle=float(current_row.get('BB_Middle', 0)),
                current_bb_lower=float(current_row.get('BB_Lower', 0)),
                current_rsi=float(current_row.get('RSI', 50)),
                
                # 5ì¼ ì˜ˆì¸¡ ê²°ê³¼
                predicted_prices=predictions[0, :, 0].tolist(),
                predicted_bb_upper=predictions[0, :, 1].tolist(),
                predicted_bb_lower=predictions[0, :, 2].tolist(),
                predicted_dates=predicted_dates,
                
                # ë©”íƒ€ë°ì´í„°
                confidence_score=float(confidence_score),
                model_version=self.model_version,
                data_quality_score=float(data_quality_score)
            )
            
            
        except Exception as e:
            self.logger.error(f"Error during inference for {symbol}: {str(e)}")
            return None
    
    def run_batch_inference(self, symbols: List[str], days: int = 60, max_workers: int = 5) -> List[PredictionOutput]:
        """
        ì—¬ëŸ¬ ì¢…ëª©ì— ëŒ€í•œ ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰
        
        Args:
            symbols: ì£¼ì‹ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸
            days: ì‚¬ìš©í•  ê³¼ê±° ë°ì´í„° ì¼ìˆ˜
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        self.logger.info(f"Running batch inference for {len(symbols)} symbols")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ì‘ì—… ì œì¶œ
            future_to_symbol = {
                executor.submit(self.run_inference, symbol, days): symbol 
                for symbol in symbols
            }
            
            # ê²°ê³¼ ìˆ˜ì§‘
            from concurrent.futures import as_completed
            for future in as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                try:
                    result = future.result()
                    if result is not None:
                        results.append(result)
                except Exception as e:
                    self.logger.error(f"Error in batch inference for {symbol}: {str(e)}")
        
        self.logger.info(f"Batch inference completed: {len(results)}/{len(symbols)} successful")
        return results
    
    def save_results_to_json(self, results: List[PredictionOutput], filepath: str):
        """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            results_dict = [asdict(result) for result in results]
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Results saved to {filepath}")
        except Exception as e:
            self.logger.error(f"Error saving results: {str(e)}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Stock Inference Pipeline")
    parser.add_argument("--symbols", nargs="+", required=True, help="Stock symbols to predict")
    parser.add_argument("--days", type=int, default=60, help="Days of historical data to use")
    parser.add_argument("--output", default="predictions.json", help="Output JSON file")
    parser.add_argument("--batch-size", type=int, default=5, help="Batch size for parallel processing")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = InferencePipeline()
    
    if len(args.symbols) == 1:
        # ë‹¨ì¼ ì¶”ë¡ 
        result = pipeline.run_inference(args.symbols[0], args.days)
        if result:
            pipeline.save_results_to_json([result], args.output)
            print(f"Prediction completed for {args.symbols[0]}")
        else:
            print(f"Failed to generate prediction for {args.symbols[0]}")
    else:
        # ë°°ì¹˜ ì¶”ë¡ 
        results = pipeline.run_batch_inference(args.symbols, args.days, args.batch_size)
        if results:
            pipeline.save_results_to_json(results, args.output)
            print(f"Batch prediction completed: {len(results)}/{len(args.symbols)} successful")
            
            # ì‹ í˜¸ ìš”ì•½

        else:
            print("No successful predictions")


if __name__ == "__main__":
    main()