"""
GPU ìµœì í™”ëœ PyTorch LSTM ëª¨ë¸
RTX 4090ì„ 100% í™œìš©í•˜ë„ë¡ ì„¤ê³„
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
from typing import Tuple, Dict, List, Optional
import logging
import pickle
import os
import time
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

class OptimizedStockLSTM(nn.Module):
    def __init__(self, 
                 input_size: int = 18,
                 hidden_size: int = 256,  # ë” í° hidden size
                 num_layers: int = 3,     # ë” ë§ì€ ë ˆì´ì–´
                 output_size: int = 15,
                 dropout: float = 0.2):
        super(OptimizedStockLSTM, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        
        # ğŸ”¥ ë” í° LSTM ìŠ¤íƒ (GPU ì‚¬ìš©ë¥  ì¦ê°€)
        self.lstm1 = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True,
            dropout=0,
            bidirectional=False
        )
        
        self.dropout1 = nn.Dropout(dropout)
        self.layernorm1 = nn.LayerNorm(hidden_size)
        
        self.lstm2 = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True,
            dropout=0,
            bidirectional=False
        )
        
        self.dropout2 = nn.Dropout(dropout)
        self.layernorm2 = nn.LayerNorm(hidden_size)
        
        # ì¶”ê°€ LSTM ë ˆì´ì–´ (GPU ì‚¬ìš©ë¥  ë” ì¦ê°€)
        self.lstm3 = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size//2,
            num_layers=1,
            batch_first=True,
            dropout=0,
            bidirectional=False
        )
        
        self.dropout3 = nn.Dropout(dropout)
        self.layernorm3 = nn.LayerNorm(hidden_size//2)
        
        # ğŸ”¥ ë” í° Dense ë ˆì´ì–´ë“¤ (GPU ì—°ì‚° ì¦ê°€)
        self.fc1 = nn.Linear(hidden_size//2, hidden_size)
        self.dropout4 = nn.Dropout(dropout)
        
        self.fc2 = nn.Linear(hidden_size, hidden_size//2)
        self.dropout5 = nn.Dropout(dropout)
        
        self.fc3 = nn.Linear(hidden_size//2, 64)
        self.dropout6 = nn.Dropout(dropout)
        
        self.output_layer = nn.Linear(64, output_size)
        
        # í™œì„±í™” í•¨ìˆ˜
        self.relu = nn.ReLU()
        self.gelu = nn.GELU()  # ë” ë³µì¡í•œ í™œì„±í™” í•¨ìˆ˜
        
    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        
        # LSTM 1
        lstm1_out, _ = self.lstm1(x)
        lstm1_out = self.dropout1(lstm1_out)
        lstm1_out = self.layernorm1(lstm1_out)
        
        # LSTM 2
        lstm2_out, _ = self.lstm2(lstm1_out)
        lstm2_out = self.dropout2(lstm2_out)
        lstm2_out = self.layernorm2(lstm2_out)
        
        # LSTM 3
        lstm3_out, _ = self.lstm3(lstm2_out)
        lstm3_out = self.dropout3(lstm3_out)
        lstm3_out = self.layernorm3(lstm3_out)
        
        # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ë§Œ ì‚¬ìš©
        last_output = lstm3_out[:, -1, :]
        
        # ğŸ”¥ ë” ë§ì€ Dense ì—°ì‚° (GPU ì‚¬ìš©ë¥  ì¦ê°€)
        x = self.gelu(self.fc1(last_output))
        x = self.dropout4(x)
        
        x = self.relu(self.fc2(x))
        x = self.dropout5(x)
        
        x = self.relu(self.fc3(x))
        x = self.dropout6(x)
        
        # ì¶œë ¥
        output = self.output_layer(x)
        
        # (batch_size, 15) -> (batch_size, 5, 3) í˜•íƒœë¡œ ë³€í™˜
        output = output.view(-1, 5, 3)
        
        return output

class OptimizedPyTorchStockLSTM:
    def __init__(self, 
                 sequence_length: int = 60,
                 prediction_length: int = 5,
                 num_features: int = 18,
                 num_targets: int = 3,
                 device: str = None):
        
        self.sequence_length = sequence_length
        self.prediction_length = prediction_length
        self.num_features = num_features
        self.num_targets = num_targets
        
        # GPU ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ğŸš€ Using device: {self.device}")
        if self.device.type == 'cuda':
            print(f"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.scheduler = None  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€
        self.history = {'train_loss': [], 'val_loss': []}
        
        self.logger = logging.getLogger(__name__)
        
    def build_model(self, 
                   hidden_size: int = 256,  # ê¸°ë³¸ê°’ ì¦ê°€
                   num_layers: int = 3,     # ê¸°ë³¸ê°’ ì¦ê°€
                   dropout: float = 0.2):
        """GPU ìµœì í™”ëœ ëª¨ë¸ êµ¬ì¶•"""
        
        output_size = self.prediction_length * self.num_targets
        
        self.model = OptimizedStockLSTM(
            input_size=self.num_features,
            hidden_size=hidden_size,
            num_layers=num_layers,
            output_size=output_size,
            dropout=dropout
        ).to(self.device)  # GPUë¡œ ì´ë™
        
        # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤í•¨ìˆ˜
        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=1e-5)
        self.criterion = nn.MSELoss()
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=10, verbose=True
        )
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ—ï¸ Optimized model built with {total_params:,} trainable parameters")
        
        return self.model
    
    def train_model(self,
                   X_train: np.ndarray,
                   y_train: np.ndarray,
                   X_val: np.ndarray = None,
                   y_val: np.ndarray = None,
                   epochs: int = 100,
                   batch_size: int = 64,    # ë” í° ë°°ì¹˜ í¬ê¸°
                   patience: int = 15,
                   num_workers: int = 4):   # DataLoader ìµœì í™”
        """GPU ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ"""
        
        if self.model is None:
            raise ValueError("Model not built. Call build_model() first.")
        
        print(f"ğŸ”¥ GPU ìµœì í™” í•™ìŠµ ì‹œì‘!")
        print(f"ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"ğŸ’¾ ì˜ˆìƒ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~{batch_size * self.sequence_length * self.num_features * 4 / 1024**3:.2f}GB")
        
        # ğŸ”¥ ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™ (í•œ ë²ˆì— ëª¨ë“  ë°ì´í„°)
        print("ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™ ì¤‘...")
        X_train_tensor = torch.FloatTensor(X_train).to(self.device)
        y_train_tensor = torch.FloatTensor(y_train).to(self.device)
        
        print(f"âœ… í•™ìŠµ ë°ì´í„° GPU ì´ë™ ì™„ë£Œ: X={X_train_tensor.device}, y={y_train_tensor.device}")
        print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
        
        # DataLoader ì„¤ì • (GPU ìµœì í™”)
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0,  # GPUì—ì„œëŠ” 0ì´ ë” ë¹ ë¦„
            pin_memory=False  # ì´ë¯¸ GPUì— ìˆìœ¼ë¯€ë¡œ False
        )
        
        # ê²€ì¦ ë°ì´í„° ì¤€ë¹„
        val_loader = None
        if X_val is not None and y_val is not None:
            X_val_tensor = torch.FloatTensor(X_val).to(self.device)
            y_val_tensor = torch.FloatTensor(y_val).to(self.device)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # í•™ìŠµ ì‹œì‘
        print(f"ğŸš€ Starting optimized training on {self.device}")
        print(f"ğŸ“Š Training samples: {len(X_train)}")
        print(f"ğŸ”¢ Total batches per epoch: {len(train_loader)}")
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            # í•™ìŠµ ëª¨ë“œ
            self.model.train()
            train_loss = 0.0
            
            # ğŸ”¥ GPU ì‚¬ìš© ìƒíƒœ ì¶œë ¥ (ì²« epochì—ë§Œ)
            if epoch == 0 and self.device.type == 'cuda':
                print(f"ğŸš€ GPU í•™ìŠµ ì‹œì‘ - ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            
            batch_count = 0
            for batch_X, batch_y in train_loader:
                batch_count += 1
                
                # ğŸ”¥ ì²« ë°°ì¹˜ì—ì„œ ë””ë°”ì´ìŠ¤ í™•ì¸
                if epoch == 0 and batch_count == 1:
                    print(f"ğŸ“Š Batch X device: {batch_X.device}")
                    print(f"ğŸ“Š Batch y device: {batch_y.device}")
                    print(f"ğŸ“Š Model device: {next(self.model.parameters()).device}")
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
                self.optimizer.zero_grad()
                
                # ìˆœì „íŒŒ
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                
                # ğŸ”¥ ì²« ë°°ì¹˜ì—ì„œ ì¶œë ¥ í™•ì¸
                if epoch == 0 and batch_count == 1:
                    print(f"ğŸ“Š Outputs device: {outputs.device}")
                    print(f"ğŸ“Š Loss device: {loss.device}")
                
                # ì—­ì „íŒŒ
                loss.backward()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                self.optimizer.step()
                
                train_loss += loss.item()
                
                # ğŸ”¥ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ê¸°ì  ì¶œë ¥
                if batch_count % 20 == 0 and self.device.type == 'cuda':
                    print(f"  Batch {batch_count}/{len(train_loader)}: GPU ë©”ëª¨ë¦¬ {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
            
            avg_train_loss = train_loss / len(train_loader)
            self.history['train_loss'].append(avg_train_loss)
            
            # ê²€ì¦
            val_loss = 0.0
            if val_loader is not None:
                self.model.eval()
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        outputs = self.model(batch_X)
                        loss = self.criterion(outputs, batch_y)
                        val_loss += loss.item()
                
                avg_val_loss = val_loss / len(val_loader)
                self.history['val_loss'].append(avg_val_loss)
                
                # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                self.scheduler.step(avg_val_loss)
                
                epoch_time = time.time() - epoch_start_time
                print(f"Epoch {epoch+1}/{epochs} ({epoch_time:.1f}s) - "
                      f"Train Loss: {avg_train_loss:.6f} - "
                      f"Val Loss: {avg_val_loss:.6f}")
                
                # Early stopping
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"Early stopping at epoch {epoch+1}")
                        break
            else:
                epoch_time = time.time() - epoch_start_time
                print(f"Epoch {epoch+1}/{epochs} ({epoch_time:.1f}s) - Train Loss: {avg_train_loss:.6f}")
        
        print("ğŸ‰ GPU ìµœì í™” í•™ìŠµ ì™„ë£Œ!")
        print(f"ìµœì¢… GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
        return self.history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """GPU ìµœì í™”ëœ ì˜ˆì¸¡"""
        if self.model is None:
            raise ValueError("Model not trained. Train the model first.")
        
        self.model.eval()
        
        # GPUë¡œ ì´ë™
        X_tensor = torch.FloatTensor(X).to(self.device)
        
        with torch.no_grad():
            predictions = self.model(X_tensor)
        
        # CPUë¡œ ì´ë™í•˜ê³  numpyë¡œ ë³€í™˜
        return predictions.cpu().numpy()
    
    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """ëª¨ë¸ í‰ê°€"""
        predictions = self.predict(X_test)
        
        # ì „ì²´ í‰ê°€
        mse = mean_squared_error(y_test.reshape(-1), predictions.reshape(-1))
        mae = mean_absolute_error(y_test.reshape(-1), predictions.reshape(-1))
        
        # íƒ€ê²Ÿë³„ í‰ê°€
        target_names = ['Close', 'BB_Upper', 'BB_Lower']
        metrics = {'Overall_MSE': mse, 'Overall_MAE': mae}
        
        for i, target_name in enumerate(target_names):
            target_true = y_test[:, :, i].reshape(-1)
            target_pred = predictions[:, :, i].reshape(-1)
            
            target_mse = mean_squared_error(target_true, target_pred)
            target_mae = mean_absolute_error(target_true, target_pred)
            
            metrics[f'{target_name}_MSE'] = target_mse
            metrics[f'{target_name}_MAE'] = target_mae
        
        return metrics
    
    def save_model(self, save_path: str):
        """ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            raise ValueError("No model to save")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'history': self.history,
            'model_config': {
                'sequence_length': self.sequence_length,
                'prediction_length': self.prediction_length,
                'num_features': self.num_features,
                'num_targets': self.num_targets
            }
        }, save_path)
        
        print(f"ğŸ’¾ Optimized model saved to {save_path}")
    
    def load_model(self, load_path: str, hidden_size: int = 256):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(load_path, map_location=self.device)
        
        # ëª¨ë¸ êµ¬ì¶•
        self.build_model(hidden_size=hidden_size)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.history = checkpoint['history']
        
        print(f"ğŸ“¥ Optimized model loaded from {load_path}")

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=== GPU ìµœì í™” LSTM ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    
    # ë” í° ë”ë¯¸ ë°ì´í„° ìƒì„±
    batch_size = 500   # ë” í° ë°°ì¹˜
    sequence_length = 60
    num_features = 18
    
    X_dummy = np.random.randn(batch_size, sequence_length, num_features).astype(np.float32)
    y_dummy = np.random.randn(batch_size, 5, 3).astype(np.float32)
    
    print(f"ğŸ“Š Test data shape: X={X_dummy.shape}, y={y_dummy.shape}")
    
    # GPU ìµœì í™” ëª¨ë¸ ìƒì„±
    model = OptimizedPyTorchStockLSTM()
    model.build_model(hidden_size=256, num_layers=3)
    
    print("\nğŸš€ GPU ìµœì í™” í•™ìŠµ ì‹œì‘...")
    print("ğŸ’¡ nvidia-smië¡œ GPU ì‚¬ìš©ë¥  í™•ì¸í•˜ì„¸ìš”!")
    
    history = model.train_model(
        X_dummy, y_dummy, 
        epochs=10, 
        batch_size=128  # í° ë°°ì¹˜ í¬ê¸°
    )
    
    print("\nğŸ”® ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸...")
    predictions = model.predict(X_dummy[:50])
    print(f"ì˜ˆì¸¡ ê²°ê³¼: {predictions.shape}")
    
    print("\nâœ… GPU ìµœì í™” LSTM í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("GPU ì‚¬ìš©ë¥ ì´ 90% ì´ìƒ ì˜¬ë¼ê°”ë‚˜ìš”? ğŸ”¥")