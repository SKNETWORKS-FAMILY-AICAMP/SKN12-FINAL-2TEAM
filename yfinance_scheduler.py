import yfinance as yf
import pandas as pd
import hashlib
import json
import os
import schedule
import time
from datetime import datetime

# --- ì„¤ì • ---
# íŒŒì¼ ê²½ë¡œ ì„¤ì •
HASH_FILE = "news_hashes.json"      # í•´ì‹œê°’ ì €ì¥ íŒŒì¼
YAHOO_NEWS_FILE = "yahoo_finance_news.json"  # Yahoo Finance ë‰´ìŠ¤ íŒŒì¼ (ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸)

class NewsDeduplicator:
    """ë‰´ìŠ¤ ì¤‘ë³µ ì œê±° í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”: ê¸°ì¡´ í•´ì‹œê°’ ë° ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ"""
        self.current_dir = os.path.dirname(os.path.abspath(__file__))
        self.hash_file_path = os.path.join(self.current_dir, HASH_FILE)
        self.yahoo_news_file_path = os.path.join(self.current_dir, YAHOO_NEWS_FILE)
        
        # ê¸°ì¡´ í•´ì‹œê°’ ë¡œë“œ
        self.existing_hashes = self._load_existing_hashes()
        print(f"ğŸ“ ê¸°ì¡´ í•´ì‹œê°’ {len(self.existing_hashes)}ê°œ ë¡œë“œ ì™„ë£Œ")
        
        # ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (yahoo_finance_news.jsonì—ì„œ)
        self.existing_news = self._load_existing_news()
        print(f"ğŸ“ ê¸°ì¡´ ë‰´ìŠ¤ {len(self.existing_news)}ê°œ ë¡œë“œ ì™„ë£Œ")
    
    def _load_existing_hashes(self):
        """ê¸°ì¡´ í•´ì‹œê°’ë“¤ì„ JSON íŒŒì¼ì—ì„œ ë¡œë“œ"""
        try:
            if os.path.exists(self.hash_file_path):
                with open(self.hash_file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('hashes', []))
            return set()
        except Exception as e:
            print(f"âš ï¸ í•´ì‹œ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return set()
    
    def _load_existing_news(self):
        """ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ yahoo_finance_news.json íŒŒì¼ì—ì„œ ë¡œë“œ"""
        try:
            if os.path.exists(self.yahoo_news_file_path):
                with open(self.yahoo_news_file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get('articles', [])
            return []
        except Exception as e:
            print(f"âš ï¸ ë‰´ìŠ¤ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    

    def _collect_latest_news(self):
        """yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ ì§ì ‘ ìˆ˜ì§‘"""
        print("ğŸ“¡ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        
        # Ticker ë¦¬ìŠ¤íŠ¸
        tickers = [ # ëŒ€í˜• í…Œí¬ì£¼ (FAANG+)
         "AAPL", "MSFT", "GOOG", "GOOGL", "AMZN", "META", "TSLA", "NVDA", "NFLX", "ORCL", "CRM", "ADBE", "INTC", "AMD", "CSCO", "IBM",
    
    # ë°˜ë„ì²´
    "QCOM", "AVGO", "TXN", "MU", "AMAT", "LRCX", "ADI", "MCHP", "KLAC", "MRVL",
    
    # ì£¼ìš” ETFë“¤
    "SPY", "QQQ", "VTI", "IWM", "VEA", "VWO", "EFA", "GLD", "SLV", "TLT", "HYG", "LQD", "XLF", "XLK", "XLE", "XLV", "XLI", "XLP", "XLU", "XLRE",
    
    # ê¸ˆìœµ (ì€í–‰, ë³´í—˜, í•€í…Œí¬)
    "JPM", "BAC", "WFC", "C", "GS", "MS", "BRK-B", "V", "MA", "PYPL", "SQ", "AXP", "USB", "PNC", "TFC", "COF",
    
    # í—¬ìŠ¤ì¼€ì–´/ì œì•½
    "JNJ", "PFE", "UNH", "ABBV", "MRK", "TMO", "ABT", "DHR", "BMY", "AMGN", "GILD", "BIIB", "CVS", "CI", "ANTM", "HUM",
    
    # ì†Œë¹„ì¬ (í•„ìˆ˜/ì„ì˜)
    "PG", "KO", "PEP", "WMT", "HD", "MCD", "NKE", "SBUX", "TGT", "LOW", "COST", "DIS", "BABA", "AMZN", "EBAY", "ETSY",
    
    # ì—ë„ˆì§€
    "XOM", "CVX", "COP", "EOG", "SLB", "PSX", "VLO", "KMI", "OKE", "WMB",
    
    # ì‚°ì—…ì¬/í•­ê³µ
    "BA", "CAT", "DE", "GE", "HON", "MMM", "UPS", "FDX", "LMT", "RTX", "AAL", "DAL", "UAL", "LUV",
    
    # í†µì‹ 
    "VZ", "T", "TMUS", "CHTR", "CMCSA", "DISH",
    
    # ìë™ì°¨
    "F", "GM", "RIVN", "LCID", "NIO", "XPEV", "LI",
    
    # ë¶€ë™ì‚° REITs
    "AMT", "PLD", "CCI", "EQIX", "SPG", "O", "WELL", "EXR", "AVB", "EQR",
    
    # ìœ í‹¸ë¦¬í‹°
    "NEE", "DUK", "SO", "D", "AEP", "EXC", "XEL", "SRE", "PEG", "ED",
    
    # ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´
    "DIS", "NFLX", "ROKU", "SPOT", "WBD", "PARA", "FOX", "FOXA",
    
    # ì¤‘êµ­ ADR
    "BABA", "JD", "PDD", "BIDU", "BILI", "DIDI", "TME",
    
    # ì•”í˜¸í™”í ê´€ë ¨
    "COIN", "MSTR", "RIOT", "MARA", "BITB", "IBIT",
    
    # ê¸°íƒ€ ì£¼ìš” ê¸°ì—…ë“¤
    "TSLA", "UBER", "LYFT", "SNAP", "TWTR", "ZOOM", "DOCU", "PLTR", "SNOW", "CRM", "WORK", "ZM", "PTON", "ARKK", "ARKG", "ARKW"
    ]         
        new_news_list = []
        
        for ticker_symbol in tickers:
            try:
                print(f"  -> {ticker_symbol} ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
                
                # yfinanceë¥¼ í†µí•´ ë‰´ìŠ¤ ìˆ˜ì§‘
                ticker = yf.Ticker(ticker_symbol)
                news_list = ticker.news
                
                if not news_list:
                    print(f"    â„¹ï¸ {ticker_symbol}: ë‰´ìŠ¤ ì—†ìŒ")
                    continue
                
                # ë‰´ìŠ¤ ë°ì´í„° íŒŒì‹±
                parsed_count = 0
                for news_item in news_list:
                    try:
                        if not news_item or not isinstance(news_item, dict):
                            continue
                        
                        content = news_item.get('content', {})
                        if not content or not isinstance(content, dict):
                            continue
                        
                        # ì œëª© ì¶”ì¶œ
                        title = content.get('title', '')
                        if not title or title == 'N/A':
                            continue
                        
                        # ê¸°íƒ€ ì •ë³´ ì¶”ì¶œ
                        provider = content.get('provider', {})
                        if not isinstance(provider, dict):
                            provider = {}
                        
                        click_url = content.get('clickThroughUrl', {})
                        if not isinstance(click_url, dict):
                            click_url = {}
                        
                        # ë‚ ì§œ íŒŒì‹±
                        pub_date_str = content.get('pubDate', '')
                        if pub_date_str:
                            try:
                                pub_date = pd.to_datetime(pub_date_str)
                                if pub_date.tz is None:
                                    pub_date = pub_date.tz_localize('UTC')
                                formatted_date = pub_date.strftime('%Y-%m-%d %H:%M')
                            except:
                                formatted_date = datetime.now().strftime('%Y-%m-%d %H:%M')
                        else:
                            formatted_date = datetime.now().strftime('%Y-%m-%d %H:%M')
                        
                        # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì„±
                        news_data = {
                            'ë‚ ì§œ': formatted_date,
                            'í‹°ì»¤': ticker_symbol,
                            'ì œëª©': title,
                            'ì–¸ë¡ ì‚¬': provider.get('displayName', 'N/A'),
                            'ë§í¬': click_url.get('url', 'N/A')
                        }
                        
                        new_news_list.append(news_data)
                        parsed_count += 1
                        
                    except Exception as e:
                        print(f"    âš ï¸ ë‰´ìŠ¤ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                
                print(f"    âœ… {ticker_symbol}: {parsed_count}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
                time.sleep(0.5)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                
            except Exception as e:
                print(f"    âŒ {ticker_symbol} ì˜¤ë¥˜: {e}")
                continue
        
        print(f"âœ… ì´ {len(new_news_list)}ê°œì˜ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return new_news_list
    
    def _generate_hash(self, title):
        """ë‰´ìŠ¤ ì œëª©ì„ MD5 í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜"""
        # 1. ì œëª©ì„ UTF-8ë¡œ ì¸ì½”ë”©í•œ í›„ MD5 í•´ì‹œ ìƒì„±
        return hashlib.md5(title.encode('utf-8')).hexdigest()
    
    def _save_hashes(self):
        """í•´ì‹œê°’ë“¤ì„ JSON íŒŒì¼ì— ì €ì¥"""
        try:
            hash_data = {
                'hashes': list(self.existing_hashes),
                'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_count': len(self.existing_hashes)
            }
            
            with open(self.hash_file_path, 'w', encoding='utf-8') as f:
                json.dump(hash_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ í•´ì‹œê°’ {len(self.existing_hashes)}ê°œ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ í•´ì‹œ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _save_news_to_yahoo_file(self):
        """ì¤‘ë³µ ì œê±°ëœ ë‰´ìŠ¤ë¥¼ yahoo_finance_news.json íŒŒì¼ì— ì €ì¥"""
        try:
            # 4. ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ (ë‚ ì§œ ê¸°ì¤€)
            sorted_news = sorted(self.existing_news, 
                                key=lambda x: x.get('ë‚ ì§œ', ''), 
                                reverse=True)
            
            # ê¸°ì¡´ yahoo_finance_news.json êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì—…ë°ì´íŠ¸
            news_data = {
                'articles': sorted_news,
                'metadata': {
                    'total_count': len(sorted_news),
                    'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'unique_hashes_count': len(self.existing_hashes),
                    'collection_method': 'Scheduled with duplicate removal'
                }
            }
            
            with open(self.yahoo_news_file_path, 'w', encoding='utf-8') as f:
                json.dump(news_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ yahoo_finance_news.json íŒŒì¼ì— {len(sorted_news)}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def collect_and_process_news(self):
        """ìµœì‹  ë‰´ìŠ¤ë¥¼ ì§ì ‘ ìˆ˜ì§‘í•˜ì—¬ ì¤‘ë³µ ì œê±° ì²˜ë¦¬"""
        print("ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        try:
            # 1. ì§ì ‘ yfinanceë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
            new_news_list = self._collect_latest_news()
            
            if not new_news_list:
                print("â„¹ï¸ ìƒˆë¡œ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            print(f"ğŸ“Š ìƒˆë¡œ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ {len(new_news_list)}ê°œ í™•ì¸")
            
            # 2. ì¤‘ë³µ ì œê±° ì²˜ë¦¬
            return self._process_duplicate_removal(new_news_list)
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0
    
    def _process_duplicate_removal(self, news_list):
        """ë‰´ìŠ¤ ì¤‘ë³µ ì œê±° ì²˜ë¦¬"""
        print("ğŸ”„ ë‰´ìŠ¤ ì¤‘ë³µ ì œê±° ì²˜ë¦¬ ì‹œì‘...")
        
        new_count = 0
        duplicate_count = 0
        
        # ê° ë‰´ìŠ¤ì— ëŒ€í•´ ì¤‘ë³µ ê²€ì‚¬
        for news in news_list:
            title = news.get('ì œëª©', '')
            if not title or title == 'N/A':
                continue
            
            # 1. ë‰´ìŠ¤ ì œëª©ì„ MD5 í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜
            title_hash = self._generate_hash(title)
            
            # 2. ê¸°ì¡´ í•´ì‹œê°’ê³¼ ë¹„êµí•˜ì—¬ ì¤‘ë³µ ê²€ì‚¬
            if title_hash in self.existing_hashes:
                duplicate_count += 1
                print(f"    ğŸ”„ ì¤‘ë³µ ë‰´ìŠ¤ ì œì™¸: {title[:50]}...")
            else:
                # 3. ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš° ì €ì¥
                self.existing_hashes.add(title_hash)
                # ìˆ˜ì§‘ ì‹œê°„ ì •ë³´ ì¶”ê°€
                news_with_collection_time = news.copy()
                news_with_collection_time['ìˆ˜ì§‘ì‹œê°„'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                self.existing_news.append(news_with_collection_time)
                new_count += 1
                print(f"    âœ… ìƒˆë¡œìš´ ë‰´ìŠ¤ ì¶”ê°€: {title[:50]}...")
        
        print(f"ğŸ“ˆ ì²˜ë¦¬ ê²°ê³¼: ìƒˆë¡œìš´ ë‰´ìŠ¤ {new_count}ê°œ, ì¤‘ë³µ {duplicate_count}ê°œ")
        
        # 4. íŒŒì¼ì— ì €ì¥
        if new_count > 0:
            self._save_hashes()
            self._save_news_to_yahoo_file()
            print(f"ğŸ’¾ {new_count}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ yahoo_finance_news.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print("â„¹ï¸ ì €ì¥í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        return new_count

def run_news_collection():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰"""
    print("=" * 70)
    print(f"ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—… ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    try:
        # ë‰´ìŠ¤ ì¤‘ë³µ ì œê±° ê°ì²´ ìƒì„±
        deduplicator = NewsDeduplicator()
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±° ì²˜ë¦¬
        new_count = deduplicator.collect_and_process_news()
        
        if new_count > 0:
            print(f"ğŸ‰ ì‘ì—… ì™„ë£Œ: {new_count}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ ì¶”ê°€")
        else:
            print("â„¹ï¸ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    print("=" * 70)
    print(f"âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—… ì™„ë£Œ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    print()

def main():
    """ë©”ì¸ í•¨ìˆ˜: ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ë° ì‹¤í–‰"""
    print("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
    print("â° 1ë¶„ë§ˆë‹¤ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    print("ğŸ›‘ ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    print()
    
    # 5. ìŠ¤ì¼€ì¤„ ì„¤ì •: 1ë¶„ë§ˆë‹¤ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰
    schedule.every(1).minutes.do(run_news_collection)
    
    # ì‹œì‘ ì‹œ ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰
    print("ğŸ”„ ì´ˆê¸° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‘ì—… ì‹¤í–‰...")
    run_news_collection()
    
    # 6. ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (ë¬´í•œ ë£¨í”„)
    while True:
        try:
            schedule.run_pending()
            time.sleep(1)
        except KeyboardInterrupt:
            print("\n")
            print("ğŸ›‘ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ìŠ¤ì¼€ì¤„ëŸ¬ ì˜¤ë¥˜: {e}")
            time.sleep(5)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„

if __name__ == "__main__":
    main() 