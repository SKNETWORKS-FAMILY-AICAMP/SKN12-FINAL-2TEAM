from fastapi import FastAPI, Request
import uuid

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from AIChat.Router import run_question  # LangGraph íˆ´ ì‹¤í–‰ ê²°ê³¼

app = FastAPI()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. LangChain GPT ëª¨ë¸ ì¤€ë¹„
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. GPT ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(user_question: str, tool_result: str | list) -> str:
    # ë¬¸ìì—´ ë°°ì—´ì´ë©´ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì´ì–´ë¶™ì„
    if isinstance(tool_result, list):
        joined_result = "\n".join(tool_result)
    else:
        joined_result = str(tool_result)

    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì§ˆë¬¸ + ë„êµ¬ ê²°ê³¼ ëª¨ë‘ í¬í•¨)
    prompt = PromptTemplate.from_template(
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸: \"{user_question}\"\n\n"
        "ì•„ë˜ëŠ” ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë„êµ¬ë¥¼ í†µí•´ ìˆ˜ì§‘í•œ ì •ë³´ì…ë‹ˆë‹¤:\n\n"
        "{tool_result}\n\n"
        "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    )
    chain = prompt | llm

    # GPT ì‘ë‹µ ìƒì„±
    return chain.invoke({
        "user_question": user_question,
        "tool_result": joined_result
    }).content

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. /chat ì—”ë“œí¬ì¸íŠ¸ (APIìš©)
@app.post("/chat")
async def chat(req: Request):
    data = await req.json()
    user_msg: str = data["message"]
    session_id: str = data.get("session_id") or str(uuid.uuid4())

    # íˆ´ ì‹¤í–‰ ë° GPT ì‘ë‹µ ìƒì„±
    tool_result = run_question(user_msg)
    reply = generate_response(user_msg, tool_result)

    return {
        "session_id": session_id,
        "reply": reply
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. CLI ì‹¤í–‰ (í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ìš©)
if __name__ == "__main__":
    print("ğŸ’¬ LangGraph ê¸°ë°˜ GPT CLI (ì¢…ë£Œ: Ctrl+C)\n")
    try:
        while True:
            q = input("ì§ˆë¬¸ > ").strip()
            if not q:
                continue
            tool_result = run_question(q)
            answer = generate_response(q, tool_result)
            print("\nğŸ§  GPT ì‘ë‹µ:\n" + answer + "\n")
    except (EOFError, KeyboardInterrupt):
        print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
