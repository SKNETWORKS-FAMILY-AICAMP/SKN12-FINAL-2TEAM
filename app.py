"""
app_chat_stream.py
FastAPI  :  REST /chat   +   WebSocket /stream  (í† í° ìŠ¤íŠ¸ë¦¬ë°)
RedisChatMessageHistory :  ì„¸ì…˜ë³„ ëŒ€í™” ì˜êµ¬ ì €ì¥
"""

import os, uuid, asyncio, json
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect, HTTPException
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import RedisChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from base_server.service.llm.AIChat.Router import run_question           # LangGraph íˆ´ ì‹¤í–‰

# â”€â”€ í™˜ê²½
REDIS_URL  = os.getenv("REDIS_URL", "redis://localhost:6379/0")
KEY_PREFIX = "chat:"
MAX_TOKENS = 8_000

app = FastAPI()

llm        = ChatOpenAI(model="gpt-4o", temperature=0)
llm_stream = ChatOpenAI(model="gpt-4o", temperature=0, streaming=True)   # ìŠ¤íŠ¸ë¦¬ë°ìš©

# â”€â”€ ì„¸ì…˜ë³„ ë©”ëª¨ë¦¬
_session_mem: dict[str, ConversationBufferMemory] = {}
def mem(session_id: str) -> ConversationBufferMemory:
    if session_id not in _session_mem:
        history = RedisChatMessageHistory(
            session_id=session_id,
            url=REDIS_URL,
            key_prefix=KEY_PREFIX,
        )
        _session_mem[session_id] = ConversationBufferMemory(
            chat_memory=history,
            return_messages=True
        )
    return _session_mem[session_id]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ REST  /chat
@app.post("/chat")
async def chat(req: Request):
    body = await req.json()
    q = body["message"].strip()
    if not q:
        raise HTTPException(400, "message empty")
    sid = body.get("session_id") or str(uuid.uuid4())

    loop = asyncio.get_event_loop()
    tool_out = await loop.run_in_executor(None, run_question, q)
    answer = await _full_answer(sid, q, tool_out)
    return {"session_id": sid, "reply": answer}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WebSocket /stream
@app.websocket("/stream")
async def ws_stream(ws: WebSocket):
    await ws.accept()
    try:
        while True:
            # â”€â”€ 0) í´ë¼ì´ì–¸íŠ¸ ì§ˆë¬¸ ëŒ€ê¸°
            data = await ws.receive_text()
            try:
                req = json.loads(data)
                q   = req["message"].strip()
                sid = req.get("session_id") or str(uuid.uuid4())
            except (KeyError, json.JSONDecodeError):
                await ws.send_text(json.dumps({"error": "bad payload"}))
                continue
            if not q:
                await ws.send_text(json.dumps({"error": "empty message"}))
                continue

            # â”€â”€ 1) LangGraph íˆ´ (blocking â†’ executor)
            tool_out = await asyncio.get_running_loop().run_in_executor(
                None, run_question, q
            )
            joined = "\n".join(tool_out) if isinstance(tool_out, list) else str(tool_out)

            # â”€â”€ 2) í”„ë¡¬í”„íŠ¸
            memory = mem(sid)
            prompt = ChatPromptTemplate.from_messages(
                [("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì •í™•í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.")] +
                memory.buffer +
                [("user", f'{q}\n\nğŸ›  ë„êµ¬ ê²°ê³¼:\n{joined}')]
            )

            # â”€â”€ 3) ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            stream = (prompt | llm_stream).astream({})
            full_resp = ""
            async for chunk in stream:
                token = getattr(chunk, "content", "")     # âœ”ï¸ LangChain 0.2
                if token:
                    full_resp += token
                    await ws.send_text(token)             # í† í° ë‹¨ìœ„ ì „ì†¡

            # â”€â”€ 4) ì¢…ë£Œ ì‹ í˜¸
            await ws.send_text("[DONE]")

            # â”€â”€ 5) íˆìŠ¤í† ë¦¬ ì €ì¥
            memory.chat_memory.add_user_message(q)
            memory.chat_memory.add_ai_message(full_resp)

    except WebSocketDisconnect:
        return

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‚´ë¶€ í•¨ìˆ˜ (REST ì „ì²´ ì‘ë‹µ)
async def _full_answer(sid: str, question: str, tool_out):
    joined = "\n".join(tool_out) if isinstance(tool_out, list) else str(tool_out)
    memory = mem(sid)
    prompt = ChatPromptTemplate.from_messages(
        [("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì •í™•í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.")] +
        memory.buffer +
        [("user", f'{question}\n\nğŸ›  ë„êµ¬ ê²°ê³¼:\n{joined}')]
    )
    answer = (prompt | llm).invoke({}).content
    memory.chat_memory.add_user_message(question)
    if isinstance(answer, list):
        answer = "\n".join(str(x) for x in answer)
    memory.chat_memory.add_ai_message(answer)
    return answer
